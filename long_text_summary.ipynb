{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ea5c15b-88bd-4470-8ad8-63ec3ba25608",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b19e5b6d-6b8d-406f-a332-0ae779868832",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameters():\n",
    "    batch_size:int=16\n",
    "    EMBEDDING_MODEL:str=\"text-embedding-ada-002\"\n",
    "    COMPLETION_MODEL:str=\"gpt-3.5-turbo-16k-0613\"\n",
    "    AVERAGE_TOPIC_WORDS:int=500\n",
    "    CONTEXT_LENGTH:int=16000 # The permitted context length in tokens\n",
    "    SUMMARIZE_TOPIC_TOKENS:int=14000 # 14000 The last to-summary-text token num\n",
    "    PHASE_1_MODEL:str=\"gpt-3.5-turbo-0613\"\n",
    "    PHASE_1_CHUNK_LEN:int=1500 # Number of tokens\n",
    "    PHASE_1_PARA_LEN:int=200 # token num\n",
    "    PHASE_1_PARAGRAPH_NUM:int=PHASE_1_CHUNK_LEN//PHASE_1_PARA_LEN # prompt to divide each chunk into at least 5 paragraphs - one paragraph with 200 tokens\n",
    "    PHASE_1_CHUNK_LEN_IN_WORD:int=0 # Number of words, updated by function `get_phase_1_chunks`\n",
    "    PHASE_1_CHUNK_OVERLAY:int=30 # Overlapped word number\n",
    "    PHASE_1_CONTEXT_LENGTH:int=4096 # Phase 1 is for cleanup text. It might use a different model\n",
    "    TOPIC_SUMMARY_LENGTH:int=800 # The length of topic summary in words\n",
    "    FINAL_SUMMARY_LENGTH:int=1200 # The length of final summary in words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f87ffb7-6692-4049-a500-229c63ac5e3f",
   "metadata": {},
   "source": [
    "## Little utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d81e01d-af92-4d61-a24a-6ad268d4fce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, re, os, json\n",
    "import asyncio\n",
    "import concurrent.futures\n",
    "\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "from openaikey import OPENAI_API_KEY\n",
    "import openai\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "import random\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import tiktoken\n",
    "\n",
    "# llm = ChatOpenAI(model_name=Parameters.COMPLETION_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2140893e-1558-4df6-997a-e7de42ea2789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(txt:str)->List[str]:\n",
    "    splited_word_list = []\n",
    "    word = ''\n",
    "    # import ipdb; ipdb.set_trace()\n",
    "    for c in txt:\n",
    "        if len(c)==0:\n",
    "            continue\n",
    "        ord_c = ord(c)\n",
    "        if word!='' and ord_c in (list(range(0x00, 0x20+1)) + [0x7F] + [0xFF]):\n",
    "            splited_word_list.append(word)\n",
    "            word = ''\n",
    "        elif ord_c in (list(range(0x21, 0x7E+1)) + list(range(0x80, 0xFE+1))):\n",
    "            word += c\n",
    "        elif ord_c > 0xFF:\n",
    "            if word!='':\n",
    "                splited_word_list.append(word)\n",
    "            splited_word_list.append(c)\n",
    "            word = ''\n",
    "    if word != '':\n",
    "        splited_word_list.append(word)\n",
    "    return splited_word_list\n",
    "\n",
    "def join_texts(txt_list:List[str])->str:\n",
    "    if type(txt_list)==str:\n",
    "        return txt_list\n",
    "        \n",
    "    txt_list_new = []\n",
    "    non_asci = False\n",
    "    non_asci_word = ''\n",
    "    for w in txt_list:\n",
    "        if len(w)==0:\n",
    "            continue\n",
    "        ord_c = ord(w[0])\n",
    "        if ord_c > 0xFF: # non-ASCI\n",
    "            non_asci = True\n",
    "            non_asci_word += w\n",
    "        else: # ASCI\n",
    "            if non_asci_word != '':\n",
    "                txt_list_new.append(non_asci_word)\n",
    "                non_asci = False\n",
    "                non_asci_word = ''\n",
    "            txt_list_new.append(w)\n",
    "    if non_asci_word != '':\n",
    "        txt_list_new.append(non_asci_word)\n",
    "    return ' '.join(txt_list_new)\n",
    "\n",
    "# Get the number of words\n",
    "def strlen(txt:str)->int:\n",
    "    txt1 = re.sub(r\"[\\x21-\\x7E\\x80-\\xFE]+\", 'A', txt)\n",
    "    txt1 = re.sub(r\"[\\x00-\\x20\\xFF]\", '', txt1)\n",
    "    length = len(txt1)\n",
    "    return length\n",
    "\n",
    "# Get the number of tokens\n",
    "def get_num_tokens(txt:str, model:str=Parameters.COMPLETION_MODEL)->int:\n",
    "    # global llm\n",
    "    # if 'llm' not in globals():\n",
    "    #     llm = ChatOpenAI(model_name=model)\n",
    "    # global llm_encoding\n",
    "    # return llm.get_num_tokens(txt)\n",
    "\n",
    "    global llm_encoding\n",
    "    if 'llm_encoding' not in globals():\n",
    "        llm_encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(llm_encoding.encode(txt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ae445a-a8f0-4d74-a7eb-c9ea1cbdfd93",
   "metadata": {},
   "source": [
    "# Concurrent OpenAI Call"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968d587c-6d51-4222-8a4b-db2cb07f30e8",
   "metadata": {},
   "source": [
    "#### Batch generate using OpenAI\n",
    "\n",
    "```python\n",
    "def batch_generate(prompts:List[str], batch_size:int=8, temperature:float=0.2)->List[str]:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5375a22b-c3b2-4d05-8cad-670543b6b3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")  # for exponential backoff: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_handle_rate_limits.ipynb\n",
    "\n",
    "def _num_tokens_from_messages(messages:List[str], model:str)->int:\n",
    "    \"\"\"Returns the number of tokens used by a list of messages.\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        print(\"Warning: model not found. Using cl100k_base encoding.\")\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    if re.match('^gpt-3.5-turbo.*$', model):\n",
    "        tokens_per_message = 4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n",
    "        tokens_per_name = -1  # if there's a name, the role is omitted\n",
    "    elif re.match('^gpt-4.*$', model):\n",
    "        tokens_per_message = 3\n",
    "        tokens_per_name = 1\n",
    "    else:\n",
    "        raise NotImplementedError(f\"\"\"num_tokens_from_messages() is not implemented for model {model}. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\")\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
    "    return num_tokens\n",
    "\n",
    "def openai_usage_stat()->Dict[str, object]:\n",
    "    if 'openai_usages' not in globals() or len(openai_usages)==0:\n",
    "        return {'object': '', 'model': '', 'prompt_tokens':0, 'completion_tokens': 0, 'total_tokens': 0}\n",
    "    _usages = []\n",
    "    # import ipdb; ipdb.set_trace()\n",
    "    unique_items = list(set(tuple(sorted(d.items())) for d in [{'object': c.get('object'), 'model': c.get('model')} for c in openai_usages]))\n",
    "    unique_items = [dict(items) for items in unique_items]\n",
    "    # print(unique_items)\n",
    "    for item in unique_items:\n",
    "        object_name = item.get('object')\n",
    "        model_name = item.get('model')\n",
    "        _usage = {'object': object_name, 'model': model_name, \n",
    "                  'prompt_tokens': sum([x.get('prompt_tokens') for x in openai_usages if x.get('object') == object_name and x.get('model') == model_name]), \n",
    "                  'completion_tokens': sum([x.get('completion_tokens') for x in openai_usages if x.get('object') == object_name and x.get('model') == model_name]), \n",
    "                  'total_tokens': sum([x.get('total_tokens') for x in openai_usages if x.get('object') == object_name and x.get('model') == model_name])}\n",
    "        _usages.append(_usage)\n",
    "\n",
    "    cost = 0.\n",
    "    for a in openai_usages:\n",
    "        prompt_tokens = a.get('prompt_tokens')\n",
    "        completion_tokens = a.get('completion_tokens')\n",
    "        model_name = a.get('model')\n",
    "        object_name = a.get('object')\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "        if re.match(r'^gpt-3.5.*16k.*$', model_name, re.IGNORECASE) and re.match(r'^chat\\.', object_name, re.IGNORECASE): # Chat model, 16K context\n",
    "            cost += prompt_tokens * 0.003 / 1000\n",
    "            cost += completion_tokens * 0.004 / 1000\n",
    "        elif re.match(r'^gpt-3.5.*$', model_name, re.IGNORECASE) and re.match(r'^chat\\.', object_name, re.IGNORECASE): # Chat model, 4K context \n",
    "            cost += prompt_tokens * 0.0015 / 1000\n",
    "            cost += completion_tokens * 0.002 / 1000\n",
    "\n",
    "    return {\"TokenUsage\": _usages, \"Dollar_Usage\": cost}\n",
    "\n",
    "def _get_completion_func(model=Parameters.COMPLETION_MODEL, context_length:int=Parameters.CONTEXT_LENGTH, temperature:float=0.3, request_timeout:int=300): # Andrew mentioned that the prompt/ completion paradigm is preferable for this class\n",
    "    # help: https://platform.openai.com/docs/api-reference/chat/create\n",
    "    global openai_usages\n",
    "    if 'openai_usages' not in globals():\n",
    "        openai_usages = []\n",
    "    \n",
    "    MAX_RETRY_TIMES = 3\n",
    "    @retry(wait=wait_random_exponential(multiplier=1, min=1, max=120), stop=stop_after_attempt(MAX_RETRY_TIMES))\n",
    "    def _openai_completion(prompt:str):\n",
    "        openai.api_key = OPENAI_API_KEY\n",
    "        system_content = \"You're a meticulous and careful AI assistant who pays extreme attention to details and does not overlook any important information, especially for technical steps.\"\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}] # {\"role\": \"system\", \"content\": system_content},\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}, {\"role\": \"system\", \"content\": system_content},]\n",
    "        num_prompt_tokens = _num_tokens_from_messages(messages, model)\n",
    "        max_tokens=context_length - num_prompt_tokens\n",
    "        print(f\"Inside _get_completion_func: num_prompt_tokens = {num_prompt_tokens:,d}, max_tokens = {max_tokens:,d}, model = {model}, context_length = {context_length:,d}\")\n",
    "        # print(messages)\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            max_tokens=max_tokens, # do we really need this \"max_tokens\"?\n",
    "            request_timeout=request_timeout,\n",
    "            temperature=temperature, # this is the degree of randomness of the model's output\n",
    "        )\n",
    "        usage = response.get('usage')\n",
    "        if usage:\n",
    "            usage = dict(usage)\n",
    "            usage[\"model\"] = response.get('model')\n",
    "            usage[\"object\"] = response.get('object')\n",
    "            openai_usages.append(usage)\n",
    "        return response.choices[0].get('message').get('content')\n",
    "    return _openai_completion\n",
    "\n",
    "\n",
    "def _batch_generate(prompts:List[str], batch_size:int=4, temperature:float=0.3, model:str=Parameters.COMPLETION_MODEL, context_length:int=Parameters.CONTEXT_LENGTH, request_timeout:int=300):\n",
    "    final_results = []\n",
    "    num_prompts = len(prompts)\n",
    "    start_pos = 0\n",
    "    futures = []\n",
    "    while start_pos < num_prompts:\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            for prompt in prompts[start_pos:start_pos+batch_size]:\n",
    "                num_words = strlen(prompt)\n",
    "                num_prompt_tokens = get_num_tokens(prompt)\n",
    "                if num_prompt_tokens / num_words > 1.7: # We have too much tokens from the words and thus we need to turn to the model with longer context\n",
    "                    print(f\"num_prompt_tokens / num_words = {num_prompt_tokens / num_words:.2f}, let's switch to a model with longer context!!\")\n",
    "                    completion_func = _get_completion_func(model=Parameters.COMPLETION_MODEL, temperature=temperature, request_timeout=request_timeout, context_length=Parameters.CONTEXT_LENGTH)\n",
    "                else:\n",
    "                    completion_func = _get_completion_func(model=model, temperature=temperature, request_timeout=request_timeout, context_length=context_length)\n",
    "                futures.append(executor.submit(completion_func, prompt))\n",
    "                # futures = [executor.submit(get_completion, prompt) for prompt in prompts[start_pos:start_pos+batch_size]]\n",
    "            start_pos += batch_size\n",
    "    results = [future.result() for future in futures]\n",
    "    final_results += results\n",
    "\n",
    "    return final_results\n",
    "\n",
    "def batch_cleanup(chunks:List[str], temperature:float=0.1, batch_size:int=4, model:str=Parameters.PHASE_1_MODEL, context_length:int=Parameters.PHASE_1_CONTEXT_LENGTH):\n",
    "    #   Divide it into at least {strlen(chunk)//300 + (1 if strlen(chunk) % 300 != 0 else 0):d} sentimentally coherent sections, using \"\\n\\n\" as a delimiter. Translate the text into English, unless it's already in English or Chinese.\n",
    "    prompts = [f\"\"\"Please add (or update with) proper punctuations, like ',', '.', '?', to the following text. Correct obvious spelling or grammatic errors as well if possible. At the same time, divide the text into at least {Parameters.PHASE_1_PARAGRAPH_NUM:d} sentimentally coherent paragraphs, using \"\\n\\n\" as a delimiter. You should not give any extra comment.\n",
    "    \n",
    "    ```{chunk}```\"\"\" for chunk in chunks]    \n",
    "    return _batch_generate(prompts, temperature=temperature, batch_size=batch_size, model=model, context_length=context_length)\n",
    "\n",
    "\n",
    "def batch_summary(topic_groups:List[str], temperature:float=0.3, batch_size:int=4, summary_length:int=Parameters.TOPIC_SUMMARY_LENGTH, model:str=Parameters.COMPLETION_MODEL, context_length:int=Parameters.CONTEXT_LENGTH):\n",
    "    model = model if model else Parameters.COMPLETION_MODEL\n",
    "    print(f\"batch_summary model = {model}\")\n",
    "    prompts = [f\"\"\"```{topic_group}```\n",
    "\n",
    "You task is to give a verbose summary to the above article. The summary should be about {max(1, min(summary_length, int(strlen(topic_group)*0.5)))}-word-long in multi-paragraphs \\\n",
    "Summarize the above article. The summary should be about {max(1, min(summary_length, int(strlen(topic_group)*0.5)))}-word-long and should contain sufficient information of each key points. \\\n",
    "Do not miss any important technical details (like formulas, algorithms, reasoning steps, etc) or subtle nuances presented in the source material.\n",
    "\"\"\" for topic_group in topic_groups]\n",
    "\n",
    "    return _batch_generate(prompts, temperature=temperature, batch_size=batch_size, model=model, context_length=context_length)\n",
    "\n",
    "def batch_rephrase(topic_groups:List[str], temperature:float=0., batch_size:int=1, model:str=Parameters.COMPLETION_MODEL, context_length:int=Parameters.CONTEXT_LENGTH):\n",
    "    # prompts = [f\"\"\"Please rewrite the following text. Use English to rewrite if the provided text is not Chinese:\n",
    "    prompts = [f\"\"\"Please rewrite the following text:\n",
    "    ```\\n{topic_group}\\n```\"\"\" for topic_group in topic_groups]\n",
    "    # print(prompts)\n",
    "    return _batch_generate(prompts, temperature=temperature, batch_size=batch_size, model=model, context_length=context_length)\n",
    "\n",
    "def batch_highlights(topic_groups:List[str], temperature:float=0.5, batch_size:int=1, num_highlights:int=7, model:str=Parameters.COMPLETION_MODEL, context_length:int=Parameters.CONTEXT_LENGTH):\n",
    "    model = model if model else Parameters.COMPLETION_MODEL\n",
    "    print(f\"batch_highlights model = {model}\")\n",
    "    prompts = [f\"\"\"Guidelines: Please proceed in the following manner for your output:\n",
    "\n",
    "#### Short Synopsis\n",
    "#### Key Points\n",
    "- [Emoji] Concise Bulletpoint\n",
    "\n",
    "Your mission is to create an English summary of the text I've given you, delimited by ```, with a maximum of {num_highlights} succinct bullet points, prefixed by a proper emoji.\n",
    "\n",
    "```{topic_groups}```\n",
    "\"\"\" for topic_group in topic_groups]    \n",
    "    return _batch_generate(prompts, temperature=temperature, batch_size=batch_size, model=model, context_length=context_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bd070ad-e05e-4129-a9a2-15c12415177b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_summary = ['''In this article, the author interviews Jared Zonnerak, the co-founder of PromptLayer, a collaborative platform for teams and individuals to track, debug, and explore their language model application requests. The conversation revolves around the common denominator that all language model practitioners share: the prompt. Zonnerak explains that prompt engineering is a skill set that involves tinkering with the prompt to optimize the output of language models. He believes that prompt engineering is not just for engineers, but also for product managers and non-technical stakeholders.\\n\\nPromptLayer was launched in January and has since gained popularity among developers and teams. The platform provides a one-line code setup that allows users to track and analyze their logs, save prompt templates, and gain insights through analytics. It caters to both indie developers and teams, with features specifically designed for collaboration between technical and non-technical stakeholders.\\n\\nZonnerak discusses the journey of PromptLayer since its launch, highlighting the feature updates and the recent addition of support for the anthropic language model. The company has also raised funding to support its growth and is currently looking to hire a founding engineer.\\n\\nLooking ahead, Zonnerak shares that PromptLayer\\'s roadmap includes focusing on improving the product for both hackers and teams. They aim to build workflows that allow teams to work on prompts collaboratively and efficiently. They are also working on features like unit testing and backtesting prompts, as well as A/B testing capabilities.\\n\\nThe conversation then shifts to the challenges of being a founder in the AI space and dealing with the constant influx of news and information. Zonnerak acknowledges the noise but emphasizes the importance of staying rooted in the needs and feedback of actual users. He believes that prompt engineering is a skill set that will continue to evolve and become more important in the future, with product managers potentially taking ownership of it.\\n\\nWhen asked about prominent prompt engineers in the industry, Zonnerak explains that prompt engineering is still a relatively new field and there are no defined experts or certifications. He suggests following builders and developers who are actively shipping products rather than those who only talk about prompt engineering. He also mentions the dynamics of how teams interact with prompts, explaining that before using PromptLayer, teams often relied on tools like Google Docs or databases to manage prompts.\\n\\nThe conversation then delves into the future of prompting, with Zonnerak sharing his belief that prompts will always be necessary, even as language models become more intelligent. He argues that humans themselves rely on prompts when communicating with each other, and prompts serve as a starting point for language models. He also highlights the importance of other variables in prompt engineering, such as the choice of model, temperature, and user segmentation.\\n\\nZonnerak recommends practicing prompt engineering by playing around with language models in playgrounds like OpenAI\\'s playground. He also suggests reading Stephen Wolfram\\'s articles on language models to gain a better understanding of the underlying technology.\\n\\nThe article concludes with Zonnerak discussing the serendipitous moments and challenges of attending events in the AI space. He mentions some under-the-radar companies he is excited about, such as Great for code refactoring and DeepTune for podcast dubbing AI. Zonnerak also shares his love for books and recommends Nassim Nicholas Taleb\\'s \"Antifragile\" and \"Skin in the Game.\"\\n\\nIn closing, Zonnerak advises developers and entrepreneurs to \"just ship it\" and not spend too much time strategizing. He encourages early product releases and invites readers to check out PromptLayer and connect with him on Twitter or via email.''']\n",
    "\n",
    "# temperature = 0.0\n",
    "# num_highlights = 5\n",
    "\n",
    "# final_highlight = batch_highlights(final_summary, temperature=temperature, num_highlights=num_highlights, model=Parameters.PHASE_1_MODEL, context_length=Parameters.PHASE_1_CONTEXT_LENGTH)\n",
    "# print(final_highlight[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2567ccc-1755-4369-b0c0-4e03e2881f56",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792ba0d3-cfe1-4344-9820-84690545f4cf",
   "metadata": {},
   "source": [
    "## From Youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e579a66-373b-4bd2-98ae-fa24dd54c2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "847292dc-bf6e-4f5e-bb3a-454001e5ff67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transcript(video_id: str, languages:List[str]=['en', 'de', 'zh'])->List[Dict[str, object]]:\n",
    "    results = YouTubeTranscriptApi.get_transcripts([video_id], languages=languages)[0][video_id]\n",
    "    txt = join_texts([sub['text'] for sub in results])\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d45c9e-3613-4e96-91e4-eff9c9ad96a9",
   "metadata": {},
   "source": [
    "## From text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e5b0aa2-2d3c-49e2-8da4-3413e5176f52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_txt_from_file(txt_path:str)->str:\n",
    "    def get_text(txt_path:str):\n",
    "        with open(txt_path, 'r') as f:\n",
    "              txt = f.read()\n",
    "        return txt\n",
    "    txt = get_text(txt_path)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7ae704-0cab-4e54-9673-9b41ae80e602",
   "metadata": {},
   "source": [
    "## Cleanup text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16697bd9-785e-45eb-a70a-0bebb076e26c",
   "metadata": {},
   "source": [
    "### Phase 1 chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6a0a8fb-20ba-49ca-b628-3fcfd445d615",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phase_1_chunks(txt:str, end_pos:int=None)->List[str]:\n",
    "    num_words = strlen(txt[:end_pos])\n",
    "    num_tokens = get_num_tokens(txt[:end_pos])\n",
    "    num_chunk = num_tokens // Parameters.PHASE_1_CHUNK_LEN + 1 # use token to calculate the number of chunks\n",
    "    chunk_size = num_words // num_chunk + 1 # use word number to really calculate chunk size (not easy to split text via token)\n",
    "    Parameters.PHASE_1_CHUNK_LEN_IN_WORD = chunk_size\n",
    "    txt_word_list = split_text(txt[:end_pos])\n",
    "    phase_1_chunk_list = []\n",
    "    start_pos = 0\n",
    "    print(f\"Total words: {num_words:,d} ({num_tokens:,d} tokens), chunk_size = {chunk_size:,d} words\")\n",
    "    for _ in range(num_chunk):\n",
    "        while start_pos < num_words:\n",
    "            phase_1_chunk_list.append(join_texts(txt_word_list[start_pos:start_pos + chunk_size + Parameters.PHASE_1_CHUNK_OVERLAY]))\n",
    "            print(f\"strlen(phase_1_chunk_list[-1]) = {strlen(phase_1_chunk_list[-1]):,d}, from {start_pos:,d} to {start_pos + strlen(phase_1_chunk_list[-1]):,d}\")\n",
    "            start_pos += chunk_size\n",
    "    return phase_1_chunk_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c91d9c-b078-4929-8569-7049cb216bd7",
   "metadata": {},
   "source": [
    "### Get chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c42d3ccf-03b6-4047-a0d6-315388bcc929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned_chunk_list = ['aaaa als. ', 'asdkw ', 'sldf 2.' , 'als 2,3. ', 'sal ab.', 'aserl s2. 332aaaa.', 'aserl s2. 223 aaaa.']\n",
    "# [c.strip()+\"\\n\\n\" if re.match(r'.*[\\s]+[^0-9]+\\.[\\s]*$', c) else c.strip() for c in cleaned_chunk_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0bb2d7e-6266-44ad-88b1-eb4642625e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_large_chunks(cleaned_chunk_list0:List[str])->List[str]:\n",
    "    cleaned_chunk_list = []\n",
    "    for chunk in cleaned_chunk_list0:\n",
    "        for p in re.split(r'[\\n]+', chunk): # p: paragraph\n",
    "            n_tokens = get_num_tokens(p)\n",
    "            if n_tokens > int(Parameters.PHASE_1_PARA_LEN * 2):\n",
    "                n_paraph = n_tokens // Parameters.PHASE_1_PARA_LEN + 1\n",
    "                paraph_token_len = n_tokens // n_paraph\n",
    "                print(f\"We're going to split this sub-chunk as it has {n_tokens:,d} tokens, larger than {int(Parameters.PHASE_1_PARA_LEN * 2):,d}. Words number: {strlen(p)}. It'll be split to {n_paraph} chunks.\")\n",
    "                sub_chunk = []\n",
    "                for s in re.split(r'[.]', p):\n",
    "                    if s=='':\n",
    "                        continue\n",
    "                    s += \".\"\n",
    "                    sub_chunk_text = ''.join(sub_chunk)\n",
    "                    if get_num_tokens(sub_chunk_text)+get_num_tokens(s)>=Parameters.PHASE_1_PARA_LEN:\n",
    "                        cleaned_chunk_list.append(sub_chunk_text)\n",
    "                        sub_chunk = []\n",
    "                    sub_chunk.append(s)\n",
    "                if len(sub_chunk)>0:\n",
    "                    sub_chunk_text = ''.join(sub_chunk)\n",
    "                    if len(cleaned_chunk_list)>0 and get_num_tokens(sub_chunk_text) + get_num_tokens(cleaned_chunk_list[-1]) < int(Parameters.PHASE_1_PARA_LEN*1.6):\n",
    "                        cleaned_chunk_list[-1] = cleaned_chunk_list[-1] + sub_chunk_text\n",
    "                    else:\n",
    "                        cleaned_chunk_list.append(sub_chunk_text)\n",
    "            else:\n",
    "                cleaned_chunk_list.append(p)\n",
    "    \n",
    "    cleaned_chunk_list = [c.strip()+\"\\n\\n\" if re.match(r'.*[\\s]+[^0-9]+\\.[\\s]*$', c) else c.strip() for c in cleaned_chunk_list]\n",
    "    print(\"We've split large chunks, by words, from:\", [strlen(c) for c in cleaned_chunk_list0], \"to:\", [strlen(c) for c in cleaned_chunk_list], 'by tokens, from:', [get_num_tokens(c) for c in cleaned_chunk_list0], \"to:\", [get_num_tokens(c) for c in cleaned_chunk_list])\n",
    "    return cleaned_chunk_list\n",
    "\n",
    "# We're going to clean the overlap here\n",
    "def get_chunks(cleaned_chunk_list:List[str], overlapped:int)->(str,List[str]):\n",
    "    if overlapped != 0:\n",
    "        cleaned_chunk_list = [re.sub(r'\\W*$', '', chunk) for chunk in cleaned_chunk_list]\n",
    "    \n",
    "    def _resolve_overlap(t0:str, t1:str)->Tuple:\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "        t0_wordlist = split_text(t0)\n",
    "        search_length = len(join_texts(t0_wordlist[-overlapped:]))\n",
    "        search_start_pos = max(1, len(join_texts(t0_wordlist[-overlapped//5:]))) # Give the \"leftOver\" enough words\n",
    "        \n",
    "        t0_match = re.search(r'(\\n\\n )*[\\.!?;:,*/-]', t0[-search_start_pos:-search_length:-1])\n",
    "        if not t0_match:\n",
    "            search_start_pos = max(1, len(join_texts(t0_wordlist[int(-overlapped//1.5):]))) # Give the \"leftOver\" more words\n",
    "            t0_match = re.search(r'(\\n\\n )*[ ]', t0[-search_start_pos:-search_length:-1]) # Start from any word\n",
    "        \n",
    "        t1_match_start = 0\n",
    "\n",
    "        if t0_match:\n",
    "            t0_endAt = len(t0) - t0_match.start() - search_start_pos + 1\n",
    "            leftOver = [word for word in split_text(t0[t0_endAt:]) if word != '']\n",
    "            num_search_word = 8\n",
    "            print('leftOver: ', leftOver)\n",
    "            while num_search_word>0 and len(leftOver)>0:\n",
    "                start_from_word_idx = random.randint(0, min(len(leftOver)-1, len(leftOver)//2))\n",
    "                # print(f\"len(leftOver) = {len(leftOver):,d}, start_from_word_idx = {start_from_word_idx:,d}\")\n",
    "                word_to_search = join_texts(leftOver[start_from_word_idx:start_from_word_idx+min(num_search_word, len(leftOver) - start_from_word_idx)])\n",
    "                # print(f\"word_to_search: {word_to_search}\")\n",
    "                search_in = t1[:search_length]\n",
    "                t1_match = search_in.lower().find(word_to_search.lower())\n",
    "                # t1_match = re.search(word_to_search, t1[:search_length], re.IGNORECASE)\n",
    "                if t1_match!=-1:\n",
    "                    print(f\"Matched!!! word_to_search: {word_to_search}\")\n",
    "                    # Find matching in the next chunk - easy to resolve the overlap\n",
    "                    t1_match_start = t1_match # t1_match.start()\n",
    "                    t0_endAt += len(join_texts(leftOver[:start_from_word_idx])) + 1\n",
    "                    break\n",
    "                if random.random() < 1/(10-num_search_word):\n",
    "                    num_search_word -= 1\n",
    "            if num_search_word == 0:\n",
    "                # Can't find matching in the next chunk\n",
    "                print(f\"No matching for \\\"{leftOver}\\\"\")\n",
    "                t0_endAt = len(t0) - len(join_texts(t0_wordlist[-int(overlapped*2/3):])) # remove 2/3 the overlap\n",
    "                t1_match_start = 0\n",
    "        else:\n",
    "            t0_endAt = len(t0) - len(join_texts(t0_wordlist[-int(overlapped*2/3):])) # remove 2/3 the overlap\n",
    "            t1_match_start = 0\n",
    "    \n",
    "        # Remove overlap\n",
    "        print(\"Original end of t0: \", t0[-search_length:])\n",
    "        print(\"Original start of t1: \", t1[:search_length])\n",
    "        t0 = t0[:t0_endAt].strip()\n",
    "        t1 = t1[t1_match_start:].strip()\n",
    "        print(\"End of t0: \", t0[-100:])\n",
    "        print(\"Start of t1: \", t1[:100])\n",
    "        print('#'*50)\n",
    "        return t0, t1\n",
    "\n",
    "    for i in range(len(cleaned_chunk_list)-1):\n",
    "        t0 = cleaned_chunk_list[i]\n",
    "        t1 = cleaned_chunk_list[i+1]\n",
    "        if overlapped != 0:\n",
    "            # remove overlaps\n",
    "            t0, t1 = _resolve_overlap(t0, t1)\n",
    "        cleaned_chunk_list[i] = t0.strip()\n",
    "        cleaned_chunk_list[i+1] = t1.strip()\n",
    "\n",
    "    cleaned_chunk_list = _split_large_chunks(cleaned_chunk_list)\n",
    "    # import ipdb; ipdb.set_trace()\n",
    "    chunks = []\n",
    "    n_appended_short_chunk = 0\n",
    "    joined_chunk_list = join_texts(cleaned_chunk_list) # This is the first round chunking that some items in the \"cleaned_chunk_list\" might be splitted arbitrarily\n",
    "    if overlapped == 0: # This is NOT the first round chunking and each item in the \"cleaned_chunk_list\" should be regarded as a \"whole chunk\" already\n",
    "        joined_chunk_list = '\\n\\n'.join(cleaned_chunk_list)\n",
    "    for c in re.split(r'[\\n]{2,}', joined_chunk_list):\n",
    "        s_c = c.strip()\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "        if strlen(s_c) < 15 and n_appended_short_chunk<5: # Combined with previous chunk if it's too short\n",
    "            n_appended_short_chunk+=1\n",
    "            if len(chunks)>0:\n",
    "                chunks[-1] = chunks[-1].strip() + ' ' + s_c\n",
    "            else:\n",
    "                chunks.append(s_c)\n",
    "        else:\n",
    "            n_appended_short_chunk=0\n",
    "            chunks.append(s_c)\n",
    "    print(\"Length of splited chunks, in words:\", [strlen(c) for c in chunks], \"num_of_tokens:\", [get_num_tokens(c) for c in chunks], f\", Longest one with length: {max([strlen(c) for c in chunks]):,d} words ({max([get_num_tokens(c) for c in chunks]):,d} tokens)\")    \n",
    "    print(\"Length of cleaned_chunk_list, in words:\", [strlen(c) for c in cleaned_chunk_list], \"num_of_tokens:\", [get_num_tokens(c) for c in cleaned_chunk_list], f\", Longest one with length: {max([strlen(c) for c in cleaned_chunk_list]):,d} words ({max([get_num_tokens(c) for c in cleaned_chunk_list]):,d} tokens)\")\n",
    "\n",
    "    return chunks, cleaned_chunk_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34ee5cd-dd6c-42ab-81ba-6a1f4d41b399",
   "metadata": {},
   "source": [
    "# Get topic groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87666a1a-123a-458b-93f5-922322d51561",
   "metadata": {},
   "source": [
    "## Get Chunk embeddings from OpenAI\n",
    "\n",
    "[OpenAI Create Embeddings](https://platform.openai.com/docs/api-reference/embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2108bc5c-b182-4808-9c82-0ea3e00e0c05",
   "metadata": {},
   "source": [
    "## Embedding based text similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9bb8844-1fb7-4392-adfa-1f6577a54a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from scipy.spatial.distance import cosine\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@retry(wait=wait_random_exponential(multiplier=1, min=1, max=60), stop=stop_after_attempt(6))\n",
    "def embedding(texts:List[str], doc_embedding:bool=True): # Andrew mentioned that the prompt/ completion paradigm is preferable for this class\n",
    "    # help: https://platform.openai.com/docs/api-reference/embeddings\n",
    "    model = Parameters.EMBEDDING_MODEL # 'text-embedding-ada-002' # https://platform.openai.com/docs/models/embeddings\n",
    "    # response = openai.Embedding.create(\n",
    "    #     model=model,\n",
    "    #     input = texts\n",
    "    # )\n",
    "    # emds = [data['embedding'] for data in response.data]\n",
    "    tic = time.time()\n",
    "    embed_func = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY) # https://python.langchain.com/en/latest/modules/models/text_embedding/examples/openai.html\n",
    "    if doc_embedding:\n",
    "        emds = embed_func.embed_documents(texts)\n",
    "    else:\n",
    "        emds = embed_func.embed_query(texts)\n",
    "    toc = time.time()\n",
    "    print(f\"Time consumed for embedding {len(texts):,d} chunks is: {toc-tic:,.1f} seconds\")\n",
    "    return np.array(emds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89cca96c-74db-4c1e-8f58-dbb74fded091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get similarity matrix between the embeddings of the chunk summaries\n",
    "def get_text_similarity(text_embeds, bonus_constant:float=.25, bonus_power:float=1.):\n",
    "    # import ipdb; ipdb.set_trace()\n",
    "    num_1_chunks = text_embeds.shape[0]\n",
    "    text_similarity_matrix = np.zeros((num_1_chunks, num_1_chunks))\n",
    "    text_similarity_matrix[:] = np.nan\n",
    "    \n",
    "    for row in range(num_1_chunks):\n",
    "        for col in range(row, num_1_chunks):\n",
    "            # Calculate cosine similarity between the two vectors\n",
    "            similarity = 1- cosine(text_embeds[row], text_embeds[col])\n",
    "            text_similarity_matrix[row, col] = similarity\n",
    "            text_similarity_matrix[col, row] = similarity\n",
    "    \n",
    "    # # Draw a heatmap with the text_similarity_matrix\n",
    "    # plt.figure()\n",
    "    # plt.title('Non adjusted Similarity matrix')\n",
    "    # # Color scheme blues\n",
    "    # plt.imshow(text_similarity_matrix, cmap = 'Blues')\n",
    "\n",
    "    # Get the \"distance-adjusted\" similarity matrix\n",
    "    proximity_bonus_arr = np.zeros_like(text_similarity_matrix)\n",
    "    # Closer neighbors get higher bonus\n",
    "    for row in range(proximity_bonus_arr.shape[0]):\n",
    "        for col in range(proximity_bonus_arr.shape[1]):\n",
    "            if row == col:\n",
    "                proximity_bonus_arr[row, col] = 0\n",
    "            else:\n",
    "                proximity_bonus_arr[row, col] = 1/(abs(row-col))**bonus_power * bonus_constant # Closer neighbors get higher bonus\n",
    "    dist_adj_text_similarity_matrix = text_similarity_matrix.copy() + proximity_bonus_arr # closer neighbors, even if had same similarity-score, should be regarded as `more similiar`. That's the meaning of \"bonus\"\n",
    "    # plt.figure()\n",
    "    # plt.title('Adjusted matrix via the temporal structure')\n",
    "    # plt.imshow(dist_adj_text_similarity_matrix, cmap = 'Blues')\n",
    "    return text_similarity_matrix, dist_adj_text_similarity_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9d067a-1bce-4576-82d1-72d5a276fdeb",
   "metadata": {},
   "source": [
    "## Get Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a17d4b7-10fa-48be-bc8d-90995ca0ba9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_topics_via_topics_title(chunks:List[str], topics_title:List[List[int]]):\n",
    "    topics = []\n",
    "    for t in topics_title:\n",
    "        topics.append(join_texts([chunks[i] for i in t]))\n",
    "    print('Leng of topics, in words:', [strlen(topic) for topic in topics], \"in tokens:\", [get_num_tokens(topic) for topic in topics])\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45781bee-2f15-4e6e-b4f6-35c1bdaad360",
   "metadata": {},
   "source": [
    "## Similarity based Cluster - Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "140e89c0-25c2-495d-b238-e43966c2fc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from networkx.algorithms import community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "041d38c3-7b93-4e93-b6ed-5021885460ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap(list_data:list, title:str):\n",
    "    # The list_data should be something like: [0, 0, 0, 1, 1, 2, 2, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 5]\n",
    "    data = np.array(list_data).reshape(1, -1)\n",
    "    plt.figure(figsize = (10, 4))\n",
    "    plt.title(title)\n",
    "    plt.imshow(data, cmap = 'tab20')\n",
    "    for i in range(1, len(list_data)):\n",
    "        plt.axvline(x = i - 0.5, color = 'black', linewidth = 0.5)\n",
    "\n",
    "def _get_longest_topic_length(topics_title:List[List[int]], chunks:List[str]):\n",
    "    _topics_length = []\n",
    "    _topics_word_num = []\n",
    "    for i, topic_title in enumerate(topics_title):\n",
    "        _topics_length.append(sum([get_num_tokens(chunks[item]) for item in topic_title]))\n",
    "        _topics_word_num.append(sum([strlen(chunks[item]) for item in topic_title]))\n",
    "    longest_topic_length = max(_topics_length)\n",
    "    longest_topic_word_num = max(_topics_word_num)\n",
    "    index0 = _topics_length.index(longest_topic_length)\n",
    "    index1 = _topics_word_num.index(longest_topic_word_num)\n",
    "    return index0, longest_topic_length, index1, longest_topic_word_num\n",
    "\n",
    "# Run the community detection algorithm to get something like: [{0, 1, 2, 3,}, {4, 5}, {6, 7, 8}, {9, 10, 11}]\n",
    "def get_topics_title(text_embeds, num_topics:int, chunks:List[str], bonus_constant:float=0.25, min_size:int=3, \n",
    "               bonus_power:float=1., resolution:float=1., resolution_step:float=.1):\n",
    "\n",
    "    longest_chunk_size = max([get_num_tokens(chunk) for chunk in chunks]) # The num of tokens of the longest chunk\n",
    "    _, similarity_matrix = get_text_similarity(text_embeds)\n",
    "\n",
    "    title_nx_graph = nx.from_numpy_array(similarity_matrix)\n",
    "\n",
    "    desired_num_topics = num_topics\n",
    "    # Store the accepted partitionings\n",
    "    topics_title_accepted = []\n",
    "\n",
    "    # Find the resolution that gives the desired number of topics\n",
    "    topics_title = []\n",
    "    idx = 1\n",
    "    lower_bar = desired_num_topics -1\n",
    "    upper_bar = lower_bar + 5\n",
    "    print(f\"number of desired topics, lower_bar = {lower_bar:,d}, upper_bar = {upper_bar:,d}\")\n",
    "    longest_topic_length = 9e9 # TODO\n",
    "    num_retry_split_large_chunk = 0\n",
    "    while len(topics_title) not in range(lower_bar, upper_bar) or longest_topic_length>max(longest_chunk_size, Parameters.SUMMARIZE_TOPIC_TOKENS):\n",
    "        if len(topics_title) >= upper_bar:\n",
    "            old_resolution, old_resolution_step = resolution, resolution_step\n",
    "            if random.random() < 0.9:\n",
    "                resolution *= 0.9\n",
    "                if resolution_step > 1e-3:\n",
    "                    resolution_step *= 0.9\n",
    "            else:\n",
    "                resolution *= 0.5\n",
    "                if resolution_step > 1e-3:\n",
    "                    resolution_step *= 0.9\n",
    "            print(f\"Adjusted resolution from {old_resolution:.4f} to {resolution:.4f}, resolution_step from {old_resolution_step:.4f} to {resolution_step:.4f}, at step: {idx:,d}, because we have {len(topics_title):,d} topics which is >= upper_bar ({upper_bar:,d}).\")\n",
    "        topics_title = community.louvain_communities(title_nx_graph, weight = 'weight', resolution = resolution)\n",
    "        topics_title = [sorted(topic_title) for topic_title in topics_title] # Make the topic items sorted from: [{19, 20, 22, 21}, {24, 23}] to [{19, 20, 21, 22}, {23, 24}]\n",
    "        # print(\"Sorted: \", topics_title)\n",
    "        resolution += resolution_step\n",
    "        if idx % 100 == 0:\n",
    "            # import ipdb; ipdb.set_trace()\n",
    "            print(f\"idx = {idx:04d}, len(topics_title) = {len(topics_title):d}, resolution = {resolution:.4f}\")\n",
    "        idx += 1\n",
    "        _, longest_topic_length, _, _  = _get_longest_topic_length(topics_title, chunks)\n",
    "        if len(topics_title) in range(lower_bar, upper_bar) and longest_topic_length > Parameters.SUMMARIZE_TOPIC_TOKENS:\n",
    "            old_resolution = resolution\n",
    "            resolution = random.normalvariate(old_resolution, old_resolution*.1)\n",
    "            print(f\"Adjusted resolution from {old_resolution:.4f} to {resolution:.4f}, at step: {idx:,d}, because, though topics number in range, we have longest_topic_length as: {longest_topic_length:,d}, \\\n",
    "            which is > Parameters.SUMMARIZE_TOPIC_TOKENS ({Parameters.SUMMARIZE_TOPIC_TOKENS:,d}).\")\n",
    "            if num_retry_split_large_chunk > 2: # Increase the num of topics to split if the larget one can be split after a few times try\n",
    "                upper_bar += 1\n",
    "                print(f\"Adjusted upper_bar from: {upper_bar-1:,d} to {upper_bar} because we have tried {num_retry_split_large_chunk:,d} times to split the largets topic smaller. The new upper_bar={upper_bar:,d}.\")\n",
    "                num_retry_split_large_chunk = 0\n",
    "            else:\n",
    "                num_retry_split_large_chunk += 1\n",
    "\n",
    "    def _get_topics(topics_title:list):\n",
    "        topic_id_means = [sum(e)/len(e) for e in topics_title]\n",
    "        # Arrange title_topics in order of topic_id_means\n",
    "        topics_title = [list(c) for _, c in sorted(zip(topic_id_means, topics_title), key = lambda pair: pair[0])]\n",
    "        for t in topics_title:\n",
    "            t.sort()\n",
    "        # Create an array denoting which topic each chunk belongs to\n",
    "        chunk_topics = [None] * similarity_matrix.shape[0]\n",
    "        for i, c in enumerate(topics_title):\n",
    "            for j in c:\n",
    "                chunk_topics[j] = i\n",
    "        return {'chunk_topics': chunk_topics, 'topics': topics_title}\n",
    "    \n",
    "    plot_heatmap(_get_topics(topics_title)['chunk_topics'], title='Chunk similarity based Topics')\n",
    "    print(f\"We have totally {len(topics_title):,d} topics detected: {topics_title}\")\n",
    "    print(f\"resolution = {resolution:.4f}, resolution_step = {resolution_step:.4f}\")\n",
    "    longest_topic_index, longest_topic_tokens, longest_word_topic_index, longest_topic_word_num = _get_longest_topic_length(topics_title, chunks)\n",
    "    if (longest_topic_index == longest_word_topic_index):\n",
    "        print(f\"Longest topic contains: {longest_topic_tokens:,d} tokens ({longest_topic_word_num:,d} words), at topic No. {longest_topic_index:,d}, starting from index 0.\")\n",
    "    else:\n",
    "        print(f\"Longest topic contains: {longest_topic_tokens:,d} tokens at: No. {longest_topic_index:,d} ({longest_topic_word_num:,d} words at No. {longest_word_topic_index:,d}), starting from index 0.\")\n",
    "    return topics_title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4dac77-d7d8-4985-b53e-914828415f16",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Get Topic Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15716678-9d15-4769-b654-20a95b275872",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_topic_groups(topics:List[str]):\n",
    "    topic_groups = []\n",
    "    topic_group_length = 0\n",
    "    _topic_group = []\n",
    "    total_tokens = get_num_tokens(join_texts(topics))\n",
    "    intended_num_of_topic_groups = total_tokens // Parameters.SUMMARIZE_TOPIC_TOKENS + 1\n",
    "    MAX_TOPIC_GROUP_LENGTH = min(Parameters.SUMMARIZE_TOPIC_TOKENS, total_tokens // intended_num_of_topic_groups + 1)\n",
    "    i = 0\n",
    "    while i < len(topics):\n",
    "        if get_num_tokens(topics[i]) > Parameters.SUMMARIZE_TOPIC_TOKENS:\n",
    "            raise RuntimeError\n",
    "        while i<len(topics) and topic_group_length + get_num_tokens(topics[i]) <= MAX_TOPIC_GROUP_LENGTH:\n",
    "            _topic_group.append(topics[i])\n",
    "            topic_group_length += get_num_tokens(topics[i])\n",
    "            i += 1\n",
    "        topic_groups.append(join_texts(_topic_group))\n",
    "        _topic_group, topic_group_length = [], 0\n",
    "\n",
    "    if len(topic_groups)>=2 and get_num_tokens(topic_groups[-2]) + get_num_tokens(topic_groups[-1]) < Parameters.SUMMARIZE_TOPIC_TOKENS:\n",
    "        topic_groups[-2] = join_texts(topic_groups[-2:])\n",
    "        topic_groups = topic_groups[:-1]\n",
    "    words_topic_groups = [strlen(tg) for tg in topic_groups]\n",
    "    tokens_topic_groups = [get_num_tokens(tg) for tg in topic_groups]\n",
    "    print(\"Length of topic groups, in words:\", words_topic_groups, \"in tokens:\", tokens_topic_groups, f\" The longest one has: {max(words_topic_groups):,d} words ({max(tokens_topic_groups):,d} tokens)\")\n",
    "    return topic_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c401499-6720-447d-a47d-bde13e808b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7118, 5818, 8729]\n"
     ]
    }
   ],
   "source": [
    "aaa = [7118, 5818, 7731, 998]\n",
    "if len(aaa)>=2 and aaa[-2] + aaa[-1] < Parameters.SUMMARIZE_TOPIC_TOKENS:\n",
    "    aaa[-2] = sum(aaa[-2:])\n",
    "    aaa = aaa[:-1]\n",
    "print(aaa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedced3c-0751-49a9-a85d-e8767068891e",
   "metadata": {},
   "source": [
    "# Summarize each topic group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c81bd5a1-c9ec-4dc3-9ab3-fdc90678425c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_topic_groups(topic_groups:List[str], summary_length:int=Parameters.TOPIC_SUMMARY_LENGTH, batch_size:int=8, temperature:float=0.)->List[str]:\n",
    "    tic = time.time()\n",
    "    phase_2_summaries = batch_summary(topic_groups, summary_length=summary_length, temperature=temperature, batch_size=batch_size)\n",
    "    toc = time.time()\n",
    "    print(\"The length of summarized-topic-groups, in words:\", [strlen(summary) for summary in phase_2_summaries], \"in tokens:\", [get_num_tokens(summary) for summary in phase_2_summaries], \"Summarized total words:\", sum([strlen(summary) for summary in phase_2_summaries]), f\"(total tokens: {sum([get_num_tokens(summary) for summary in phase_2_summaries]):,d})\")\n",
    "    print(f\"Time consumed for topic groups summarization: {toc-tic:,.1f}s\")\n",
    "    return phase_2_summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8819e216-c667-4d59-860f-61dd2081a750",
   "metadata": {},
   "source": [
    "# Final Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606be5b0-6ef4-4450-8eb8-89ad3337daeb",
   "metadata": {},
   "source": [
    "## [Call OpenAI Functions](https://platform.openai.com/docs/guides/gpt/function-calling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de440bde-cc25-46b3-b2cb-04300178e470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_summary = \"\"\"In this article, the author discusses the use case for topic modeling and how it can be applied to various types of content, such as YouTube videos, podcasts, meeting notes, legal documents, movie scripts, books, and lecture notes. The author highlights the manual work involved in extracting topics from these sources and emphasizes the value of structured data in these contexts.\\n\\nThe author suggests that there is an opportunity to create a productionized service for extracting topics from podcasts and videos. They provide an example of a podcast website that does not have topics listed on their episodes, and propose the idea of offering a service to extract and provide topics for these episodes. This approach could be replicated for other podcasts, videos, or any series of information.\\n\\nThe author then introduces their tutorial on topic modeling, specifically focusing on a two-pass approach. In the first pass, they use a mapreduce method to process the entire document and extract topics and bullet points. They explain that this approach may be a bit expensive in terms of computational resources, but it allows for a comprehensive analysis of the text. In the second pass, they iterate through each topic bullet point and expand on them using a retrieval method. This involves a question and answer-like process to provide more detailed information and context.\\n\\nThe author makes several assumptions, including the absence of a table of contents or contents, and the desire for more control over the topic modeling process. They emphasize the importance of learning the ins and outs of building with AI and encourage readers to experiment with their own use cases.\\n\\nThe article then delves into the technical implementation of the topic modeling process. The author imports various packages and sets up two language models, GPT 3.5 Turbo and GPT 4. They load the transcript that will be parsed and split it into chunks using a recursive character text splitter. The author explains the reasoning behind the chunk size and overlap parameters, highlighting the need for experimentation based on individual use cases.\\n\\nNext, the author focuses on extracting topic titles and short descriptions. They create a custom prompt to instruct the language model on the desired output. They provide examples and formatting guidelines to ensure accurate extraction of topics. The author then runs the mapreduce method using the GPT 4 language model and consolidates the results to eliminate duplicates.\\n\\nThe article continues with the conversion of the extracted topics into structured data. The author defines a schema with properties for topic name, description, and tag. They demonstrate the structured data output for the extracted topics, showcasing the potential value of this approach.\\n\\nMoving on to the second pass of topic expansion, the author introduces the concept of using a retrieval method and the vector store dance. They explain that this approach allows for generating longer descriptions or summaries based on specific topics. The author sets up a custom prompt for this process and creates embeddings using the OpenAI embeddings engine. They initialize Pinecone, a remote vector store, and set up the retrieval QA process.\\n\\nThe author demonstrates how to iterate through the structured topics and generate expanded summaries using the retrieval method. They provide examples of the expanded topics, showcasing the additional information and context that can be obtained through this process.\\n\\nFinally, the author explores the topic of extracting chapters or timestamps from a transcript. They create a custom prompt for this purpose and use the retrieval QA process to find the first timestamp associated with each topic. The author provides an example of the retrieved timestamps and highlights the relevance of this information in organizing and navigating through content.\\n\\nIn conclusion, the article presents a comprehensive overview of topic modeling and its applications in various contexts. The author provides step-by-step instructions and technical details for implementing a two-pass approach to extract and expand on topics. They emphasize the value of structured data and the potential for creating productionized services based on topic modeling.\"\"\"\n",
    "# num_highlights = 5\n",
    "# func_name = \"output_key_information\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c01e2fa5-d3ea-4eb6-b649-788a2ad2c6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_with_openai_function(to_summary:str, num_highlights:int, temperature:float=0.):\n",
    "    tic = time.time()\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": to_summary}]\n",
    "    num_tokens = _num_tokens_from_messages(messages, model=Parameters.COMPLETION_MODEL)\n",
    "    if num_tokens > Parameters.PHASE_1_CONTEXT_LENGTH-1300:\n",
    "        model = Parameters.COMPLETION_MODEL\n",
    "        context_length = Parameters.CONTEXT_LENGTH\n",
    "    else:\n",
    "        model = Parameters.PHASE_1_MODEL\n",
    "        context_length = Parameters.PHASE_1_CONTEXT_LENGTH\n",
    "        \n",
    "    print(f\"Number of tokens for final highlight: {num_tokens:,d} (words: {strlen(to_summary):,d}). Highlight model: {model}\")\n",
    "\n",
    "    functions = [\n",
    "        {\n",
    "            \"name\": \"output_key_information\",\n",
    "            \"description\": \"Output key information including: title, short synopsis and bulleted key points from the given text.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"Title\": {\"type\": \"string\", \"description\": \"The title of the given text\"},\n",
    "                    \"Short\\ Synopsis\": {\"type\": \"string\", \"description\": \"A very short and concise summary of the given text.\"},\n",
    "                    \"Key\\ Points\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": f\"A list of maximum of {num_highlights} succinct bullet points, each prefixed by a proper emoji.\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"Title\", \"Short\\ Synopsis\", \"Key\\ Points\"],\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages, \n",
    "        temperature=temperature,\n",
    "        functions=functions,\n",
    "        function_call=\"auto\",  # auto is default, but we'll be explicit\n",
    "    )\n",
    "    \n",
    "    usage = response.get('usage')\n",
    "    if usage:\n",
    "        global openai_usages\n",
    "        if 'openai_usages' not in globals():\n",
    "            openai_usages = []\n",
    "        usage = dict(usage)\n",
    "        usage[\"model\"] = response.get('model')\n",
    "        usage[\"object\"] = response.get('object')\n",
    "        openai_usages.append(usage)\n",
    "    response_message = response[\"choices\"][0][\"message\"]\n",
    "    \n",
    "    if response_message.get(\"function_call\"):\n",
    "        try:\n",
    "            function_args = json.loads(response_message.get(\"function_call\").get(\"arguments\"))\n",
    "        except Exception as e:\n",
    "            import ipdb; ipdb.set_trace()\n",
    "            print(response_message)\n",
    "            print(f\"An error occurs: {e}\")\n",
    "            \n",
    "    # for key in function_args.keys():\n",
    "    #     print(f\"{key}: {function_args.get(key)}\")\n",
    "        \n",
    "    toc = time.time()\n",
    "    print(f\"Time consumed for final highlight: {toc-tic:.1f}s\")\n",
    "    return function_args, response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d732fd5-d7c4-4e33-992f-6c9269da113d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_highlight(final_summary:str, num_highlights:int=7, temperature:float=0.):\n",
    "    final_result, response = highlight_with_openai_function(final_summary, num_highlights=num_highlights, temperature=temperature)\n",
    "\n",
    "    print(\"Final highlight:\")\n",
    "    for k in final_result.keys():\n",
    "        print(f\"\\t{k}: {strlen(final_result.get(k)):,d} words ({get_num_tokens(final_result.get(k)):,d} tokens)\")\n",
    "    print(\"OpenAI usage for final highlight:\")\n",
    "    for k, v in dict(response.usage).items():\n",
    "        print(f\"\\t{k}: {v:,d}\")\n",
    "\n",
    "    final_highlights = '\\n\\n'.join([\"#### \" + k + f\" ({strlen(v):,d} words)\" + \"\\n\" + v for k, v in final_result.items()])\n",
    "\n",
    "    return final_highlights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1307bcd6-79f1-4213-bfd7-d8e7f6e6c46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_highlights = get_final_highlight(to_summary, num_highlights=5, temperature=.2)\n",
    "\n",
    "# print(final_highlights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07153d4c-3f49-4b29-9461-a0cc1f69d913",
   "metadata": {},
   "source": [
    "## Normal way to do final summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c5af39bd-80fb-4eeb-94be-0909a80b125d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_result(topic_groups_summarizations:List[str], num_highlights:int=7, temperature:float=0.):\n",
    "    to_summary = [join_texts(topic_groups_summarizations)]\n",
    "    num_words = strlen(to_summary[0])\n",
    "    num_tokens = get_num_tokens(to_summary[0])\n",
    "    print(f\"To summary text length: {num_words:,d} (num of tokens: {num_tokens:,d})\")\n",
    "    # import ipdb; ipdb.set_trace()\n",
    "    tic = time.time()\n",
    "    if num_tokens < Parameters.PHASE_1_CONTEXT_LENGTH-1000:\n",
    "        summary_model = Parameters.PHASE_1_MODEL\n",
    "        summary_context_length = Parameters.PHASE_1_CONTEXT_LENGTH\n",
    "    else:\n",
    "        summary_model = Parameters.COMPLETION_MODEL\n",
    "        summary_context_length = Parameters.CONTEXT_LENGTH\n",
    "\n",
    "    # highlight_model = Parameters.PHASE_1_MODEL\n",
    "    # highlight_context_length = Parameters.PHASE_1_CONTEXT_LENGTH\n",
    "    final_summary = batch_summary(to_summary, temperature=temperature, batch_size=1, summary_length=Parameters.FINAL_SUMMARY_LENGTH, model=summary_model, context_length=summary_context_length)[0]\n",
    "    toc = time.time()\n",
    "    print(f\"Time consumed for final summarization: {toc-tic:,.1f}s\")\n",
    "    print(f\"To highlight text length: {strlen(final_summary):,d} (num of tokens: {get_num_tokens(final_summary):,d})\")\n",
    "    # tic = time.time()\n",
    "\n",
    "    final_highlight =  get_final_highlight(final_summary, temperature=temperature, num_highlights=num_highlights) # batch_highlights(final_summary, temperature=temperature, num_highlights=num_highlights, model=highlight_model, context_length=highlight_context_length)\n",
    "    # toc = time.time()\n",
    "    # print(f\"Time consumed for final higlights: {toc-tic:,.1f}s\")\n",
    "    final_result = final_highlight + f\"\\n\\n#### Detailed Summary ({strlen(final_summary):,d} words)\\n{final_summary}\"\n",
    "    print(f\"Length of final highlight, in words: {strlen(final_highlight):,d}, in tokens: {get_num_tokens(final_highlight):,d}\")\n",
    "    print(f\"Length of final summary, in words: {strlen(final_summary):,d}, in tokens: {get_num_tokens(final_summary):,d}\")\n",
    "    print(f\"Length of final result, in words: {strlen(final_result):,d}, in tokens: {get_num_tokens(final_result):,d}\")\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c647856c-8dd6-48f2-910a-e1507081f6e5",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e14bfda-36a1-4324-91b9-be1d7b161650",
   "metadata": {},
   "source": [
    "## do_summary method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "31983b6e-9556-498a-8d30-12b3aada3997",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_summary(txt:str, output_file:str, num_highlights:int=7):\n",
    "    # import ipdb; ipdb.set_trace()\n",
    "    txt = re.sub(r'[ ]+', ' ', txt)\n",
    "    num_txt_words = strlen(txt)\n",
    "    print(f\"Num of words: {num_txt_words:,d}\")\n",
    "    num_txt_tokens = get_num_tokens(txt)\n",
    "    print(f\"Num of tokens: {num_txt_tokens:,d}\")\n",
    "    Parameters.PHASE_1_CHUNK_LEN = max(Parameters.PHASE_1_CHUNK_LEN, min(2000, num_txt_tokens // (Parameters.batch_size+1)))\n",
    "\n",
    "    if num_txt_tokens < Parameters.SUMMARIZE_TOPIC_TOKENS:\n",
    "            final_result = get_final_result([txt], num_highlights=num_highlights, temperature=0.)\n",
    "    else:\n",
    "        # Get initial chunks\n",
    "        phase_1_chunk_list = get_phase_1_chunks(txt)\n",
    "\n",
    "        # Clean up text via OpenAI API\n",
    "        tic = time.time()\n",
    "        cleaned_chunk_list0 = batch_cleanup(chunks=phase_1_chunk_list, temperature=0., batch_size=Parameters.batch_size)\n",
    "        toc = time.time()\n",
    "        print(\"Length of cleaned chunks, in words:\", [strlen(c) for c in cleaned_chunk_list0], \"in tokens:\", [get_num_tokens(c) for c in cleaned_chunk_list0])\n",
    "        print(f\"Time consumed for cleanup by OpenAI: {toc-tic:,.1f}s\")\n",
    "\n",
    "        overlapped = Parameters.PHASE_1_CHUNK_OVERLAY\n",
    "\n",
    "        while (get_num_tokens(join_texts(cleaned_chunk_list0)) > Parameters.SUMMARIZE_TOPIC_TOKENS): # Keep the process until the to_summary_text is short enough\n",
    "            # Get chunks\n",
    "            chunks, cleaned_chunk_list = get_chunks(cleaned_chunk_list0, overlapped=overlapped)\n",
    "\n",
    "            # Get embedding for the cunks\n",
    "            chunks_embedding = embedding(chunks)\n",
    "\n",
    "            # Get topic titles (something like: [[0, 1, 2, 3,], [4, 5], [6, 7, 8], [9, 10, 11]])\n",
    "            topics_title = get_topics_title(text_embeds=chunks_embedding, num_topics=strlen(txt) // Parameters.AVERAGE_TOPIC_WORDS, chunks=chunks, )\n",
    "\n",
    "            # Get topics and topic_groups\n",
    "            topics = form_topics_via_topics_title(chunks, topics_title)\n",
    "            topic_groups = get_topic_groups(topics)\n",
    "\n",
    "            # Get summarizations of each group via OpenAI API\n",
    "            phase_2_summaries = summarize_topic_groups(topic_groups, summary_length=Parameters.TOPIC_SUMMARY_LENGTH, temperature=0.2, batch_size=Parameters.batch_size)\n",
    "            cleaned_chunk_list0 = phase_2_summaries\n",
    "            overlapped=0\n",
    "\n",
    "        # Final result\n",
    "        final_result = get_final_result(cleaned_chunk_list0, num_highlights=num_highlights, temperature=0.2)\n",
    "\n",
    "    # Output final result\n",
    "    print('\\n\\n')\n",
    "    print(final_result)\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(final_result)\n",
    "\n",
    "    usages = openai_usage_stat()\n",
    "    print(\"\\n#### OpenAI Usage:\")\n",
    "    print(\"Token Usage:\", usages.get('TokenUsage'))\n",
    "    print(f\"Cost: {usages.get('Dollar_Usage')*100:,.2f} US cents\")\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8997329-3b27-4ee8-954f-ad321563cb36",
   "metadata": {},
   "source": [
    "## Different data sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b165ff1e-9814-442b-9007-de2052ff900d",
   "metadata": {},
   "source": [
    "### Youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e7631365-024b-4933-9019-a20458753334",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_youtube_summary(video_id:str, num_highlights:int=7, output_file:str=None, start_pos:int=None, end_pos:int=None):\n",
    "    if not output_file:\n",
    "        output_file = f\"results/youtube_{video_id}.txt\"\n",
    "    # Get text\n",
    "    txt = get_transcript(video_id)\n",
    "    # Do summary\n",
    "    final_result = do_summary(txt[start_pos:end_pos], output_file=output_file, num_highlights=num_highlights)\n",
    "    \n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb8c29c-5362-4e71-9441-dbc49e4f32fb",
   "metadata": {},
   "source": [
    "### Bilibili\n",
    "\n",
    "[https://github.com/huilongyeo/bilibiliGetSrt](https://github.com/huilongyeo/bilibiliGetSrt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "75c08346-d2d2-4852-a41f-e458d83f5bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Bilibili:\n",
    "#     def __init__(self, video_url:str):\n",
    "#         self.video_url = video_url\n",
    "#         pass\n",
    "    \n",
    "#     def get_subtitles(self):\n",
    "#     \"\"\"\n",
    "#     根据字幕的json 获取CC字幕\n",
    "\n",
    "#     \"\"\"        \n",
    "#     json_url=self.get_json_url()\n",
    "#     if len(json_url)>0:\n",
    "#         with open('{}.srt'.format(self.title),'w',encoding='utf-8') as f:\n",
    "#             r=requests.get(json_url)\n",
    "#             info=json.loads(r.text)['body']\n",
    "#             for i in range(len(info)):\n",
    "#                 subtitle_from=info[i]['from']\n",
    "#                 subtitle_to=info[i]['to']\n",
    "#                 content=info[i]['content']\n",
    "#                 data=self.format_subtitle(subtitle_from,subtitle_to,content,i)\n",
    "#                 f.write(data)\n",
    "\n",
    "#     def format_subtitle(self,subtitle_from,subtitle_to,content,i):        \n",
    "#         \"\"\"\n",
    "#         格式化成srt文件，形如：\n",
    "\n",
    "#         1\n",
    "#         00:00:01,035 --> 00:00:04,525\n",
    "#         远离了平行线 看吧天气预报也不怎么准\n",
    "\n",
    "#         \"\"\"\n",
    "#         subtitle_from=round(subtitle_from,3)                                              #四舍五入为三位小数\n",
    "#         subtitle_to=round(subtitle_to,3)\n",
    "#         begin=time.strftime(\"%H:%M:%S\",time.gmtime(subtitle_from))+','+self.rectify(subtitle_from)        \n",
    "#         end=time.strftime(\"%H:%M:%S\",time.gmtime(subtitle_to))+','+self.rectify(subtitle_to)\n",
    "#         data=str(i+1)+'\\n'+begin+' --> '+end+'\\n'+content+'\\n\\n'                          #格式化成srt字幕\n",
    "#         return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f225cdd-4a3a-4b7c-8e5c-f0028e58e6a7",
   "metadata": {},
   "source": [
    "### Text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c71e0689-c4ad-458c-b324-c7b521bcd960",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_txtfile_summary(txt_path:str, num_highlights:int=7, output_file:str=None, start_pos:int=None, end_pos:int=None):\n",
    "    if not output_file:\n",
    "        output_file = re.sub(r\".*/\", \"\", txt_path)\n",
    "        output_file = \"results/text_\" + re.sub(r\"\\.[^.]*$\", \"\", output_file) + \".txt\"\n",
    "    # Get text\n",
    "    txt = get_txt_from_file(txt_path)\n",
    "    # import ipdb; ipdb.set_trace()\n",
    "    # Do summary\n",
    "    final_result = do_summary(txt[start_pos:end_pos], output_file=output_file, num_highlights=num_highlights)\n",
    "    \n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389c5d8f-756c-4115-94ad-92de2475af9d",
   "metadata": {},
   "source": [
    "### PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a37eeabf-4cd4-49f4-a384-7ad56ba4d675",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import sys\n",
    "!{sys.executable} -m pip install PyPDF2 requests # aspose-words\n",
    "\n",
    "import requests\n",
    "from PyPDF2 import PdfReader\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def open_pdf(pdf_resource:str, startPage:int=1, endPage:int=None):\n",
    "    # Check if the input name is a URL or a local filename\n",
    "    parsed_url = urlparse(pdf_resource)\n",
    "    is_url = parsed_url.scheme != '' and parsed_url.netloc != ''\n",
    "\n",
    "    if is_url:\n",
    "        # The input name is a URL\n",
    "        response = requests.get(pdf_resource)\n",
    "        pdf_content = response.content\n",
    "\n",
    "        # Create a temporary PDF file from the URL content\n",
    "        pdf_file = open('temp.pdf', 'wb')\n",
    "        pdf_file.write(pdf_content)\n",
    "        pdf_file.close()\n",
    "\n",
    "        # Open the PDF file using PyPDF2\n",
    "        pdf = PdfReader('temp.pdf')\n",
    "        # Access the PDF document properties or perform other operations\n",
    "        pages = pdf.pages\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "        print(f\"The PDF has {len(pages):,d} pages.\")\n",
    "\n",
    "        # Clean up the temporary PDF file\n",
    "        import os\n",
    "        os.remove('temp.pdf')\n",
    "    else:\n",
    "        # The input name is a local filename\n",
    "        pdf = PdfReader(pdf_resource)\n",
    "        # Access the PDF document properties or perform other operations\n",
    "        pages = pdf.pages\n",
    "        print(f\"The PDF has {len(pages):,d} pages.\")\n",
    "    \n",
    "    txt = ''.join([c.extract_text() for c in pages[startPage-1:endPage]])\n",
    "    return pdf, txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3edbd659-99f9-4d89-bd2f-9fbb26073da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_pdf_summary(pdf_source:str, pdf_name:str, startPage:int=1, endPage:int=None, num_highlights:int=7, output_file:str=None, start_pos:int=None, end_pos:int=None):\n",
    "    if not output_file:\n",
    "        output_file = re.sub(r\".*/\", \"\", pdf_source)\n",
    "        output_file = \"results/pdf_\" + re.sub(r\"\\.[^.]*$\", \"\", output_file) + \".txt\"\n",
    "    if pdf_name:\n",
    "        output_file = f\"\"\"results/pdf_{re.sub(\"/\", \"_\", pdf_name)}.txt\"\"\"\n",
    "    # Get text\n",
    "    pdf,  txt = open_pdf(pdf_source, startPage, endPage)\n",
    "    print(f\"We have {strlen(txt):,d} words ({get_num_tokens(txt):,d}) tokens in the PDF file.\")\n",
    "    # import ipdb; ipdb.set_trace()\n",
    "    # Do summary\n",
    "    final_result = do_summary(txt[start_pos:end_pos], output_file=output_file, num_highlights=num_highlights)\n",
    "    \n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79943f6-c2ba-4c51-a19f-fdacf56ae513",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b350c7d9-9548-4cb1-a8de-298e9bee7ab3",
   "metadata": {},
   "source": [
    "## Text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "644a474f-0913-421a-bc16-1c1349d99790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of words: 784\n",
      "Num of tokens: 1,001\n",
      "To summary text length: 784 (num of tokens: 1,001)\n",
      "batch_summary model = gpt-3.5-turbo-0613\n",
      "Inside _get_completion_func: num_prompt_tokens = 1,124, max_tokens = 2,972, model = gpt-3.5-turbo-0613, context_length = 4,096\n",
      "Time consumed for final summarization: 11.6s\n",
      "To highlight text length: 415 (num of tokens: 454)\n",
      "Number of tokens for final highlight: 462 (words: 415). Highlight model: gpt-3.5-turbo-0613\n",
      "Time consumed for final highlight: 4.4s\n",
      "Final highlight:\n",
      "\tTitle: 5 words (6 tokens)\n",
      "\tShort Synopsis: 37 words (40 tokens)\n",
      "\tKey Points: 85 words (93 tokens)\n",
      "OpenAI usage for final highlight:\n",
      "\tprompt_tokens: 564\n",
      "\tcompletion_tokens: 169\n",
      "\ttotal_tokens: 733\n",
      "Length of final highlight, in words: 141, in tokens: 160\n",
      "Length of final summary, in words: 415, in tokens: 454\n",
      "Length of final result, in words: 561, in tokens: 621\n",
      "\n",
      "\n",
      "\n",
      "#### Title (5 words)\n",
      "Diffusion Modeling in Neural Networks\n",
      "\n",
      "#### Short Synopsis (37 words)\n",
      "Diffusion modeling is a technique used in neural networks to generate new sprites that are not represented in the training data set. It involves adding noise to images and training the network to remove the noise progressively.\n",
      "\n",
      "#### Key Points (85 words)\n",
      "- Diffusion modeling generates new sprites by adding noise to images and training a neural network to remove the noise.\n",
      "- The concept of adding noise is inspired by the diffusion of ink in water.\n",
      "- The neural network is trained to make judgments about the image at different levels of noise.\n",
      "- Training involves reconstructing the original sprite from images that are pure noise.\n",
      "- By training the network in this way, it can generate new sprites not present in the training data set.\n",
      "\n",
      "#### Detailed Summary (415 words)\n",
      "The goal of diffusion models is to generate more sprites that are not represented in the training data set. This is achieved by using a neural network that can generate new sprites through a process called diffusion modeling. To make the training data useful to the neural network, different levels of noise are added to the images. This noise serves to emphasize finer details or general outlines of the sprites.\n",
      "\n",
      "The concept of adding noise to the images is inspired by physics, specifically the diffusion of ink in water. Just as an ink drop diffuses into the water until it disappears, the noise added to the images gradually obscures the original sprite until it becomes unrecognizable. At each level of noise, the neural network is trained to make certain judgments about the image.\n",
      "\n",
      "When the image is still clearly recognizable as the original sprite, the neural network should identify it as such. As more noise is added, the network should recognize that there is some noise present and suggest possible details to make it look more like the original sprite. When only the outline of a sprite is visible, the network should suggest more general details that could belong to any likely sprite. Even when the image appears to be completely noise, the network should still try to make it look more like a sprite by proposing an outline of what a sprite might look like.\n",
      "\n",
      "The process of adding noise progressively to the images is demonstrated through an animation of an ink drop diffusing in water. This visual representation helps to illustrate the concept of diffusion modeling and how the noise gradually obscures the original sprite.\n",
      "\n",
      "Training the neural network involves teaching it to remove the noise that was added to the images. The network starts with images that are pure noise and gradually learns to reconstruct the original sprite. The \"No Idea\" level of noise is particularly important because it is normally distributed, meaning that each pixel is sampled from a normal distribution or a bell-shaped curve. This allows for the generation of new sprites by sampling noise from the normal distribution and using the neural network to remove the noise progressively.\n",
      "\n",
      "By training the neural network in this way, it becomes capable of generating new sprites that were not present in the original training data set. This allows for the expansion of the sprite collection beyond what was initially available. The next video in the series will cover the topic of sampling.\n",
      "\n",
      "#### OpenAI Usage:\n",
      "Token Usage: [{'object': 'chat.completion', 'model': 'gpt-3.5-turbo-0613', 'prompt_tokens': 1686, 'completion_tokens': 623, 'total_tokens': 2309}]\n",
      "Cost: 0.38 US cents\n",
      "\n",
      "Altogether time consumed: 16.1 seconds\n"
     ]
    }
   ],
   "source": [
    "if 'openai_usages' in globals():\n",
    "    del openai_usages\n",
    "\n",
    "tic = time.time()    \n",
    "txt_path = 'data/mls.txt'\n",
    "txt_path = 'data/stateoftheunion.txt'\n",
    "txt_path = 'data/deeplearning.ai/diffusion/01-intuition.txt'\n",
    "\n",
    "final_result = do_txtfile_summary(txt_path, num_highlights=4, start_pos=None, end_pos=None)\n",
    "toc = time.time()\n",
    "print(f\"\\nAltogether time consumed: {toc-tic:,.1f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba93fc73-3154-469d-9492-c2522d6021c0",
   "metadata": {},
   "source": [
    "## Youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "154327f3-e8fc-4977-83ee-c54e0fd27c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of words: 4,644\n",
      "Num of tokens: 5,094\n",
      "To summary text length: 4,644 (num of tokens: 5,094)\n",
      "batch_summary model = gpt-3.5-turbo-16k-0613\n",
      "Inside _get_completion_func: num_prompt_tokens = 5,219, max_tokens = 10,781, model = gpt-3.5-turbo-16k-0613, context_length = 16,000\n",
      "Time consumed for final summarization: 14.4s\n",
      "To highlight text length: 672 (num of tokens: 778)\n",
      "Number of tokens for final highlight: 786 (words: 672). Highlight model: gpt-3.5-turbo-0613\n",
      "Time consumed for final highlight: 7.3s\n",
      "Final highlight:\n",
      "\tTitle: 11 words (11 tokens)\n",
      "\tShort Synopsis: 115 words (127 tokens)\n",
      "\tKey Points: 103 words (130 tokens)\n",
      "OpenAI usage for final highlight:\n",
      "\tprompt_tokens: 888\n",
      "\tcompletion_tokens: 296\n",
      "\ttotal_tokens: 1,184\n",
      "Length of final highlight, in words: 243, in tokens: 289\n",
      "Length of final summary, in words: 672, in tokens: 778\n",
      "Length of final result, in words: 920, in tokens: 1,075\n",
      "\n",
      "\n",
      "\n",
      "#### Title (11 words)\n",
      "The Future of Bitcoin and the Economic War for Wealth Redistribution\n",
      "\n",
      "#### Short Synopsis (115 words)\n",
      "This keynote speech explores the future of Bitcoin and its role in the ongoing economic war for wealth redistribution. It discusses the impact of government policy, technology, and work on this war and emphasizes the importance of finding a way to exit weak currencies in order to preserve wealth. The speaker compares the performance of different investment assets and concludes that holding the best money, such as Bitcoin, is the winning strategy. They highlight the virtues of Bitcoin as a form of money and its potential to demonetize other assets. The speaker urges the audience to focus on spreading the word about Bitcoin and its potential to be a successful and inflation-resistant form of money.\n",
      "\n",
      "#### Key Points (103 words)\n",
      "🔑 Bitcoin's role in the ongoing economic war for wealth redistribution\n",
      "🔑 The three primary drivers of the economic war: government policy, technology, and work\n",
      "🔑 The distribution of global wealth and the value of Bitcoin compared to gold and equities\n",
      "🔑 The impact of inflation on the value of money and the decline of the US dollar\n",
      "🔑 The concept of scarcity in money and the importance of preserving wealth\n",
      "🔑 The performance of different investment assets and the strategy of holding the best money\n",
      "🔑 The virtues of Bitcoin as a form of money and its potential to demonetize other assets\n",
      "\n",
      "#### Detailed Summary (672 words)\n",
      "In this keynote speech, the speaker discusses the future of Bitcoin and its role in the ongoing economic war for wealth redistribution. The speaker emphasizes the three primary drivers of this economic war: government policy, technology, and work. They argue that while hard work is important, it is overshadowed by the power of government policy and technology. The speaker presents a chart showing the distribution of global wealth, with Bitcoin accounting for $400 billion out of $900 trillion. They compare this to the value of gold and equities, highlighting the shift in power from gold to equity due to advancements in technology.\n",
      "\n",
      "The speaker then delves into the concept of inflation and its impact on the value of money. They present a chart showing the decline of the US dollar against consumer goods over the past century, indicating a 95% decrease in purchasing power. They argue that this is a sobering thought and attribute it to the lack of sound money. They compare the decline of the dollar against gold, which has experienced a 99% decrease in value over the same period. The speaker asserts that gold is not a perfect form of money due to its increasing supply and lack of portability.\n",
      "\n",
      "The speaker further explores the concept of scarcity in money and its importance in preserving wealth. They argue that in order to preserve wealth, one must convert currency into an asset that is scarce, desirable, portable, durable, and maintainable. They present a chart showing the collapse of various foreign currencies against the dollar, highlighting the need to exit weak currencies in order to preserve wealth. The speaker emphasizes that no amount of hard work can solve the problem of being on the wrong side of an economic war, and the only strategy is to find a way to exit weak currencies.\n",
      "\n",
      "Next, the speaker discusses the performance of various investment assets and their ability to preserve wealth. They present a chart showing the returns on different asset classes over a 30-year period, with commodities and cash performing poorly. They argue that most investment gains are not true gains, as they simply track the monetary inflation rate. The speaker highlights real estate investment trusts (REITs) as a potentially better option for preserving wealth, but ultimately concludes that the best strategy is to hold the best money.\n",
      "\n",
      "The speaker then delves into the concept of money and its role in storing economic energy. They argue that perfect money is a way to escape the economic war that deprives individuals, families, and companies of their wealth. They compare the virtues of corporate equity and Bitcoin as forms of money. While corporate equity is subject to various risks such as management dilution, labor costs, competition, technology obsolescence, regulation, taxation, and war, Bitcoin avoids these risks through cybernetic control, immutability, and global accessibility.\n",
      "\n",
      "The speaker presents a chart comparing the performance of Bitcoin to other assets over the past two and a half years, highlighting Bitcoin's outperformance. They argue that Bitcoin is the superior asset and will eventually demonetize other store of value assets such as gold, real estate, and bonds. The speaker concludes by urging the audience to focus on what they can change and to spread the word about Bitcoin, as it has the potential to be a successful and inflation-resistant form of money.\n",
      "\n",
      "In summary, the speaker's keynote speech explores the future of Bitcoin and its role in the ongoing economic war for wealth redistribution. They discuss the impact of government policy, technology, and work on this war and emphasize the importance of finding a way to exit weak currencies in order to preserve wealth. The speaker compares the performance of different investment assets and concludes that holding the best money, such as Bitcoin, is the winning strategy. They highlight the virtues of Bitcoin as a form of money and its potential to demonetize other assets. The speaker urges the audience to focus on spreading the word about Bitcoin and its potential to be a successful and inflation-resistant form of money.\n",
      "\n",
      "#### OpenAI Usage:\n",
      "Token Usage: [{'object': 'chat.completion', 'model': 'gpt-3.5-turbo-0613', 'prompt_tokens': 888, 'completion_tokens': 296, 'total_tokens': 1184}, {'object': 'chat.completion', 'model': 'gpt-3.5-turbo-16k-0613', 'prompt_tokens': 5217, 'completion_tokens': 778, 'total_tokens': 5995}]\n",
      "Cost: 2.07 US cents\n",
      "\n",
      "Altogether time consumed: 22.9 seconds\n"
     ]
    }
   ],
   "source": [
    "# if 'openai_usages' in globals():\n",
    "#     del openai_usages\n",
    "\n",
    "# tic = time.time()    \n",
    "# video_id = 'UVn2NroKQCw' # Using LangChain Output Parsers to get what you want out of LLMs\n",
    "# video_id = 'KUDn7bVyIfc' # Converting a LangChain App from OpenAI to OpenSource\n",
    "# video_id = 'uzJX9Wkp0Qc' # The Secrets to Become a Better Software Engineer 6:46\n",
    "# video_id = '6UFtRwWnHws' # Building a LangChain Custom Medical Agent with Memory 17:46\n",
    "# video_id = 'uzJX9Wkp0Qc' # The Secrets to Become a Better Software Engineer 6:46\n",
    "# video_id = 'qaPMdcCqtWk' # 5 Levels Of LLM Summarizing: Novice to Expert 19:18\n",
    "# video_id = 'MPrJF3F4mHc' # Attack and Detect: VulnDC:2 vs Splunk & Security Onion 1:27:45\n",
    "# video_id = 'EmNQuK-E0kI' # What is The Quantum Wave Function, Exactly? 13:04\n",
    "# video_id = 'to7vCdkLi4s' # Reinforcement Learning with Augmented Data (Paper Explained) 22:14\n",
    "# video_id = 'x8pW19wKfXQ' # RWKV: Reinventing RNNs for the Transformer Era (Paper Explained) 1:02:16\n",
    "# video_id = 'ddG2fM9i4Kk' # OpenAssistant RELEASED! The world's best open-source Chat AI! 21:05\n",
    "# video_id = '4Cclp6yPDuw' # Scaling Transformer to 1M tokens and beyond with RMT (Paper Explained) 24:33\n",
    "# video_id = 'Eug2clsLtFs' # Understanding ReACT with LangChain 21:09\n",
    "# video_id = 'ghzsBm8vOms' # What is Platform Engineering and how it fits into DevOps and Cloud world 42:41\n",
    "# video_id = '87ZFvJ7_-n0' # How to build an OpenAPI Specification using YAML? 15:30\n",
    "# video_id = 'mViFmjcDOoA' # OpenAPI 3.0 Tutorial| Swagger Tutorial For Beginners | Design REST API Using Swagger Editor 24:57\n",
    "# video_id = 'z2aCZBAtWXs' # What can you do with 16K tokens in LangChain? 16:54\n",
    "# video_id = 'DiLPn_ldAAQ' # Lecture 21 (update): SHA-3 Hash Function by Christof Paar 1:15:06\n",
    "# video_id = 'aMckXIqqzeI' # How Diffie-Hellman Fails in Practice 1:13:19\n",
    "# video_id = 'HoKDTa5jHvg' # Diffusion Models | Paper Explanation | Math Explained 33:26\n",
    "# video_id = 'yfgfJAkzliw' # Jared Zoneraich - Future Of Prompt Engineering, Management, and Collaboration 24:50\n",
    "# video_id = '9zEXov_L0os' # Hands-on Ransomware: Exploring Cybercrime 43:27\n",
    "# video_id = 'dXxQ0LR-3Hg' # Chat with Multiple PDFs | Langchain app tutorial 1:07:29\n",
    "# video_id = '4KXK6c6TVXQ' # OpenAI Functions + LangChain : Building a Multi Tool Agent 18:51\n",
    "# video_id = 'I4n0Wj2PHQA' # OpenAI's Game Changing Updates. New Features! Bigger Savings! 12:16\n",
    "# video_id = '0lOSvOoF2to' # OpenAI GPT-4 Function Calling: Unlimited Potential 23:48\n",
    "# video_id = 'OdIHUdQ1-eQ' # PyPDF2 Crash Course - Working with PDFs in Python 52:19\n",
    "# video_id = 'a8hMgIcUEnE' # Tagging and Extraction - Classification using OpenAI Functions 16:13\n",
    "# video_id = 'Tkijsu129M0' # GPT-4 solves MIT Exam with 100% ACCURACY 31:04\n",
    "# video_id = 'ut5kp56wW_4' # Tree of Thoughts: Deliberate Problem Solving with Large Language Models (Full Paper Review) 29:28\n",
    "# video_id = 'FcUAbIQH_XY' # 数字秩序 14:58\n",
    "# video_id = 'pEkxRQFNAs4' # Extract Topics From Video/Audio With LLMs 17:33\n",
    "# video_id = '2lnW1PSB2_g' # How to write Tree of Thoughts Prompts 11:35\n",
    "# video_id = '4P-hPldEUiE' # Stable Diffusion Automation: Turbocharge Your AI Image Generation 10:27\n",
    "# video_id = 'g2BRIuln4uc' # Intuition Behind Self-Attention Mechanism in Transformer Networks 39:24\n",
    "# video_id = 'ANszao6YQuM' # Stanford CS230: Deep Learning | Autumn 2018 | Lecture 4 - Adversarial Attacks / GANs 1:22:59\n",
    "# video_id = 'cQYmePtLAT0' # Adversarial Attack and Defense on Deep Learning 3:17\n",
    "# video_id = 'zk-E2NKFjk4' # Introduction to Adversarial Attack on Machine learning model 1:36:55\n",
    "\n",
    "# video_id = '' # \n",
    "# video_id = '' # \n",
    "# video_id = '' # \n",
    "# video_id = '' # \n",
    "# video_id = '3iAaySrjZ4w' # The future of Bitcoin 33:10\n",
    "\n",
    "# final_result = do_youtube_summary(video_id, num_highlights=7)\n",
    "# toc = time.time()\n",
    "# print(f\"\\nAltogether time consumed: {toc-tic:,.1f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd0d297-9b1a-4d56-9bf7-6466d10babe4",
   "metadata": {},
   "source": [
    "## PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "df849c68-c46b-4540-83d3-46c8c43d759a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 'openai_usages' in globals():\n",
    "#     del openai_usages\n",
    "\n",
    "# tic = time.time()    \n",
    "# pdf_resource, pdf_name = \"https://arxiv.org/pdf/2210.03629.pdf\", \"REACT: SYNERGIZING REASONING AND ACTING IN LANGUAGE MODELS\" # Page 33\n",
    "# pdf_resource, pdf_name = \"https://arxiv.org/pdf/2305.13860.pdf\", \"Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study\" # Page 12\n",
    "# pdf_resource, pdf_name, startPage, endPage = \"https://arxiv.org/pdf/2210.16886.pdf\", \"DiffusER: Discrete Diffusion via Edit-Based Reconstruction\", 1, 10 # Page 13\n",
    "# pdf_resource, pdf_name, startPage, endPage = \"https://arxiv.org/pdf/2201.11903.pdf\", \"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\", 1, 19 # Page 43\n",
    "# pdf_resource, pdf_name, startPage, endPage = \"http://hanj.cs.illinois.edu/cs412/bk3/KL-divergence.pdf\", \"Kullback-Leibler Divergence\", 1, 19 # Page 43\n",
    "# pdf_resource, pdf_name, startPage, endPage = \"https://arxiv.org/pdf/1706.03762.pdf\", \"Attention is all you need\", 1, 10 # Page 15\n",
    "\n",
    "# final_result = do_pdf_summary(pdf_resource, pdf_name=pdf_name, startPage=startPage, endPage=endPage, num_highlights=7)\n",
    "# toc = time.time()\n",
    "# print(f\"\\nAltogether time consumed: {toc-tic:,.1f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c8e905c5-9625-4d06-8633-3cf405c5cae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Title (7 words)\n",
      "Adversarial Attacks and Defenses in Deep Learning\n",
      "\n",
      "#### Short Synopsis (24 words)\n",
      "This research focuses on the security concerns posed by adversarial attacks in deep learning models and explores various defense technologies to enhance their robustness.\n",
      "\n",
      "#### Key Points (66 words)\n",
      "- Adversarial attacks manipulate deep learning models to exhibit incorrect behavior\n",
      "- Adversarial examples can have serious consequences in security-critical areas\n",
      "- Research efforts have emerged to understand and mitigate adversarial attacks\n",
      "- Theoretical basis, attack algorithms, and practical application cases are discussed\n",
      "- Defense technologies include heuristic defenses, adversarial training, denoising, randomization, and certified defenses\n",
      "- Current status, development trends, and unresolved challenges are analyzed\n",
      "\n",
      "#### Detailed Summary (344 words)\n",
      "The research on adversarial attacks and defenses in deep learning has been conducted by the Rehnquist team from Jaejoong University in collaboration with the University of Toronto and McGill University. In recent years, artificial intelligence (AI) and deep learning technologies have made significant advancements and achieved remarkable performance in various applications such as image classification, natural language processing, autonomous vehicles, and speech recognition. However, as these technologies continue to develop rapidly, it has become increasingly crucial to ensure the security and robustness of deep learning algorithms.\n",
      "\n",
      "Adversarial attacks pose a significant security issue for deep learning models. Adversaries can manipulate a well-performing model to exhibit incorrect behavior by adding small perturbations to benign samples. These manipulated samples, known as adversarial examples, can have serious consequences, particularly in security-critical areas. For instance, in autonomous driving, if an adversary modifies a stop sign, the object detection system of vehicles may misclassify it, leading to potentially disastrous accidents.\n",
      "\n",
      "In recent years, numerous research efforts have emerged in the field of adversarial attacks. The existing research has been summarized, and the current state and future directions of this work have been discussed. The research begins by establishing the theoretical basis of adversarial attacks, including the fundamental concepts and attack settings. It then delves into describing various general attack algorithms in detail. Additionally, practical application cases of adversarial attacks in different domains are presented, providing readers with a comprehensive understanding of the theory of adversarial attacks and their security threats in real-world scenarios.\n",
      "\n",
      "To address these security concerns, several representative defense technologies have been introduced. These include heuristic defenses, adversarial training, denoising, randomization, and certified defenses that utilize mathematical methods. These defense technologies aim to enhance the security and robustness of deep learning models based on specific needs and characteristics.\n",
      "\n",
      "Finally, the current status of adversarial attacks and defenses in AI technology is analyzed and discussed. The development trends and several unresolved challenges in this critical area are highlighted. The article concludes by encouraging further research efforts to tackle these challenges and improve the security of deep learning algorithms.\n"
     ]
    }
   ],
   "source": [
    "print(final_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
