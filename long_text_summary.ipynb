{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ea5c15b-88bd-4470-8ad8-63ec3ba25608",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b19e5b6d-6b8d-406f-a332-0ae779868832",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameters():\n",
    "    batch_size:int=16\n",
    "    EMBEDDING_MODEL:str=\"text-embedding-ada-002\"\n",
    "    COMPLETION_MODEL:str=\"gpt-3.5-turbo-16k-0613\"\n",
    "    AVERAGE_TOPIC_WORDS:int=500\n",
    "    CONTEXT_LENGTH:int=16000 # The permitted context length in tokens\n",
    "    SUMMARIZE_TOPIC_TOKENS:int=14000 # 14000 The last to-summary-text token num\n",
    "    PHASE_1_MODEL:str=\"gpt-3.5-turbo-0613\"\n",
    "    PHASE_1_CHUNK_LEN:int=1500 # Number of tokens\n",
    "    PHASE_1_PARA_LEN:int=200 # token num\n",
    "    PHASE_1_PARAGRAPH_NUM:int=PHASE_1_CHUNK_LEN//PHASE_1_PARA_LEN # prompt to divide each chunk into at least 5 paragraphs - one paragraph with 200 tokens\n",
    "    PHASE_1_CHUNK_LEN_IN_WORD:int=0 # Number of words, updated by function `get_phase_1_chunks`\n",
    "    PHASE_1_CHUNK_OVERLAY:int=30 # Overlapped word number\n",
    "    PHASE_1_CONTEXT_LENGTH:int=4096 # Phase 1 is for cleanup text. It might use a different model\n",
    "    TOPIC_SUMMARY_LENGTH:int=800 # The length of topic summary in words\n",
    "    FINAL_SUMMARY_LENGTH:int=1200 # The length of final summary in words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f87ffb7-6692-4049-a500-229c63ac5e3f",
   "metadata": {},
   "source": [
    "## Little utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d81e01d-af92-4d61-a24a-6ad268d4fce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, re, os, json\n",
    "import asyncio\n",
    "import concurrent.futures\n",
    "from getSrt import getSrtJson\n",
    "\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "from openaikey import OPENAI_API_KEY\n",
    "import openai\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "import random\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import tiktoken\n",
    "\n",
    "# llm = ChatOpenAI(model_name=Parameters.COMPLETION_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2140893e-1558-4df6-997a-e7de42ea2789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(txt:str)->List[str]:\n",
    "    splited_word_list = []\n",
    "    word = ''\n",
    "    # import ipdb; ipdb.set_trace()\n",
    "    for c in txt:\n",
    "        if len(c)==0:\n",
    "            continue\n",
    "        ord_c = ord(c)\n",
    "        if word!='' and ord_c in (list(range(0x00, 0x20+1)) + [0x7F] + [0xFF]):\n",
    "            splited_word_list.append(word)\n",
    "            word = ''\n",
    "        elif ord_c in (list(range(0x21, 0x7E+1)) + list(range(0x80, 0xFE+1))):\n",
    "            word += c\n",
    "        elif ord_c > 0xFF:\n",
    "            if word!='':\n",
    "                splited_word_list.append(word)\n",
    "            splited_word_list.append(c)\n",
    "            word = ''\n",
    "    if word != '':\n",
    "        splited_word_list.append(word)\n",
    "    return splited_word_list\n",
    "\n",
    "def join_texts(txt_list:List[str])->str:\n",
    "    if type(txt_list)==str:\n",
    "        return txt_list\n",
    "        \n",
    "    txt_list_new = []\n",
    "    non_asci = False\n",
    "    non_asci_word = ''\n",
    "    for w in txt_list:\n",
    "        if len(w)==0:\n",
    "            continue\n",
    "        ord_c = ord(w[0])\n",
    "        if ord_c > 0xFF: # non-ASCI\n",
    "            non_asci = True\n",
    "            non_asci_word += w\n",
    "        else: # ASCI\n",
    "            if non_asci_word != '':\n",
    "                txt_list_new.append(non_asci_word)\n",
    "                non_asci = False\n",
    "                non_asci_word = ''\n",
    "            txt_list_new.append(w)\n",
    "    if non_asci_word != '':\n",
    "        txt_list_new.append(non_asci_word)\n",
    "    return ' '.join(txt_list_new)\n",
    "\n",
    "# Get the number of words\n",
    "def strlen(txt:str)->int:\n",
    "    txt1 = re.sub(r\"[\\x21-\\x7E\\x80-\\xFE]+\", 'A', txt)\n",
    "    txt1 = re.sub(r\"[\\x00-\\x20\\xFF]\", '', txt1)\n",
    "    length = len(txt1)\n",
    "    return length\n",
    "\n",
    "# Get the number of tokens\n",
    "def get_num_tokens(txt:str, model:str=Parameters.COMPLETION_MODEL)->int:\n",
    "    # global llm\n",
    "    # if 'llm' not in globals():\n",
    "    #     llm = ChatOpenAI(model_name=model)\n",
    "    # global llm_encoding\n",
    "    # return llm.get_num_tokens(txt)\n",
    "\n",
    "    global llm_encoding\n",
    "    if 'llm_encoding' not in globals():\n",
    "        llm_encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(llm_encoding.encode(txt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ae445a-a8f0-4d74-a7eb-c9ea1cbdfd93",
   "metadata": {},
   "source": [
    "# Concurrent OpenAI Call"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968d587c-6d51-4222-8a4b-db2cb07f30e8",
   "metadata": {},
   "source": [
    "## Batch generate using OpenAI\n",
    "\n",
    "```python\n",
    "def batch_generate(prompts:List[str], batch_size:int=8, temperature:float=0.2)->List[str]:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb690b1-656c-4596-b252-732f9e129f05",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfcee793-2315-4a81-a281-59006abc6ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")  # for exponential backoff: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_handle_rate_limits.ipynb\n",
    "\n",
    "def _num_tokens_from_messages(messages:List[str], model:str)->int:\n",
    "    \"\"\"Returns the number of tokens used by a list of messages.\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        print(\"Warning: model not found. Using cl100k_base encoding.\")\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    if re.match('^gpt-3.5-turbo.*$', model):\n",
    "        tokens_per_message = 4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n",
    "        tokens_per_name = -1  # if there's a name, the role is omitted\n",
    "    elif re.match('^gpt-4.*$', model):\n",
    "        tokens_per_message = 3\n",
    "        tokens_per_name = 1\n",
    "    else:\n",
    "        raise NotImplementedError(f\"\"\"num_tokens_from_messages() is not implemented for model {model}. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\")\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
    "    return num_tokens\n",
    "\n",
    "def openai_usage_stat()->Dict[str, object]:\n",
    "    if 'openai_usages' not in globals() or len(openai_usages)==0:\n",
    "        return {'object': '', 'model': '', 'prompt_tokens':0, 'completion_tokens': 0, 'total_tokens': 0}\n",
    "    _usages = []\n",
    "    # import ipdb; ipdb.set_trace()\n",
    "    unique_items = list(set(tuple(sorted(d.items())) for d in [{'object': c.get('object'), 'model': c.get('model')} for c in openai_usages]))\n",
    "    unique_items = [dict(items) for items in unique_items]\n",
    "    # print(unique_items)\n",
    "    for item in unique_items:\n",
    "        object_name = item.get('object')\n",
    "        model_name = item.get('model')\n",
    "        _usage = {'object': object_name, 'model': model_name, \n",
    "                  'prompt_tokens': sum([x.get('prompt_tokens') for x in openai_usages if x.get('object') == object_name and x.get('model') == model_name]), \n",
    "                  'completion_tokens': sum([x.get('completion_tokens') for x in openai_usages if x.get('object') == object_name and x.get('model') == model_name]), \n",
    "                  'total_tokens': sum([x.get('total_tokens') for x in openai_usages if x.get('object') == object_name and x.get('model') == model_name])}\n",
    "        _usages.append(_usage)\n",
    "\n",
    "    cost = 0.\n",
    "    for a in openai_usages:\n",
    "        prompt_tokens = a.get('prompt_tokens')\n",
    "        completion_tokens = a.get('completion_tokens')\n",
    "        model_name = a.get('model')\n",
    "        object_name = a.get('object')\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "        if re.match(r'^gpt-3.5.*16k.*$', model_name, re.IGNORECASE) and re.match(r'^chat\\.', object_name, re.IGNORECASE): # Chat model, 16K context\n",
    "            cost += prompt_tokens * 0.003 / 1000\n",
    "            cost += completion_tokens * 0.004 / 1000\n",
    "        elif re.match(r'^gpt-3.5.*$', model_name, re.IGNORECASE) and re.match(r'^chat\\.', object_name, re.IGNORECASE): # Chat model, 4K context \n",
    "            cost += prompt_tokens * 0.0015 / 1000\n",
    "            cost += completion_tokens * 0.002 / 1000\n",
    "\n",
    "    return {\"TokenUsage\": _usages, \"Dollar_Usage\": cost}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2996b8-3cac-46ca-a5cd-be30f7eec439",
   "metadata": {},
   "source": [
    "### OpenAI functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19c72266-7d4b-440c-bd5c-a92d8b1d67d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_completion_func(model=Parameters.COMPLETION_MODEL, context_length:int=Parameters.CONTEXT_LENGTH, temperature:float=0.3, request_timeout:int=300): # Andrew mentioned that the prompt/ completion paradigm is preferable for this class\n",
    "    # help: https://platform.openai.com/docs/api-reference/chat/create\n",
    "    global openai_usages\n",
    "    if 'openai_usages' not in globals():\n",
    "        openai_usages = []\n",
    "    \n",
    "    MAX_RETRY_TIMES = 3\n",
    "    @retry(wait=wait_random_exponential(multiplier=1, min=1, max=120), stop=stop_after_attempt(MAX_RETRY_TIMES))\n",
    "    def _openai_completion(prompt:str):\n",
    "        openai.api_key = OPENAI_API_KEY\n",
    "        system_content = \"You're a meticulous and careful AI assistant who pays extreme attention to details and does not overlook any important information, especially for technical steps.\"\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}] # {\"role\": \"system\", \"content\": system_content},\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}, {\"role\": \"system\", \"content\": system_content},]\n",
    "        num_prompt_tokens = _num_tokens_from_messages(messages, model)\n",
    "        max_tokens=context_length - num_prompt_tokens\n",
    "        print(f\"Inside _get_completion_func: num_prompt_tokens = {num_prompt_tokens:,d}, max_tokens = {max_tokens:,d}, model = {model}, context_length = {context_length:,d}\")\n",
    "        # print(messages)\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            max_tokens=max_tokens, # do we really need this \"max_tokens\"?\n",
    "            request_timeout=request_timeout,\n",
    "            temperature=temperature, # this is the degree of randomness of the model's output\n",
    "        )\n",
    "        usage = response.get('usage')\n",
    "        if usage:\n",
    "            usage = dict(usage)\n",
    "            usage[\"model\"] = response.get('model')\n",
    "            usage[\"object\"] = response.get('object')\n",
    "            openai_usages.append(usage)\n",
    "        return response.choices[0].get('message').get('content')\n",
    "    return _openai_completion\n",
    "\n",
    "\n",
    "def _batch_generate(prompts:List[str], batch_size:int=4, temperature:float=0.3, model:str=Parameters.COMPLETION_MODEL, context_length:int=Parameters.CONTEXT_LENGTH, request_timeout:int=300):\n",
    "    final_results = []\n",
    "    num_prompts = len(prompts)\n",
    "    start_pos = 0\n",
    "    futures = []\n",
    "    while start_pos < num_prompts:\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            for prompt in prompts[start_pos:start_pos+batch_size]:\n",
    "                num_words = strlen(prompt)\n",
    "                num_prompt_tokens = get_num_tokens(prompt)\n",
    "                if num_prompt_tokens / num_words > 1.7: # We have too much tokens from the words and thus we need to turn to the model with longer context\n",
    "                    print(f\"num_prompt_tokens / num_words = {num_prompt_tokens / num_words:.2f}, let's switch to a model with longer context!!\")\n",
    "                    completion_func = _get_completion_func(model=Parameters.COMPLETION_MODEL, temperature=temperature, request_timeout=request_timeout, context_length=Parameters.CONTEXT_LENGTH)\n",
    "                else:\n",
    "                    completion_func = _get_completion_func(model=model, temperature=temperature, request_timeout=request_timeout, context_length=context_length)\n",
    "                futures.append(executor.submit(completion_func, prompt))\n",
    "                # futures = [executor.submit(get_completion, prompt) for prompt in prompts[start_pos:start_pos+batch_size]]\n",
    "            start_pos += batch_size\n",
    "    results = [future.result() for future in futures]\n",
    "    final_results += results\n",
    "\n",
    "    return final_results\n",
    "\n",
    "def batch_cleanup(chunks:List[str], temperature:float=0.1, batch_size:int=4, model:str=Parameters.PHASE_1_MODEL, context_length:int=Parameters.PHASE_1_CONTEXT_LENGTH):\n",
    "    #   Divide it into at least {strlen(chunk)//300 + (1 if strlen(chunk) % 300 != 0 else 0):d} sentimentally coherent sections, using \"\\n\\n\" as a delimiter. Translate the text into English, unless it's already in English or Chinese.\n",
    "    prompts = [f\"\"\"Please add (or update with) proper punctuations, like ',', '.', '?', to the following text. Correct obvious spelling or grammatic errors as well if possible. At the same time, divide the text into at least {Parameters.PHASE_1_PARAGRAPH_NUM:d} sentimentally coherent paragraphs, using \"\\n\\n\" as a delimiter. You should not give any extra comment.\n",
    "    \n",
    "    ```{chunk}```\"\"\" for chunk in chunks]    \n",
    "    return _batch_generate(prompts, temperature=temperature, batch_size=batch_size, model=model, context_length=context_length)\n",
    "\n",
    "\n",
    "def batch_summary(topic_groups:List[str], temperature:float=0.3, batch_size:int=4, summary_length:int=Parameters.TOPIC_SUMMARY_LENGTH, model:str=Parameters.COMPLETION_MODEL, context_length:int=Parameters.CONTEXT_LENGTH):\n",
    "    model = model if model else Parameters.COMPLETION_MODEL\n",
    "    print(f\"batch_summary model = {model}\")\n",
    "    prompts = [f\"\"\"```{topic_group}```\n",
    "\n",
    "You task is to give a verbose summary to the above article. The summary should be about {max(1, min(summary_length, int(strlen(topic_group)*0.5)))}-word-long in multi-paragraphs \\\n",
    "Summarize the above article. The summary should be about {max(1, min(summary_length, int(strlen(topic_group)*0.5)))}-word-long and should contain sufficient information of each key points. \\\n",
    "Do not miss any important technical details (like formulas, algorithms, reasoning steps, etc) or subtle nuances presented in the source material.\n",
    "Note: the output should be in the same language as the input, e.g. English to English, Simplified Chinese to Simplified Chinese.\n",
    "\"\"\" for topic_group in topic_groups]\n",
    "\n",
    "    return _batch_generate(prompts, temperature=temperature, batch_size=batch_size, model=model, context_length=context_length)\n",
    "\n",
    "def batch_rephrase(topic_groups:List[str], temperature:float=0., batch_size:int=1, model:str=Parameters.COMPLETION_MODEL, context_length:int=Parameters.CONTEXT_LENGTH):\n",
    "    # prompts = [f\"\"\"Please rewrite the following text. Use English to rewrite if the provided text is not Chinese:\n",
    "    prompts = [f\"\"\"Please rewrite the following text:\n",
    "    ```\\n{topic_group}\\n```\"\"\" for topic_group in topic_groups]\n",
    "    # print(prompts)\n",
    "    return _batch_generate(prompts, temperature=temperature, batch_size=batch_size, model=model, context_length=context_length)\n",
    "\n",
    "def batch_highlights(topic_groups:List[str], temperature:float=0.5, batch_size:int=1, num_highlights:int=7, model:str=Parameters.COMPLETION_MODEL, context_length:int=Parameters.CONTEXT_LENGTH):\n",
    "    model = model if model else Parameters.COMPLETION_MODEL\n",
    "    print(f\"batch_highlights model = {model}\")\n",
    "    prompts = [f\"\"\"Guidelines: Please proceed in the following manner for your output:\n",
    "\n",
    "#### Short Synopsis\n",
    "#### Key Points\n",
    "- [Emoji] Concise Bulletpoint\n",
    "\n",
    "Your mission is to create a summary of the text I've given you, delimited by ```, with a maximum of {num_highlights} succinct bullet points, prefixed by a proper emoji. The output should use the same language as the input.\n",
    "\n",
    "```{topic_groups}```\n",
    "\"\"\" for topic_group in topic_groups]    \n",
    "    return _batch_generate(prompts, temperature=temperature, batch_size=batch_size, model=model, context_length=context_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bd070ad-e05e-4129-a9a2-15c12415177b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_summary = ['''In this article, the author interviews Jared Zonnerak, the co-founder of PromptLayer, a collaborative platform for teams and individuals to track, debug, and explore their language model application requests. The conversation revolves around the common denominator that all language model practitioners share: the prompt. Zonnerak explains that prompt engineering is a skill set that involves tinkering with the prompt to optimize the output of language models. He believes that prompt engineering is not just for engineers, but also for product managers and non-technical stakeholders.\\n\\nPromptLayer was launched in January and has since gained popularity among developers and teams. The platform provides a one-line code setup that allows users to track and analyze their logs, save prompt templates, and gain insights through analytics. It caters to both indie developers and teams, with features specifically designed for collaboration between technical and non-technical stakeholders.\\n\\nZonnerak discusses the journey of PromptLayer since its launch, highlighting the feature updates and the recent addition of support for the anthropic language model. The company has also raised funding to support its growth and is currently looking to hire a founding engineer.\\n\\nLooking ahead, Zonnerak shares that PromptLayer\\'s roadmap includes focusing on improving the product for both hackers and teams. They aim to build workflows that allow teams to work on prompts collaboratively and efficiently. They are also working on features like unit testing and backtesting prompts, as well as A/B testing capabilities.\\n\\nThe conversation then shifts to the challenges of being a founder in the AI space and dealing with the constant influx of news and information. Zonnerak acknowledges the noise but emphasizes the importance of staying rooted in the needs and feedback of actual users. He believes that prompt engineering is a skill set that will continue to evolve and become more important in the future, with product managers potentially taking ownership of it.\\n\\nWhen asked about prominent prompt engineers in the industry, Zonnerak explains that prompt engineering is still a relatively new field and there are no defined experts or certifications. He suggests following builders and developers who are actively shipping products rather than those who only talk about prompt engineering. He also mentions the dynamics of how teams interact with prompts, explaining that before using PromptLayer, teams often relied on tools like Google Docs or databases to manage prompts.\\n\\nThe conversation then delves into the future of prompting, with Zonnerak sharing his belief that prompts will always be necessary, even as language models become more intelligent. He argues that humans themselves rely on prompts when communicating with each other, and prompts serve as a starting point for language models. He also highlights the importance of other variables in prompt engineering, such as the choice of model, temperature, and user segmentation.\\n\\nZonnerak recommends practicing prompt engineering by playing around with language models in playgrounds like OpenAI\\'s playground. He also suggests reading Stephen Wolfram\\'s articles on language models to gain a better understanding of the underlying technology.\\n\\nThe article concludes with Zonnerak discussing the serendipitous moments and challenges of attending events in the AI space. He mentions some under-the-radar companies he is excited about, such as Great for code refactoring and DeepTune for podcast dubbing AI. Zonnerak also shares his love for books and recommends Nassim Nicholas Taleb\\'s \"Antifragile\" and \"Skin in the Game.\"\\n\\nIn closing, Zonnerak advises developers and entrepreneurs to \"just ship it\" and not spend too much time strategizing. He encourages early product releases and invites readers to check out PromptLayer and connect with him on Twitter or via email.''']\n",
    "\n",
    "# temperature = 0.0\n",
    "# num_highlights = 5\n",
    "\n",
    "# final_highlight = batch_highlights(final_summary, temperature=temperature, num_highlights=num_highlights, model=Parameters.PHASE_1_MODEL, context_length=Parameters.PHASE_1_CONTEXT_LENGTH)\n",
    "# print(final_highlight[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2567ccc-1755-4369-b0c0-4e03e2881f56",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e912200e-63da-47a1-b4da-3ba0de11c038",
   "metadata": {},
   "source": [
    "## From bilibili"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728bfc4c-5ad0-4dc7-8918-c109adf29cc7",
   "metadata": {},
   "source": [
    "#### Configure env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469f430e-d3f1-4334-a06d-cac4ef44afa3",
   "metadata": {},
   "source": [
    "```python\n",
    "# Install necessary liberies to fetch subtitles\n",
    "import sys\n",
    "!{sys.executable} -m pip install selenium\n",
    "!{sys.executable} -m pip install msedge-selenium-tools\n",
    "!{sys.executable} -m pip install beautifulsoup4\n",
    "\n",
    "# Install Google Chrome:\n",
    "!wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
    "!sudo dpkg -i google-chrome-stable_current_amd64.deb\n",
    "\n",
    "# Install ChromeDriver\n",
    "chrome_driver_version=\"114.0.5735.90\"\n",
    "chrome_dirver_filename=\"chromedriver_linux64.zip\"\n",
    "chrome_driver_url=f\"https://chromedriver.storage.googleapis.com/{chrome_driver_version}/{chrome_dirver_filename}\"\n",
    "!rm -rf /tmp/chrome; mkdir -p /tmp/chrome\n",
    "!wget {chrome_driver_url} -O /tmp/chrome/{chrome_dirver_filename}\n",
    "!unzip /tmp/chrome/{chrome_dirver_filename} -d /tmp/chrome\n",
    "!sudo rm -f /usr/bin/chromedirver\n",
    "!sudo cp /tmp/chrome/chromedriver /usr/bin/chromedriver\n",
    "!sudo chown root:root /usr/bin/chromedriver\n",
    "!sudo chmod +x /usr/bin/chromedriver\n",
    "!sudo rm -rf /tmp/chrome\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acd991d-3516-4d19-906f-42154f9b1aea",
   "metadata": {},
   "source": [
    "#### functions to fetch subtitles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5001158d-c8b9-4641-b893-1c514401745b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bilibili_transcript(bvid:str, pageNo:int=1)->str:\n",
    "    subtitle = getSrtJson(bvid, pageNo)\n",
    "    _result = []\n",
    "    _result.append(f\"Video category: {subtitle.get('tname')}\")\n",
    "    _result.append(f\"Video title: {subtitle.get('title')}\")\n",
    "    _result.append(f\"Video description: {subtitle.get('desc')}\")\n",
    "    _result.append(f\"Video contents: \\n\\n{join_texts([c.get('content') for c in subtitle.get('subtitle_json').get('body')])}\")\n",
    "    return '\\n\\n'.join(_result)\n",
    "# txt = get_bilibili_transcript('BV1TD4y137mP')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792ba0d3-cfe1-4344-9820-84690545f4cf",
   "metadata": {},
   "source": [
    "## From Youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e579a66-373b-4bd2-98ae-fa24dd54c2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "847292dc-bf6e-4f5e-bb3a-454001e5ff67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_youtube_transcript(video_id: str, languages:List[str]=['en', 'de', 'zh'])->List[Dict[str, object]]:\n",
    "    results = YouTubeTranscriptApi.get_transcripts([video_id], languages=languages)[0][video_id]\n",
    "    txt = join_texts([sub['text'] for sub in results])\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d45c9e-3613-4e96-91e4-eff9c9ad96a9",
   "metadata": {},
   "source": [
    "## From text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e5b0aa2-2d3c-49e2-8da4-3413e5176f52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_txt_from_file(txt_path:str)->str:\n",
    "    def get_text(txt_path:str):\n",
    "        with open(txt_path, 'r') as f:\n",
    "              txt = f.read()\n",
    "        return txt\n",
    "    txt = get_text(txt_path)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7ae704-0cab-4e54-9673-9b41ae80e602",
   "metadata": {},
   "source": [
    "## Cleanup text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16697bd9-785e-45eb-a70a-0bebb076e26c",
   "metadata": {},
   "source": [
    "### Phase 1 chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6a0a8fb-20ba-49ca-b628-3fcfd445d615",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phase_1_chunks(txt:str, end_pos:int=None)->List[str]:\n",
    "    num_words = strlen(txt[:end_pos])\n",
    "    num_tokens = get_num_tokens(txt[:end_pos])\n",
    "    num_chunk = num_tokens // Parameters.PHASE_1_CHUNK_LEN + 1 # use token to calculate the number of chunks\n",
    "    chunk_size = num_words // num_chunk + 1 # use word number to really calculate chunk size (not easy to split text via token)\n",
    "    Parameters.PHASE_1_CHUNK_LEN_IN_WORD = chunk_size\n",
    "    txt_word_list = split_text(txt[:end_pos])\n",
    "    phase_1_chunk_list = []\n",
    "    start_pos = 0\n",
    "    print(f\"Total words: {num_words:,d} ({num_tokens:,d} tokens), chunk_size = {chunk_size:,d} words\")\n",
    "    for _ in range(num_chunk):\n",
    "        while start_pos < num_words:\n",
    "            phase_1_chunk_list.append(join_texts(txt_word_list[start_pos:start_pos + chunk_size + Parameters.PHASE_1_CHUNK_OVERLAY]))\n",
    "            print(f\"strlen(phase_1_chunk_list[-1]) = {strlen(phase_1_chunk_list[-1]):,d}, from {start_pos:,d} to {start_pos + strlen(phase_1_chunk_list[-1]):,d}\")\n",
    "            start_pos += chunk_size\n",
    "    return phase_1_chunk_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c91d9c-b078-4929-8569-7049cb216bd7",
   "metadata": {},
   "source": [
    "### Get chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c42d3ccf-03b6-4047-a0d6-315388bcc929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned_chunk_list = ['aaaa als. ', 'asdkw ', 'sldf 2.' , 'als 2,3. ', 'sal ab.', 'aserl s2. 332aaaa.', 'aserl s2. 223 aaaa.']\n",
    "# [c.strip()+\"\\n\\n\" if re.match(r'.*[\\s]+[^0-9]+\\.[\\s]*$', c) else c.strip() for c in cleaned_chunk_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0bb2d7e-6266-44ad-88b1-eb4642625e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_large_chunks(cleaned_chunk_list0:List[str])->List[str]:\n",
    "    cleaned_chunk_list = []\n",
    "    for chunk in cleaned_chunk_list0:\n",
    "        for p in re.split(r'[\\n]+', chunk): # p: paragraph\n",
    "            n_tokens = get_num_tokens(p)\n",
    "            if n_tokens > int(Parameters.PHASE_1_PARA_LEN * 2):\n",
    "                n_paraph = n_tokens // Parameters.PHASE_1_PARA_LEN + 1\n",
    "                paraph_token_len = n_tokens // n_paraph\n",
    "                print(f\"We're going to split this sub-chunk as it has {n_tokens:,d} tokens, larger than {int(Parameters.PHASE_1_PARA_LEN * 2):,d}. Words number: {strlen(p)}. It'll be split to {n_paraph} chunks.\")\n",
    "                sub_chunk = []\n",
    "                for s in re.split(r'[.]', p):\n",
    "                    if s=='':\n",
    "                        continue\n",
    "                    s += \".\"\n",
    "                    sub_chunk_text = ''.join(sub_chunk)\n",
    "                    if get_num_tokens(sub_chunk_text)+get_num_tokens(s)>=Parameters.PHASE_1_PARA_LEN:\n",
    "                        cleaned_chunk_list.append(sub_chunk_text)\n",
    "                        sub_chunk = []\n",
    "                    sub_chunk.append(s)\n",
    "                if len(sub_chunk)>0:\n",
    "                    sub_chunk_text = ''.join(sub_chunk)\n",
    "                    if len(cleaned_chunk_list)>0 and get_num_tokens(sub_chunk_text) + get_num_tokens(cleaned_chunk_list[-1]) < int(Parameters.PHASE_1_PARA_LEN*1.6):\n",
    "                        cleaned_chunk_list[-1] = cleaned_chunk_list[-1] + sub_chunk_text\n",
    "                    else:\n",
    "                        cleaned_chunk_list.append(sub_chunk_text)\n",
    "            else:\n",
    "                cleaned_chunk_list.append(p)\n",
    "    \n",
    "    cleaned_chunk_list = [c.strip()+\"\\n\\n\" if re.match(r'.*[\\s]+[^0-9]+\\.[\\s]*$', c) else c.strip() for c in cleaned_chunk_list]\n",
    "    print(\"We've split large chunks, by words, from:\", [strlen(c) for c in cleaned_chunk_list0], \"to:\", [strlen(c) for c in cleaned_chunk_list], 'by tokens, from:', [get_num_tokens(c) for c in cleaned_chunk_list0], \"to:\", [get_num_tokens(c) for c in cleaned_chunk_list])\n",
    "    return cleaned_chunk_list\n",
    "\n",
    "# We're going to clean the overlap here\n",
    "def get_chunks(cleaned_chunk_list:List[str], overlapped:int)->(str,List[str]):\n",
    "    if overlapped != 0:\n",
    "        cleaned_chunk_list = [re.sub(r'\\W*$', '', chunk) for chunk in cleaned_chunk_list]\n",
    "    \n",
    "    def _resolve_overlap(t0:str, t1:str)->Tuple:\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "        t0_wordlist = split_text(t0)\n",
    "        search_length = len(join_texts(t0_wordlist[-overlapped:]))\n",
    "        search_start_pos = max(1, len(join_texts(t0_wordlist[-overlapped//5:]))) # Give the \"leftOver\" enough words\n",
    "        \n",
    "        t0_match = re.search(r'(\\n\\n )*[\\.!?;:,*/-]', t0[-search_start_pos:-search_length:-1])\n",
    "        if not t0_match:\n",
    "            search_start_pos = max(1, len(join_texts(t0_wordlist[int(-overlapped//1.5):]))) # Give the \"leftOver\" more words\n",
    "            t0_match = re.search(r'(\\n\\n )*[ ]', t0[-search_start_pos:-search_length:-1]) # Start from any word\n",
    "        \n",
    "        t1_match_start = 0\n",
    "\n",
    "        if t0_match:\n",
    "            t0_endAt = len(t0) - t0_match.start() - search_start_pos + 1\n",
    "            leftOver = [word for word in split_text(t0[t0_endAt:]) if word != '']\n",
    "            num_search_word = 8\n",
    "            print('leftOver: ', leftOver)\n",
    "            while num_search_word>0 and len(leftOver)>0:\n",
    "                start_from_word_idx = random.randint(0, min(len(leftOver)-1, len(leftOver)//2))\n",
    "                # print(f\"len(leftOver) = {len(leftOver):,d}, start_from_word_idx = {start_from_word_idx:,d}\")\n",
    "                word_to_search = join_texts(leftOver[start_from_word_idx:start_from_word_idx+min(num_search_word, len(leftOver) - start_from_word_idx)])\n",
    "                # print(f\"word_to_search: {word_to_search}\")\n",
    "                search_in = t1[:search_length]\n",
    "                t1_match = search_in.lower().find(word_to_search.lower())\n",
    "                # t1_match = re.search(word_to_search, t1[:search_length], re.IGNORECASE)\n",
    "                if t1_match!=-1:\n",
    "                    print(f\"Matched!!! word_to_search: {word_to_search}\")\n",
    "                    # Find matching in the next chunk - easy to resolve the overlap\n",
    "                    t1_match_start = t1_match # t1_match.start()\n",
    "                    t0_endAt += len(join_texts(leftOver[:start_from_word_idx])) + 1\n",
    "                    break\n",
    "                if random.random() < 1/(10-num_search_word):\n",
    "                    num_search_word -= 1\n",
    "            if num_search_word == 0:\n",
    "                # Can't find matching in the next chunk\n",
    "                print(f\"No matching for \\\"{leftOver}\\\"\")\n",
    "                t0_endAt = len(t0) - len(join_texts(t0_wordlist[-int(overlapped*2/3):])) # remove 2/3 the overlap\n",
    "                t1_match_start = 0\n",
    "        else:\n",
    "            t0_endAt = len(t0) - len(join_texts(t0_wordlist[-int(overlapped*2/3):])) # remove 2/3 the overlap\n",
    "            t1_match_start = 0\n",
    "    \n",
    "        # Remove overlap\n",
    "        print(\"Original end of t0: \", t0[-search_length:])\n",
    "        print(\"Original start of t1: \", t1[:search_length])\n",
    "        t0 = t0[:t0_endAt].strip()\n",
    "        t1 = t1[t1_match_start:].strip()\n",
    "        print(\"End of t0: \", t0[-100:])\n",
    "        print(\"Start of t1: \", t1[:100])\n",
    "        print('#'*50)\n",
    "        return t0, t1\n",
    "\n",
    "    for i in range(len(cleaned_chunk_list)-1):\n",
    "        t0 = cleaned_chunk_list[i]\n",
    "        t1 = cleaned_chunk_list[i+1]\n",
    "        if overlapped != 0:\n",
    "            # remove overlaps\n",
    "            t0, t1 = _resolve_overlap(t0, t1)\n",
    "        cleaned_chunk_list[i] = t0.strip()\n",
    "        cleaned_chunk_list[i+1] = t1.strip()\n",
    "\n",
    "    cleaned_chunk_list = _split_large_chunks(cleaned_chunk_list)\n",
    "    # import ipdb; ipdb.set_trace()\n",
    "    chunks = []\n",
    "    n_appended_short_chunk = 0\n",
    "    joined_chunk_list = join_texts(cleaned_chunk_list) # This is the first round chunking that some items in the \"cleaned_chunk_list\" might be splitted arbitrarily\n",
    "    if overlapped == 0: # This is NOT the first round chunking and each item in the \"cleaned_chunk_list\" should be regarded as a \"whole chunk\" already\n",
    "        joined_chunk_list = '\\n\\n'.join(cleaned_chunk_list)\n",
    "    for c in re.split(r'[\\n]{2,}', joined_chunk_list):\n",
    "        s_c = c.strip()\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "        if strlen(s_c) < 15 and n_appended_short_chunk<5: # Combined with previous chunk if it's too short\n",
    "            n_appended_short_chunk+=1\n",
    "            if len(chunks)>0:\n",
    "                chunks[-1] = chunks[-1].strip() + ' ' + s_c\n",
    "            else:\n",
    "                chunks.append(s_c)\n",
    "        else:\n",
    "            n_appended_short_chunk=0\n",
    "            chunks.append(s_c)\n",
    "    print(\"Length of splited chunks, in words:\", [strlen(c) for c in chunks], \"num_of_tokens:\", [get_num_tokens(c) for c in chunks], f\", Longest one with length: {max([strlen(c) for c in chunks]):,d} words ({max([get_num_tokens(c) for c in chunks]):,d} tokens)\")    \n",
    "    print(\"Length of cleaned_chunk_list, in words:\", [strlen(c) for c in cleaned_chunk_list], \"num_of_tokens:\", [get_num_tokens(c) for c in cleaned_chunk_list], f\", Longest one with length: {max([strlen(c) for c in cleaned_chunk_list]):,d} words ({max([get_num_tokens(c) for c in cleaned_chunk_list]):,d} tokens)\")\n",
    "\n",
    "    return chunks, cleaned_chunk_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34ee5cd-dd6c-42ab-81ba-6a1f4d41b399",
   "metadata": {},
   "source": [
    "# Get topic groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87666a1a-123a-458b-93f5-922322d51561",
   "metadata": {},
   "source": [
    "## Get Chunk embeddings from OpenAI\n",
    "\n",
    "[OpenAI Create Embeddings](https://platform.openai.com/docs/api-reference/embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2108bc5c-b182-4808-9c82-0ea3e00e0c05",
   "metadata": {},
   "source": [
    "## Embedding based text similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9bb8844-1fb7-4392-adfa-1f6577a54a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from scipy.spatial.distance import cosine\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@retry(wait=wait_random_exponential(multiplier=1, min=1, max=60), stop=stop_after_attempt(6))\n",
    "def embedding(texts:List[str], doc_embedding:bool=True): # Andrew mentioned that the prompt/ completion paradigm is preferable for this class\n",
    "    # help: https://platform.openai.com/docs/api-reference/embeddings\n",
    "    model = Parameters.EMBEDDING_MODEL # 'text-embedding-ada-002' # https://platform.openai.com/docs/models/embeddings\n",
    "    # response = openai.Embedding.create(\n",
    "    #     model=model,\n",
    "    #     input = texts\n",
    "    # )\n",
    "    # emds = [data['embedding'] for data in response.data]\n",
    "    tic = time.time()\n",
    "    embed_func = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY) # https://python.langchain.com/en/latest/modules/models/text_embedding/examples/openai.html\n",
    "    if doc_embedding:\n",
    "        emds = embed_func.embed_documents(texts)\n",
    "    else:\n",
    "        emds = embed_func.embed_query(texts)\n",
    "    toc = time.time()\n",
    "    print(f\"Time consumed for embedding {len(texts):,d} chunks is: {toc-tic:,.1f} seconds\")\n",
    "    return np.array(emds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89cca96c-74db-4c1e-8f58-dbb74fded091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get similarity matrix between the embeddings of the chunk summaries\n",
    "def get_text_similarity(text_embeds, bonus_constant:float=.25, bonus_power:float=1.):\n",
    "    # import ipdb; ipdb.set_trace()\n",
    "    num_1_chunks = text_embeds.shape[0]\n",
    "    text_similarity_matrix = np.zeros((num_1_chunks, num_1_chunks))\n",
    "    text_similarity_matrix[:] = np.nan\n",
    "    \n",
    "    for row in range(num_1_chunks):\n",
    "        for col in range(row, num_1_chunks):\n",
    "            # Calculate cosine similarity between the two vectors\n",
    "            similarity = 1- cosine(text_embeds[row], text_embeds[col])\n",
    "            text_similarity_matrix[row, col] = similarity\n",
    "            text_similarity_matrix[col, row] = similarity\n",
    "    \n",
    "    # # Draw a heatmap with the text_similarity_matrix\n",
    "    # plt.figure()\n",
    "    # plt.title('Non adjusted Similarity matrix')\n",
    "    # # Color scheme blues\n",
    "    # plt.imshow(text_similarity_matrix, cmap = 'Blues')\n",
    "\n",
    "    # Get the \"distance-adjusted\" similarity matrix\n",
    "    proximity_bonus_arr = np.zeros_like(text_similarity_matrix)\n",
    "    # Closer neighbors get higher bonus\n",
    "    for row in range(proximity_bonus_arr.shape[0]):\n",
    "        for col in range(proximity_bonus_arr.shape[1]):\n",
    "            if row == col:\n",
    "                proximity_bonus_arr[row, col] = 0\n",
    "            else:\n",
    "                proximity_bonus_arr[row, col] = 1/(abs(row-col))**bonus_power * bonus_constant # Closer neighbors get higher bonus\n",
    "    dist_adj_text_similarity_matrix = text_similarity_matrix.copy() + proximity_bonus_arr # closer neighbors, even if had same similarity-score, should be regarded as `more similiar`. That's the meaning of \"bonus\"\n",
    "    # plt.figure()\n",
    "    # plt.title('Adjusted matrix via the temporal structure')\n",
    "    # plt.imshow(dist_adj_text_similarity_matrix, cmap = 'Blues')\n",
    "    return text_similarity_matrix, dist_adj_text_similarity_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9d067a-1bce-4576-82d1-72d5a276fdeb",
   "metadata": {},
   "source": [
    "## Get Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a17d4b7-10fa-48be-bc8d-90995ca0ba9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_topics_via_topics_title(chunks:List[str], topics_title:List[List[int]]):\n",
    "    topics = []\n",
    "    for t in topics_title:\n",
    "        topics.append(join_texts([chunks[i] for i in t]))\n",
    "    print('Leng of topics, in words:', [strlen(topic) for topic in topics], \"in tokens:\", [get_num_tokens(topic) for topic in topics])\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45781bee-2f15-4e6e-b4f6-35c1bdaad360",
   "metadata": {},
   "source": [
    "## Similarity based Cluster - Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "140e89c0-25c2-495d-b238-e43966c2fc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from networkx.algorithms import community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "041d38c3-7b93-4e93-b6ed-5021885460ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap(list_data:list, title:str):\n",
    "    # The list_data should be something like: [0, 0, 0, 1, 1, 2, 2, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 5]\n",
    "    data = np.array(list_data).reshape(1, -1)\n",
    "    plt.figure(figsize = (10, 4))\n",
    "    plt.title(title)\n",
    "    plt.imshow(data, cmap = 'tab20')\n",
    "    for i in range(1, len(list_data)):\n",
    "        plt.axvline(x = i - 0.5, color = 'black', linewidth = 0.5)\n",
    "\n",
    "def _get_longest_topic_length(topics_title:List[List[int]], chunks:List[str]):\n",
    "    _topics_length = []\n",
    "    _topics_word_num = []\n",
    "    for i, topic_title in enumerate(topics_title):\n",
    "        _topics_length.append(sum([get_num_tokens(chunks[item]) for item in topic_title]))\n",
    "        _topics_word_num.append(sum([strlen(chunks[item]) for item in topic_title]))\n",
    "    longest_topic_length = max(_topics_length)\n",
    "    longest_topic_word_num = max(_topics_word_num)\n",
    "    index0 = _topics_length.index(longest_topic_length)\n",
    "    index1 = _topics_word_num.index(longest_topic_word_num)\n",
    "    return index0, longest_topic_length, index1, longest_topic_word_num\n",
    "\n",
    "# Run the community detection algorithm to get something like: [{0, 1, 2, 3,}, {4, 5}, {6, 7, 8}, {9, 10, 11}]\n",
    "def get_topics_title(text_embeds, num_topics:int, chunks:List[str], bonus_constant:float=0.25, min_size:int=3, \n",
    "               bonus_power:float=1., resolution:float=1., resolution_step:float=.1):\n",
    "\n",
    "    longest_chunk_size = max([get_num_tokens(chunk) for chunk in chunks]) # The num of tokens of the longest chunk\n",
    "    _, similarity_matrix = get_text_similarity(text_embeds)\n",
    "\n",
    "    title_nx_graph = nx.from_numpy_array(similarity_matrix)\n",
    "\n",
    "    desired_num_topics = num_topics\n",
    "    # Store the accepted partitionings\n",
    "    topics_title_accepted = []\n",
    "\n",
    "    # Find the resolution that gives the desired number of topics\n",
    "    topics_title = []\n",
    "    idx = 1\n",
    "    lower_bar = desired_num_topics -1\n",
    "    upper_bar = lower_bar + 5\n",
    "    print(f\"number of desired topics, lower_bar = {lower_bar:,d}, upper_bar = {upper_bar:,d}\")\n",
    "    longest_topic_length = 9e9 # TODO\n",
    "    num_retry_split_large_chunk = 0\n",
    "    while len(topics_title) not in range(lower_bar, upper_bar) or longest_topic_length>max(longest_chunk_size, Parameters.SUMMARIZE_TOPIC_TOKENS):\n",
    "        if len(topics_title) >= upper_bar:\n",
    "            old_resolution, old_resolution_step = resolution, resolution_step\n",
    "            if random.random() < 0.9:\n",
    "                resolution *= 0.9\n",
    "                if resolution_step > 1e-3:\n",
    "                    resolution_step *= 0.9\n",
    "            else:\n",
    "                resolution *= 0.5\n",
    "                if resolution_step > 1e-3:\n",
    "                    resolution_step *= 0.9\n",
    "            print(f\"Adjusted resolution from {old_resolution:.4f} to {resolution:.4f}, resolution_step from {old_resolution_step:.4f} to {resolution_step:.4f}, at step: {idx:,d}, because we have {len(topics_title):,d} topics which is >= upper_bar ({upper_bar:,d}).\")\n",
    "        topics_title = community.louvain_communities(title_nx_graph, weight = 'weight', resolution = resolution)\n",
    "        topics_title = [sorted(topic_title) for topic_title in topics_title] # Make the topic items sorted from: [{19, 20, 22, 21}, {24, 23}] to [{19, 20, 21, 22}, {23, 24}]\n",
    "        # print(\"Sorted: \", topics_title)\n",
    "        resolution += resolution_step\n",
    "        if idx % 100 == 0:\n",
    "            # import ipdb; ipdb.set_trace()\n",
    "            print(f\"idx = {idx:04d}, len(topics_title) = {len(topics_title):d}, resolution = {resolution:.4f}\")\n",
    "        idx += 1\n",
    "        _, longest_topic_length, _, _  = _get_longest_topic_length(topics_title, chunks)\n",
    "        if len(topics_title) in range(lower_bar, upper_bar) and longest_topic_length > Parameters.SUMMARIZE_TOPIC_TOKENS:\n",
    "            old_resolution = resolution\n",
    "            resolution = random.normalvariate(old_resolution, old_resolution*.1)\n",
    "            print(f\"Adjusted resolution from {old_resolution:.4f} to {resolution:.4f}, at step: {idx:,d}, because, though topics number in range, we have longest_topic_length as: {longest_topic_length:,d}, \\\n",
    "            which is > Parameters.SUMMARIZE_TOPIC_TOKENS ({Parameters.SUMMARIZE_TOPIC_TOKENS:,d}).\")\n",
    "            if num_retry_split_large_chunk > 2: # Increase the num of topics to split if the larget one can be split after a few times try\n",
    "                upper_bar += 1\n",
    "                print(f\"Adjusted upper_bar from: {upper_bar-1:,d} to {upper_bar} because we have tried {num_retry_split_large_chunk:,d} times to split the largets topic smaller. The new upper_bar={upper_bar:,d}.\")\n",
    "                num_retry_split_large_chunk = 0\n",
    "            else:\n",
    "                num_retry_split_large_chunk += 1\n",
    "\n",
    "    def _get_topics(topics_title:list):\n",
    "        topic_id_means = [sum(e)/len(e) for e in topics_title]\n",
    "        # Arrange title_topics in order of topic_id_means\n",
    "        topics_title = [list(c) for _, c in sorted(zip(topic_id_means, topics_title), key = lambda pair: pair[0])]\n",
    "        for t in topics_title:\n",
    "            t.sort()\n",
    "        # Create an array denoting which topic each chunk belongs to\n",
    "        chunk_topics = [None] * similarity_matrix.shape[0]\n",
    "        for i, c in enumerate(topics_title):\n",
    "            for j in c:\n",
    "                chunk_topics[j] = i\n",
    "        return {'chunk_topics': chunk_topics, 'topics': topics_title}\n",
    "    \n",
    "    plot_heatmap(_get_topics(topics_title)['chunk_topics'], title='Chunk similarity based Topics')\n",
    "    print(f\"We have totally {len(topics_title):,d} topics detected: {topics_title}\")\n",
    "    print(f\"resolution = {resolution:.4f}, resolution_step = {resolution_step:.4f}\")\n",
    "    longest_topic_index, longest_topic_tokens, longest_word_topic_index, longest_topic_word_num = _get_longest_topic_length(topics_title, chunks)\n",
    "    if (longest_topic_index == longest_word_topic_index):\n",
    "        print(f\"Longest topic contains: {longest_topic_tokens:,d} tokens ({longest_topic_word_num:,d} words), at topic No. {longest_topic_index:,d}, starting from index 0.\")\n",
    "    else:\n",
    "        print(f\"Longest topic contains: {longest_topic_tokens:,d} tokens at: No. {longest_topic_index:,d} ({longest_topic_word_num:,d} words at No. {longest_word_topic_index:,d}), starting from index 0.\")\n",
    "    return topics_title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4dac77-d7d8-4985-b53e-914828415f16",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Get Topic Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15716678-9d15-4769-b654-20a95b275872",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_topic_groups(topics:List[str]):\n",
    "    topic_groups = []\n",
    "    topic_group_length = 0\n",
    "    _topic_group = []\n",
    "    total_tokens = get_num_tokens(join_texts(topics))\n",
    "    intended_num_of_topic_groups = total_tokens // Parameters.SUMMARIZE_TOPIC_TOKENS + 1\n",
    "    MAX_TOPIC_GROUP_LENGTH = min(Parameters.SUMMARIZE_TOPIC_TOKENS, total_tokens // intended_num_of_topic_groups + 1)\n",
    "    i = 0\n",
    "    while i < len(topics):\n",
    "        if get_num_tokens(topics[i]) > Parameters.SUMMARIZE_TOPIC_TOKENS:\n",
    "            raise RuntimeError\n",
    "        while i<len(topics) and topic_group_length + get_num_tokens(topics[i]) <= MAX_TOPIC_GROUP_LENGTH:\n",
    "            _topic_group.append(topics[i])\n",
    "            topic_group_length += get_num_tokens(topics[i])\n",
    "            i += 1\n",
    "        topic_groups.append(join_texts(_topic_group))\n",
    "        _topic_group, topic_group_length = [], 0\n",
    "\n",
    "    if len(topic_groups)>=2 and get_num_tokens(topic_groups[-2]) + get_num_tokens(topic_groups[-1]) < Parameters.SUMMARIZE_TOPIC_TOKENS:\n",
    "        topic_groups[-2] = join_texts(topic_groups[-2:])\n",
    "        topic_groups = topic_groups[:-1]\n",
    "    words_topic_groups = [strlen(tg) for tg in topic_groups]\n",
    "    tokens_topic_groups = [get_num_tokens(tg) for tg in topic_groups]\n",
    "    print(\"Length of topic groups, in words:\", words_topic_groups, \"in tokens:\", tokens_topic_groups, f\" The longest one has: {max(words_topic_groups):,d} words ({max(tokens_topic_groups):,d} tokens)\")\n",
    "    return topic_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c401499-6720-447d-a47d-bde13e808b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7118, 5818, 8729]\n"
     ]
    }
   ],
   "source": [
    "aaa = [7118, 5818, 7731, 998]\n",
    "if len(aaa)>=2 and aaa[-2] + aaa[-1] < Parameters.SUMMARIZE_TOPIC_TOKENS:\n",
    "    aaa[-2] = sum(aaa[-2:])\n",
    "    aaa = aaa[:-1]\n",
    "print(aaa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedced3c-0751-49a9-a85d-e8767068891e",
   "metadata": {},
   "source": [
    "# Summarize each topic group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c81bd5a1-c9ec-4dc3-9ab3-fdc90678425c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_topic_groups(topic_groups:List[str], summary_length:int=Parameters.TOPIC_SUMMARY_LENGTH, batch_size:int=8, temperature:float=0.)->List[str]:\n",
    "    tic = time.time()\n",
    "    phase_2_summaries = batch_summary(topic_groups, summary_length=summary_length, temperature=temperature, batch_size=batch_size)\n",
    "    toc = time.time()\n",
    "    print(\"The length of summarized-topic-groups, in words:\", [strlen(summary) for summary in phase_2_summaries], \"in tokens:\", [get_num_tokens(summary) for summary in phase_2_summaries], \"Summarized total words:\", sum([strlen(summary) for summary in phase_2_summaries]), f\"(total tokens: {sum([get_num_tokens(summary) for summary in phase_2_summaries]):,d})\")\n",
    "    print(f\"Time consumed for topic groups summarization: {toc-tic:,.1f}s\")\n",
    "    return phase_2_summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8819e216-c667-4d59-860f-61dd2081a750",
   "metadata": {},
   "source": [
    "# Final Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606be5b0-6ef4-4450-8eb8-89ad3337daeb",
   "metadata": {},
   "source": [
    "## [Call OpenAI Functions](https://platform.openai.com/docs/guides/gpt/function-calling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de440bde-cc25-46b3-b2cb-04300178e470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_summary = \"\"\"In this article, the author discusses the use case for topic modeling and how it can be applied to various types of content, such as YouTube videos, podcasts, meeting notes, legal documents, movie scripts, books, and lecture notes. The author highlights the manual work involved in extracting topics from these sources and emphasizes the value of structured data in these contexts.\\n\\nThe author suggests that there is an opportunity to create a productionized service for extracting topics from podcasts and videos. They provide an example of a podcast website that does not have topics listed on their episodes, and propose the idea of offering a service to extract and provide topics for these episodes. This approach could be replicated for other podcasts, videos, or any series of information.\\n\\nThe author then introduces their tutorial on topic modeling, specifically focusing on a two-pass approach. In the first pass, they use a mapreduce method to process the entire document and extract topics and bullet points. They explain that this approach may be a bit expensive in terms of computational resources, but it allows for a comprehensive analysis of the text. In the second pass, they iterate through each topic bullet point and expand on them using a retrieval method. This involves a question and answer-like process to provide more detailed information and context.\\n\\nThe author makes several assumptions, including the absence of a table of contents or contents, and the desire for more control over the topic modeling process. They emphasize the importance of learning the ins and outs of building with AI and encourage readers to experiment with their own use cases.\\n\\nThe article then delves into the technical implementation of the topic modeling process. The author imports various packages and sets up two language models, GPT 3.5 Turbo and GPT 4. They load the transcript that will be parsed and split it into chunks using a recursive character text splitter. The author explains the reasoning behind the chunk size and overlap parameters, highlighting the need for experimentation based on individual use cases.\\n\\nNext, the author focuses on extracting topic titles and short descriptions. They create a custom prompt to instruct the language model on the desired output. They provide examples and formatting guidelines to ensure accurate extraction of topics. The author then runs the mapreduce method using the GPT 4 language model and consolidates the results to eliminate duplicates.\\n\\nThe article continues with the conversion of the extracted topics into structured data. The author defines a schema with properties for topic name, description, and tag. They demonstrate the structured data output for the extracted topics, showcasing the potential value of this approach.\\n\\nMoving on to the second pass of topic expansion, the author introduces the concept of using a retrieval method and the vector store dance. They explain that this approach allows for generating longer descriptions or summaries based on specific topics. The author sets up a custom prompt for this process and creates embeddings using the OpenAI embeddings engine. They initialize Pinecone, a remote vector store, and set up the retrieval QA process.\\n\\nThe author demonstrates how to iterate through the structured topics and generate expanded summaries using the retrieval method. They provide examples of the expanded topics, showcasing the additional information and context that can be obtained through this process.\\n\\nFinally, the author explores the topic of extracting chapters or timestamps from a transcript. They create a custom prompt for this purpose and use the retrieval QA process to find the first timestamp associated with each topic. The author provides an example of the retrieved timestamps and highlights the relevance of this information in organizing and navigating through content.\\n\\nIn conclusion, the article presents a comprehensive overview of topic modeling and its applications in various contexts. The author provides step-by-step instructions and technical details for implementing a two-pass approach to extract and expand on topics. They emphasize the value of structured data and the potential for creating productionized services based on topic modeling.\"\"\"\n",
    "# num_highlights = 5\n",
    "# func_name = \"output_key_information\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c01e2fa5-d3ea-4eb6-b649-788a2ad2c6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_with_openai_function(to_summary:str, num_highlights:int, temperature:float=0.):\n",
    "    tic = time.time()\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": to_summary}]\n",
    "    num_tokens = _num_tokens_from_messages(messages, model=Parameters.COMPLETION_MODEL)\n",
    "    if num_tokens > Parameters.PHASE_1_CONTEXT_LENGTH-1300:\n",
    "        model = Parameters.COMPLETION_MODEL\n",
    "        context_length = Parameters.CONTEXT_LENGTH\n",
    "    else:\n",
    "        model = Parameters.PHASE_1_MODEL\n",
    "        context_length = Parameters.PHASE_1_CONTEXT_LENGTH\n",
    "        \n",
    "    print(f\"Number of tokens for final highlight: {num_tokens:,d} (words: {strlen(to_summary):,d}). Highlight model: {model}\")\n",
    "\n",
    "    functions = [\n",
    "        {\n",
    "            \"name\": \"output_key_information\",\n",
    "            \"description\": \"Output key information including: title, short synopsis and bulleted key points from the given text. Note: the output should be in the same language as the input, like English to English, Simplified Chinese to Simplified Chinese.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"Title\": {\"type\": \"string\", \"description\": \"The title of the given text\"},\n",
    "                    \"Short\\ Synopsis\": {\"type\": \"string\", \"description\": \"A very short and concise summary of the given text.\"},\n",
    "                    \"Key\\ Points\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": f\"A list of maximum of {num_highlights} succinct bullet points, each prefixed by a proper emoji.\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"Title\", \"Short\\ Synopsis\", \"Key\\ Points\"],\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages, \n",
    "        temperature=temperature,\n",
    "        functions=functions,\n",
    "        function_call=\"auto\",  # auto is default, but we'll be explicit\n",
    "    )\n",
    "    \n",
    "    usage = response.get('usage')\n",
    "    if usage:\n",
    "        global openai_usages\n",
    "        if 'openai_usages' not in globals():\n",
    "            openai_usages = []\n",
    "        usage = dict(usage)\n",
    "        usage[\"model\"] = response.get('model')\n",
    "        usage[\"object\"] = response.get('object')\n",
    "        openai_usages.append(usage)\n",
    "    response_message = response[\"choices\"][0][\"message\"]\n",
    "    \n",
    "    if response_message.get(\"function_call\"):\n",
    "        try:\n",
    "            function_args = json.loads(response_message.get(\"function_call\").get(\"arguments\"))\n",
    "        except Exception as e:\n",
    "            import ipdb; ipdb.set_trace()\n",
    "            print(response_message)\n",
    "            print(f\"An error occurs: {e}\")\n",
    "            \n",
    "    # for key in function_args.keys():\n",
    "    #     print(f\"{key}: {function_args.get(key)}\")\n",
    "        \n",
    "    toc = time.time()\n",
    "    print(f\"Time consumed for final highlight: {toc-tic:.1f}s\")\n",
    "    return function_args, response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d732fd5-d7c4-4e33-992f-6c9269da113d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_highlight(final_summary:str, num_highlights:int=7, temperature:float=0.):\n",
    "    final_result, response = highlight_with_openai_function(final_summary, num_highlights=num_highlights, temperature=temperature)\n",
    "\n",
    "    print(\"Final highlight:\")\n",
    "    for k in final_result.keys():\n",
    "        print(f\"\\t{k}: {strlen(final_result.get(k)):,d} words ({get_num_tokens(final_result.get(k)):,d} tokens)\")\n",
    "    print(\"OpenAI usage for final highlight:\")\n",
    "    for k, v in dict(response.usage).items():\n",
    "        print(f\"\\t{k}: {v:,d}\")\n",
    "\n",
    "    final_highlights = '\\n\\n'.join([\"#### \" + k + f\" ({strlen(v):,d} words)\" + \"\\n\" + v for k, v in final_result.items()])\n",
    "\n",
    "    return final_highlights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1307bcd6-79f1-4213-bfd7-d8e7f6e6c46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_highlights = get_final_highlight(to_summary, num_highlights=5, temperature=.2)\n",
    "\n",
    "# print(final_highlights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07153d4c-3f49-4b29-9461-a0cc1f69d913",
   "metadata": {},
   "source": [
    "## Normal way to do final summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c5af39bd-80fb-4eeb-94be-0909a80b125d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_result(topic_groups_summarizations:List[str], num_highlights:int=7, temperature:float=0.):\n",
    "    to_summary = [join_texts(topic_groups_summarizations)]\n",
    "    num_words = strlen(to_summary[0])\n",
    "    num_tokens = get_num_tokens(to_summary[0])\n",
    "    print(f\"To summary text length: {num_words:,d} (num of tokens: {num_tokens:,d})\")\n",
    "    # import ipdb; ipdb.set_trace()\n",
    "    tic = time.time()\n",
    "    if num_tokens < Parameters.PHASE_1_CONTEXT_LENGTH-1000:\n",
    "        summary_model = Parameters.PHASE_1_MODEL\n",
    "        summary_context_length = Parameters.PHASE_1_CONTEXT_LENGTH\n",
    "    else:\n",
    "        summary_model = Parameters.COMPLETION_MODEL\n",
    "        summary_context_length = Parameters.CONTEXT_LENGTH\n",
    "\n",
    "    # highlight_model = Parameters.PHASE_1_MODEL\n",
    "    # highlight_context_length = Parameters.PHASE_1_CONTEXT_LENGTH\n",
    "    final_summary = batch_summary(to_summary, temperature=temperature, batch_size=1, summary_length=Parameters.FINAL_SUMMARY_LENGTH, model=summary_model, context_length=summary_context_length)[0]\n",
    "    toc = time.time()\n",
    "    print(f\"Time consumed for final summarization: {toc-tic:,.1f}s\")\n",
    "    print(f\"To highlight text length: {strlen(final_summary):,d} (num of tokens: {get_num_tokens(final_summary):,d})\")\n",
    "    # tic = time.time()\n",
    "\n",
    "    final_highlight =  get_final_highlight(final_summary, temperature=temperature, num_highlights=num_highlights) # batch_highlights(final_summary, temperature=temperature, num_highlights=num_highlights, model=highlight_model, context_length=highlight_context_length)\n",
    "    # toc = time.time()\n",
    "    # print(f\"Time consumed for final higlights: {toc-tic:,.1f}s\")\n",
    "    final_result = final_highlight + f\"\\n\\n#### Detailed Summary ({strlen(final_summary):,d} words)\\n{final_summary}\"\n",
    "    print(f\"Length of final highlight, in words: {strlen(final_highlight):,d}, in tokens: {get_num_tokens(final_highlight):,d}\")\n",
    "    print(f\"Length of final summary, in words: {strlen(final_summary):,d}, in tokens: {get_num_tokens(final_summary):,d}\")\n",
    "    print(f\"Length of final result, in words: {strlen(final_result):,d}, in tokens: {get_num_tokens(final_result):,d}\")\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c647856c-8dd6-48f2-910a-e1507081f6e5",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e14bfda-36a1-4324-91b9-be1d7b161650",
   "metadata": {},
   "source": [
    "## do_summary method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "31983b6e-9556-498a-8d30-12b3aada3997",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_summary(txt:str, output_file:str, num_highlights:int=7):\n",
    "    # import ipdb; ipdb.set_trace()\n",
    "    txt = re.sub(r'[ ]+', ' ', txt)\n",
    "    num_txt_words = strlen(txt)\n",
    "    print(f\"Num of words: {num_txt_words:,d}\")\n",
    "    num_txt_tokens = get_num_tokens(txt)\n",
    "    print(f\"Num of tokens: {num_txt_tokens:,d}\")\n",
    "    Parameters.PHASE_1_CHUNK_LEN = max(Parameters.PHASE_1_CHUNK_LEN, min(2000, num_txt_tokens // (Parameters.batch_size+1)))\n",
    "\n",
    "    if num_txt_tokens < Parameters.SUMMARIZE_TOPIC_TOKENS:\n",
    "            final_result = get_final_result([txt], num_highlights=num_highlights, temperature=0.)\n",
    "    else:\n",
    "        # Get initial chunks\n",
    "        phase_1_chunk_list = get_phase_1_chunks(txt)\n",
    "\n",
    "        # Clean up text via OpenAI API\n",
    "        tic = time.time()\n",
    "        cleaned_chunk_list0 = batch_cleanup(chunks=phase_1_chunk_list, temperature=0., batch_size=Parameters.batch_size)\n",
    "        toc = time.time()\n",
    "        print(\"Length of cleaned chunks, in words:\", [strlen(c) for c in cleaned_chunk_list0], \"in tokens:\", [get_num_tokens(c) for c in cleaned_chunk_list0])\n",
    "        print(f\"Time consumed for cleanup by OpenAI: {toc-tic:,.1f}s\")\n",
    "\n",
    "        overlapped = Parameters.PHASE_1_CHUNK_OVERLAY\n",
    "\n",
    "        while (get_num_tokens(join_texts(cleaned_chunk_list0)) > Parameters.SUMMARIZE_TOPIC_TOKENS): # Keep the process until the to_summary_text is short enough\n",
    "            # Get chunks\n",
    "            chunks, cleaned_chunk_list = get_chunks(cleaned_chunk_list0, overlapped=overlapped)\n",
    "\n",
    "            # Get embedding for the cunks\n",
    "            chunks_embedding = embedding(chunks)\n",
    "\n",
    "            # Get topic titles (something like: [[0, 1, 2, 3,], [4, 5], [6, 7, 8], [9, 10, 11]])\n",
    "            topics_title = get_topics_title(text_embeds=chunks_embedding, num_topics=strlen(txt) // Parameters.AVERAGE_TOPIC_WORDS, chunks=chunks, )\n",
    "\n",
    "            # Get topics and topic_groups\n",
    "            topics = form_topics_via_topics_title(chunks, topics_title)\n",
    "            topic_groups = get_topic_groups(topics)\n",
    "\n",
    "            # Get summarizations of each group via OpenAI API\n",
    "            phase_2_summaries = summarize_topic_groups(topic_groups, summary_length=Parameters.TOPIC_SUMMARY_LENGTH, temperature=0.2, batch_size=Parameters.batch_size)\n",
    "            cleaned_chunk_list0 = phase_2_summaries\n",
    "            overlapped=0\n",
    "\n",
    "        # Final result\n",
    "        final_result = get_final_result(cleaned_chunk_list0, num_highlights=num_highlights, temperature=0.2)\n",
    "\n",
    "    # Output final result\n",
    "    print('\\n\\n')\n",
    "    print(final_result)\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(final_result)\n",
    "\n",
    "    usages = openai_usage_stat()\n",
    "    print(\"\\n#### OpenAI Usage:\")\n",
    "    print(\"Token Usage:\", usages.get('TokenUsage'))\n",
    "    print(f\"Cost: {usages.get('Dollar_Usage')*100:,.2f} US cents\")\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8997329-3b27-4ee8-954f-ad321563cb36",
   "metadata": {},
   "source": [
    "## Different data sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b165ff1e-9814-442b-9007-de2052ff900d",
   "metadata": {},
   "source": [
    "### Youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e7631365-024b-4933-9019-a20458753334",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_youtube_summary(video_id:str, num_highlights:int=7, output_file:str=None, start_pos:int=None, end_pos:int=None):\n",
    "    if not output_file:\n",
    "        output_file = f\"results/youtube_{video_id}.txt\"\n",
    "    # Get text\n",
    "    txt = get_youtube_transcript(video_id)\n",
    "    # Do summary\n",
    "    final_result = do_summary(txt[start_pos:end_pos], output_file=output_file, num_highlights=num_highlights)\n",
    "    \n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb8c29c-5362-4e71-9441-dbc49e4f32fb",
   "metadata": {},
   "source": [
    "### Bilibili\n",
    "\n",
    "[https://github.com/huilongyeo/bilibiliGetSrt](https://github.com/huilongyeo/bilibiliGetSrt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "36e22486-f755-4506-a14c-80dc1e34efb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_bilibili_summary(video_id:str, pageNo:int=1, num_highlights:int=7, output_file:str=None, start_pos:int=None, end_pos:int=None):\n",
    "    if not output_file:\n",
    "        output_file = f\"results/bilibili_{video_id}.txt\"\n",
    "    # Get text\n",
    "    txt = get_bilibili_transcript(video_id, pageNo)\n",
    "    # Do summary\n",
    "    final_result = do_summary(txt[start_pos:end_pos], output_file=output_file, num_highlights=num_highlights)\n",
    "    \n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f225cdd-4a3a-4b7c-8e5c-f0028e58e6a7",
   "metadata": {},
   "source": [
    "### Text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c71e0689-c4ad-458c-b324-c7b521bcd960",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_txtfile_summary(txt_path:str, num_highlights:int=7, output_file:str=None, start_pos:int=None, end_pos:int=None):\n",
    "    if not output_file:\n",
    "        output_file = re.sub(r\".*/\", \"\", txt_path)\n",
    "        output_file = \"results/text_\" + re.sub(r\"\\.[^.]*$\", \"\", output_file) + \".txt\"\n",
    "    # Get text\n",
    "    txt = get_txt_from_file(txt_path)\n",
    "    # import ipdb; ipdb.set_trace()\n",
    "    # Do summary\n",
    "    final_result = do_summary(txt[start_pos:end_pos], output_file=output_file, num_highlights=num_highlights)\n",
    "    \n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389c5d8f-756c-4115-94ad-92de2475af9d",
   "metadata": {},
   "source": [
    "### PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a37eeabf-4cd4-49f4-a384-7ad56ba4d675",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import sys\n",
    "!{sys.executable} -m pip install PyPDF2 requests # aspose-words\n",
    "\n",
    "import requests\n",
    "from PyPDF2 import PdfReader\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def open_pdf(pdf_resource:str, startPage:int=1, endPage:int=None):\n",
    "    # Check if the input name is a URL or a local filename\n",
    "    parsed_url = urlparse(pdf_resource)\n",
    "    is_url = parsed_url.scheme != '' and parsed_url.netloc != ''\n",
    "\n",
    "    if is_url:\n",
    "        # The input name is a URL\n",
    "        response = requests.get(pdf_resource)\n",
    "        pdf_content = response.content\n",
    "\n",
    "        # Create a temporary PDF file from the URL content\n",
    "        pdf_file = open('temp.pdf', 'wb')\n",
    "        pdf_file.write(pdf_content)\n",
    "        pdf_file.close()\n",
    "\n",
    "        # Open the PDF file using PyPDF2\n",
    "        pdf = PdfReader('temp.pdf')\n",
    "        # Access the PDF document properties or perform other operations\n",
    "        pages = pdf.pages\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "        print(f\"The PDF has {len(pages):,d} pages.\")\n",
    "\n",
    "        # Clean up the temporary PDF file\n",
    "        import os\n",
    "        os.remove('temp.pdf')\n",
    "    else:\n",
    "        # The input name is a local filename\n",
    "        pdf = PdfReader(pdf_resource)\n",
    "        # Access the PDF document properties or perform other operations\n",
    "        pages = pdf.pages\n",
    "        print(f\"The PDF has {len(pages):,d} pages.\")\n",
    "    \n",
    "    txt = ''.join([c.extract_text() for c in pages[startPage-1:endPage]])\n",
    "    return pdf, txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3edbd659-99f9-4d89-bd2f-9fbb26073da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_pdf_summary(pdf_source:str, pdf_name:str, startPage:int=1, endPage:int=None, num_highlights:int=7, output_file:str=None, start_pos:int=None, end_pos:int=None):\n",
    "    if not output_file:\n",
    "        output_file = re.sub(r\".*/\", \"\", pdf_source)\n",
    "        output_file = \"results/pdf_\" + re.sub(r\"\\.[^.]*$\", \"\", output_file) + \".txt\"\n",
    "    if pdf_name:\n",
    "        output_file = f\"\"\"results/pdf_{re.sub(\"/\", \"_\", pdf_name)}.txt\"\"\"\n",
    "    # Get text\n",
    "    pdf,  txt = open_pdf(pdf_source, startPage, endPage)\n",
    "    print(f\"We have {strlen(txt):,d} words ({get_num_tokens(txt):,d}) tokens in the PDF file.\")\n",
    "    # import ipdb; ipdb.set_trace()\n",
    "    # Do summary\n",
    "    final_result = do_summary(txt[start_pos:end_pos], output_file=output_file, num_highlights=num_highlights)\n",
    "    \n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79943f6-c2ba-4c51-a19f-fdacf56ae513",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b350c7d9-9548-4cb1-a8de-298e9bee7ab3",
   "metadata": {},
   "source": [
    "## Text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "644a474f-0913-421a-bc16-1c1349d99790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 'openai_usages' in globals():\n",
    "#     del openai_usages\n",
    "\n",
    "# tic = time.time()    \n",
    "# txt_path = 'data/mls.txt'\n",
    "# txt_path = 'data/stateoftheunion.txt'\n",
    "# txt_path = 'data/deeplearning.ai/diffusion/01-intuition.txt'\n",
    "\n",
    "# final_result = do_txtfile_summary(txt_path, num_highlights=4, start_pos=None, end_pos=None)\n",
    "# toc = time.time()\n",
    "# print(f\"\\nAltogether time consumed: {toc-tic:,.1f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba93fc73-3154-469d-9492-c2522d6021c0",
   "metadata": {},
   "source": [
    "## Youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "154327f3-e8fc-4977-83ee-c54e0fd27c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 'openai_usages' in globals():\n",
    "#     del openai_usages\n",
    "\n",
    "# tic = time.time()    \n",
    "# video_id = 'UVn2NroKQCw' # Using LangChain Output Parsers to get what you want out of LLMs\n",
    "# video_id = 'KUDn7bVyIfc' # Converting a LangChain App from OpenAI to OpenSource\n",
    "# video_id = 'uzJX9Wkp0Qc' # The Secrets to Become a Better Software Engineer 6:46\n",
    "# video_id = '6UFtRwWnHws' # Building a LangChain Custom Medical Agent with Memory 17:46\n",
    "# video_id = 'uzJX9Wkp0Qc' # The Secrets to Become a Better Software Engineer 6:46\n",
    "# video_id = 'qaPMdcCqtWk' # 5 Levels Of LLM Summarizing: Novice to Expert 19:18\n",
    "# video_id = 'MPrJF3F4mHc' # Attack and Detect: VulnDC:2 vs Splunk & Security Onion 1:27:45\n",
    "# video_id = 'EmNQuK-E0kI' # What is The Quantum Wave Function, Exactly? 13:04\n",
    "# video_id = 'to7vCdkLi4s' # Reinforcement Learning with Augmented Data (Paper Explained) 22:14\n",
    "# video_id = 'x8pW19wKfXQ' # RWKV: Reinventing RNNs for the Transformer Era (Paper Explained) 1:02:16\n",
    "# video_id = 'ddG2fM9i4Kk' # OpenAssistant RELEASED! The world's best open-source Chat AI! 21:05\n",
    "# video_id = '4Cclp6yPDuw' # Scaling Transformer to 1M tokens and beyond with RMT (Paper Explained) 24:33\n",
    "# video_id = 'Eug2clsLtFs' # Understanding ReACT with LangChain 21:09\n",
    "# video_id = 'ghzsBm8vOms' # What is Platform Engineering and how it fits into DevOps and Cloud world 42:41\n",
    "# video_id = '87ZFvJ7_-n0' # How to build an OpenAPI Specification using YAML? 15:30\n",
    "# video_id = 'mViFmjcDOoA' # OpenAPI 3.0 Tutorial| Swagger Tutorial For Beginners | Design REST API Using Swagger Editor 24:57\n",
    "# video_id = 'z2aCZBAtWXs' # What can you do with 16K tokens in LangChain? 16:54\n",
    "# video_id = 'DiLPn_ldAAQ' # Lecture 21 (update): SHA-3 Hash Function by Christof Paar 1:15:06\n",
    "# video_id = 'aMckXIqqzeI' # How Diffie-Hellman Fails in Practice 1:13:19\n",
    "# video_id = 'HoKDTa5jHvg' # Diffusion Models | Paper Explanation | Math Explained 33:26\n",
    "# video_id = 'yfgfJAkzliw' # Jared Zoneraich - Future Of Prompt Engineering, Management, and Collaboration 24:50\n",
    "# video_id = '9zEXov_L0os' # Hands-on Ransomware: Exploring Cybercrime 43:27\n",
    "# video_id = 'dXxQ0LR-3Hg' # Chat with Multiple PDFs | Langchain app tutorial 1:07:29\n",
    "# video_id = '4KXK6c6TVXQ' # OpenAI Functions + LangChain : Building a Multi Tool Agent 18:51\n",
    "# video_id = 'I4n0Wj2PHQA' # OpenAI's Game Changing Updates. New Features! Bigger Savings! 12:16\n",
    "# video_id = '0lOSvOoF2to' # OpenAI GPT-4 Function Calling: Unlimited Potential 23:48\n",
    "# video_id = 'OdIHUdQ1-eQ' # PyPDF2 Crash Course - Working with PDFs in Python 52:19\n",
    "# video_id = 'a8hMgIcUEnE' # Tagging and Extraction - Classification using OpenAI Functions 16:13\n",
    "# video_id = 'Tkijsu129M0' # GPT-4 solves MIT Exam with 100% ACCURACY 31:04\n",
    "# video_id = 'ut5kp56wW_4' # Tree of Thoughts: Deliberate Problem Solving with Large Language Models (Full Paper Review) 29:28\n",
    "# video_id = 'FcUAbIQH_XY' #  14:58\n",
    "# video_id = 'pEkxRQFNAs4' # Extract Topics From Video/Audio With LLMs 17:33\n",
    "# video_id = '2lnW1PSB2_g' # How to write Tree of Thoughts Prompts 11:35\n",
    "# video_id = '4P-hPldEUiE' # Stable Diffusion Automation: Turbocharge Your AI Image Generation 10:27\n",
    "# video_id = 'g2BRIuln4uc' # Intuition Behind Self-Attention Mechanism in Transformer Networks 39:24\n",
    "# video_id = 'ANszao6YQuM' # Stanford CS230: Deep Learning | Autumn 2018 | Lecture 4 - Adversarial Attacks / GANs 1:22:59\n",
    "# video_id = 'cQYmePtLAT0' # Adversarial Attack and Defense on Deep Learning 3:17\n",
    "# video_id = 'zk-E2NKFjk4' # Introduction to Adversarial Attack on Machine learning model 1:36:55\n",
    "\n",
    "# video_id = '' # \n",
    "# video_id = '' # \n",
    "# video_id = '' # \n",
    "# video_id = '' # \n",
    "# video_id = '3iAaySrjZ4w' # The future of Bitcoin 33:10\n",
    "\n",
    "# final_result = do_youtube_summary(video_id, num_highlights=7)\n",
    "# toc = time.time()\n",
    "# print(f\"\\nAltogether time consumed: {toc-tic:,.1f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701349e4-43b2-4ef1-9a66-ecf633bbcae7",
   "metadata": {},
   "source": [
    "## Bilibili"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "93702f4e-17bb-4d91-9814-066230ffaea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of words: 10,765\n",
      "Num of tokens: 12,273\n",
      "To summary text length: 10,765 (num of tokens: 12,273)\n",
      "batch_summary model = gpt-3.5-turbo-16k-0613\n",
      "Inside _get_completion_func: num_prompt_tokens = 12,427, max_tokens = 3,573, model = gpt-3.5-turbo-16k-0613, context_length = 16,000\n",
      "Time consumed for final summarization: 14.1s\n",
      "To highlight text length: 639 (num of tokens: 728)\n",
      "Number of tokens for final highlight: 736 (words: 639). Highlight model: gpt-3.5-turbo-0613\n",
      "Time consumed for final highlight: 7.5s\n",
      "Final highlight:\n",
      "\tTitle: 5 words (5 tokens)\n",
      "\tShort Synopsis: 114 words (132 tokens)\n",
      "\tKey Points: 116 words (133 tokens)\n",
      "OpenAI usage for final highlight:\n",
      "\tprompt_tokens: 865\n",
      "\tcompletion_tokens: 297\n",
      "\ttotal_tokens: 1,162\n",
      "Length of final highlight, in words: 249, in tokens: 291\n",
      "Length of final summary, in words: 639, in tokens: 728\n",
      "Length of final result, in words: 893, in tokens: 1,027\n",
      "\n",
      "\n",
      "\n",
      "#### Title (5 words)\n",
      "Introduction to Machine Learning Course\n",
      "\n",
      "#### Short Synopsis (114 words)\n",
      "The article provides a detailed overview of a machine learning course taught by Professor Li Hongyi. It covers the course schedule, classroom arrangements, course requirements, and the use of live streaming for lectures. The course assumes basic knowledge of mathematics and Python programming. It focuses on deep learning techniques and covers topics such as computer vision, natural language processing, and reinforcement learning. Assignments provide hands-on experience with various applications. Course materials are available on the course website, and assignments are graded based on reports and a leaderboard. The instructor emphasizes academic integrity and provides guidelines for the course. Students can apply for exemptions from course requirements. Office hours and support are available for students.\n",
      "\n",
      "#### Key Points (116 words)\n",
      "- The machine learning course is taught by Professor Li Hongyi\n",
      "- The course schedule, classroom arrangements, and the use of live streaming for lectures are discussed\n",
      "- The course assumes basic knowledge of mathematics and Python programming\n",
      "- The course focuses on deep learning techniques and covers topics such as computer vision, natural language processing, and reinforcement learning\n",
      "- Assignments provide hands-on experience with various applications\n",
      "- Course materials are available on the course website\n",
      "- Assignments are graded based on reports and a leaderboard\n",
      "- The instructor emphasizes academic integrity and provides guidelines for the course\n",
      "- Students can apply for exemptions from course requirements\n",
      "- Office hours and support are available for students\n",
      "\n",
      "#### Detailed Summary (639 words)\n",
      "The article is a transcript of a video introducing a machine learning course taught by Professor Li Hongyi. The video provides information about the course schedule, classroom arrangements, and the use of live streaming for lectures. It also discusses the course requirements, including the completion of assignments and the absence of midterm and final exams. The course is designed to be completed online, allowing students to listen to lectures and submit assignments remotely.\n",
      "\n",
      "The course assumes that students have a basic understanding of mathematics, including calculus, linear algebra, and probability. It also assumes that students are familiar with Python programming. While the course focuses on deep learning techniques, it serves as an introduction to machine learning as a whole. The instructor emphasizes that deep learning is a widely used and highly regarded technique in the field of machine learning.\n",
      "\n",
      "The course covers a wide range of topics, including computer vision, natural language processing, and reinforcement learning. The assignments are designed to provide hands-on experience with these applications. For example, students will work on tasks such as image classification, image generation, translation, question answering, speech recognition, and reinforcement learning in gaming.\n",
      "\n",
      "All course materials, including lecture recordings and assignment instructions, are available on the course website. The instructor recommends that students watch the lecture recordings in the \"pre\" section of the website, as they provide the necessary background knowledge for completing the assignments. The lectures are designed to be self-contained, allowing students to choose how much they want to learn beyond the required material.\n",
      "\n",
      "The assignments are graded based on a report and a leaderboard. The report consists of a set of questions that students need to answer, while the leaderboard ranks students based on their performance. The instructor emphasizes that the leaderboard is not the sole determinant of a student's grade, as the final grade is based on the highest scores from ten out of the fifteen assignments. The instructor also mentions that completing all fifteen assignments with a score of nine or above in each assignment will earn students a special reward.\n",
      "\n",
      "The instructor provides some disclaimers and guidelines for the course. He clarifies that the course does not teach Python programming or the use of specific packages, such as PyTorch. The teaching assistants are available to answer questions related to machine learning and PyTorch, but they may not be able to assist with other topics. The instructor also acknowledges that training deep neural networks can be time-consuming and frustrating, but encourages students to persevere and not get discouraged by slow progress.\n",
      "\n",
      "The instructor emphasizes the importance of academic integrity and warns against plagiarism. Students are not allowed to copy others' work or directly use others' model results. The instructor also advises against using public datasets or searching for answers online, as this violates the course rules. Students are expected to train their models using the provided training data and submit their own results.\n",
      "\n",
      "The course offers an opportunity for students to apply for an exemption from the course requirements. Students from the School of Electrical and Computer Engineering, as well as related disciplines, can apply for an exemption by filling out a Google form. The instructor mentions that the exemption is not guaranteed and will be determined on a case-by-case basis.\n",
      "\n",
      "The instructor concludes by providing information about office hours and support for students. There will be online TA hours and in-person TA hours for students to ask questions and seek assistance. Students are encouraged to use the online forum for general questions, and to email the teaching assistants for more specific or private inquiries.\n",
      "\n",
      "Overall, the article provides a detailed overview of the machine learning course, including the course structure, assignments, guidelines, and support available to students. It emphasizes the importance of academic integrity and the need for students to take responsibility for their own learning.\n",
      "\n",
      "#### OpenAI Usage:\n",
      "Token Usage: [{'object': 'chat.completion', 'model': 'gpt-3.5-turbo-16k-0613', 'prompt_tokens': 12425, 'completion_tokens': 728, 'total_tokens': 13153}, {'object': 'chat.completion', 'model': 'gpt-3.5-turbo-0613', 'prompt_tokens': 865, 'completion_tokens': 297, 'total_tokens': 1162}]\n",
      "Cost: 4.21 US cents\n",
      "\n",
      "Altogether time consumed: 24.1 seconds\n"
     ]
    }
   ],
   "source": [
    "if 'openai_usages' in globals():\n",
    "    del openai_usages\n",
    "\n",
    "tic = time.time()    \n",
    "video_id, pageNo = 'BV1TD4y137mP', 1 # 2023 P1 - Introduction 40:56\n",
    "video_id, pageNo = 'BV1TD4y137mP', 2 # 2023 P2 - ChatGPT  19:58\n",
    "\n",
    "\n",
    "\n",
    "final_result = do_bilibili_summary(video_id, pageNo, num_highlights=7)\n",
    "toc = time.time()\n",
    "print(f\"\\nAltogether time consumed: {toc-tic:,.1f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd0d297-9b1a-4d56-9bf7-6466d10babe4",
   "metadata": {},
   "source": [
    "## PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "df849c68-c46b-4540-83d3-46c8c43d759a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 'openai_usages' in globals():\n",
    "#     del openai_usages\n",
    "\n",
    "# tic = time.time()    \n",
    "# pdf_resource, pdf_name = \"https://arxiv.org/pdf/2210.03629.pdf\", \"REACT: SYNERGIZING REASONING AND ACTING IN LANGUAGE MODELS\" # Page 33\n",
    "# pdf_resource, pdf_name = \"https://arxiv.org/pdf/2305.13860.pdf\", \"Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study\" # Page 12\n",
    "# pdf_resource, pdf_name, startPage, endPage = \"https://arxiv.org/pdf/2210.16886.pdf\", \"DiffusER: Discrete Diffusion via Edit-Based Reconstruction\", 1, 10 # Page 13\n",
    "# pdf_resource, pdf_name, startPage, endPage = \"https://arxiv.org/pdf/2201.11903.pdf\", \"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\", 1, 19 # Page 43\n",
    "# pdf_resource, pdf_name, startPage, endPage = \"http://hanj.cs.illinois.edu/cs412/bk3/KL-divergence.pdf\", \"Kullback-Leibler Divergence\", 1, 19 # Page 43\n",
    "# pdf_resource, pdf_name, startPage, endPage = \"https://arxiv.org/pdf/1706.03762.pdf\", \"Attention is all you need\", 1, 10 # Page 15\n",
    "\n",
    "# final_result = do_pdf_summary(pdf_resource, pdf_name=pdf_name, startPage=startPage, endPage=endPage, num_highlights=7)\n",
    "# toc = time.time()\n",
    "# print(f\"\\nAltogether time consumed: {toc-tic:,.1f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c8e905c5-9625-4d06-8633-3cf405c5cae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Title (5 words)\n",
      "Introduction to Machine Learning Course\n",
      "\n",
      "#### Short Synopsis (59 words)\n",
      "The video introduces a machine learning course taught by Professor Li Hongyi. It provides information about the course schedule, assignments, grading system, and academic integrity policies. It also discusses the use of the Cargo platform for submitting assignments and viewing the leaderboard. The video emphasizes the importance of academic integrity and encourages students to seek help from the TAs.\n",
      "\n",
      "#### Key Points (42 words)\n",
      "- Machine learning course taught by Professor Li Hongyi\n",
      "- Course schedule, assignments, grading system, and academic integrity policies\n",
      "- Use of the Cargo platform for submitting assignments and viewing the leaderboard\n",
      "- Importance of academic integrity and seeking help from TAs\n",
      "\n",
      "#### Detailed Summary (586 words)\n",
      "The article is a transcript of a video introducing a machine learning course taught by Professor Li Hongyi. The video provides information about the course schedule, classroom arrangements, and the use of live streaming for lectures. It also discusses the course requirements, including the ability to read and write Python and a background in mathematics. The course focuses on deep learning and covers various applications such as computer vision, natural language processing, and reinforcement learning.\n",
      "\n",
      "The video emphasizes that the course is the first step in learning machine learning and provides a foundation for further exploration in the field. It also mentions that the course covers a wide range of topics but only provides a shallow introduction to each, as the goal is to familiarize students with the concepts and techniques rather than delve deeply into each topic.\n",
      "\n",
      "The video introduces the course assignments, which include tasks related to computer vision, natural language processing, and reinforcement learning. The assignments are designed to provide hands-on experience with machine learning applications and techniques. The video also mentions that all course materials, including lecture recordings and assignment instructions, will be available on the course website.\n",
      "\n",
      "The video discusses the grading system for the course, which is based on the highest scores achieved in ten out of the fifteen assignments. It also mentions that there is no midterm or final exam, and the course can be completed entirely online. The video encourages students to complete as many assignments as possible but emphasizes that only the highest-scoring ten assignments will be considered for the final grade.\n",
      "\n",
      "The video introduces the use of the Cargo platform for submitting assignments and viewing the leaderboard. It explains that the leaderboard shows the rankings of students based on their assignment scores and mentions that there is a public leaderboard and a hidden leaderboard. The public leaderboard shows real-time rankings, while the hidden leaderboard is only revealed at the end of the course.\n",
      "\n",
      "The video emphasizes the importance of academic integrity and warns against plagiarism. It states that copying others' work, including code and reports, is strictly prohibited. It also mentions that using public resources, such as Google Colab, is allowed but should be disclosed in the report. The video advises students to protect their work and not share it with others.\n",
      "\n",
      "The video provides information about the process of applying for an add-drop request for the course. It states that add-drop requests are open to students from the School of Electrical and Electronic Engineering, as well as students from other departments who have been approved by their respective departments. The video mentions that the add-drop request form should be filled out and submitted by a specific deadline.\n",
      "\n",
      "The video concludes by providing information about office hours with the teaching assistants (TAs) and the availability of TA support on the NTU COOL platform. It mentions that TA hours will be held both online and in person, and students can ask questions on the NTU COOL platform or via email. The video encourages students to seek help from the TAs and emphasizes that the TAs will do their best to respond to questions within 24 hours.\n",
      "\n",
      "In summary, the video introduces a machine learning course taught by Professor Li Hongyi. It provides information about the course schedule, assignments, grading system, and academic integrity policies. It also discusses the use of the Cargo platform for submitting assignments and viewing the leaderboard. The video emphasizes the importance of academic integrity and encourages students to seek help from the TAs.\n"
     ]
    }
   ],
   "source": [
    "print(final_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
