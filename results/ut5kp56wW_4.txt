#### ç®€çŸ­æ¦‚è¦
è¿™ç¯‡åä¸ºã€Šæ€ç»´ä¹‹æ ‘ï¼šåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œæœ‰æ„è¯†çš„é—®é¢˜è§£å†³ã€‹çš„è®ºæ–‡æå‡ºäº†ä¸€ç§ç‹¬ç‰¹çš„è§£ç æŠ€æœ¯ï¼Œåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ã€‚ä½œè€…å»ºè®®ä¸ä»…ä»…ä¸€æ¬¡æ€§è¯¢é—®æ¨¡å‹çš„è¾“å‡ºï¼Œè€Œæ˜¯ä½¿ç”¨å¯¹è¯­è¨€æ¨¡å‹è¾“å‡ºçš„æ˜¾å¼æ ‘æœç´¢ã€‚è¿™ç§æ–¹æ³•å…è®¸åˆ†æ”¯å’Œå›æº¯ï¼Œå¯¹äºéœ€è¦è¿›è¡Œè°ƒæŸ¥æ¨¡å¼çš„ä»»åŠ¡ç‰¹åˆ«æœ‰ç”¨ã€‚è®ºæ–‡ä»‹ç»äº†è§£ç æŠ€æœ¯ï¼Œå¹¶æå‡ºäº†é¢„è®¡èƒ½å¤Ÿè¡¨ç°è‰¯å¥½çš„æ–°ä»»åŠ¡ã€‚

#### ä¸»è¦è§‚ç‚¹
- ğŸŒ³ è¯¥è®ºæ–‡çš„ä¸»è¦è§‚ç‚¹æ˜¯é€šè¿‡è¾“å…¥-è¾“å‡ºæç¤ºæ¥æ¿€å‘è¯­è¨€æ¨¡å‹ã€‚è¿™æ„å‘³ç€æŒ‡å®šä»»åŠ¡å¹¶å¯é€‰æ‹©æä¾›è¾“å‡ºæ ¼å¼ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥è¦æ±‚æ¨¡å‹ä»¥ç‰¹å®šæ ¼å¼ç»™è€æ¿å†™ä¸€å°ç”µå­é‚®ä»¶ã€‚è¿™ç§æ–¹æ³•ç±»ä¼¼äºæ€ç»´é“¾æç¤ºæŠ€æœ¯ï¼Œå…¶ä¸­æ¨¡å‹è¢«æŒ‡ç¤ºæ˜ç¡®æä¾›é—®é¢˜è§£å†³çš„ä¸­é—´æ­¥éª¤ã€‚ç„¶è€Œï¼Œä½œè€…è®¤ä¸ºæ¶‰åŠæ¨¡å‹è¾“å‡ºçš„æ ‘æœç´¢çš„æ€ç»´ä¹‹æ ‘æŠ€æœ¯å¯ä»¥äº§ç”Ÿæ›´å¥½çš„ç»“æœã€‚æ¨¡å‹æœ¬èº«è¯„ä¼°æ ‘ä¸­çš„çŠ¶æ€ï¼Œå…è®¸åˆ†æ”¯å’Œå›æº¯ã€‚

#### è¯„ä¼°ä»»åŠ¡
- ğŸ® è®ºæ–‡æ¢è®¨äº†ä¸‰ä¸ªä¸»è¦ä»»åŠ¡æ¥è¯„ä¼°æ€ç»´ä¹‹æ ‘æŠ€æœ¯çš„æœ‰æ•ˆæ€§ã€‚ç¬¬ä¸€ä¸ªä»»åŠ¡æ˜¯24ç‚¹æ¸¸æˆï¼Œæ¨¡å‹è¢«ç»™å®šå››ä¸ªæ•°å­—ï¼Œå¹¶è¦æ±‚æä¾›ä¸€ä¸ªç»“æœä¸º24çš„æ•°å­¦è¡¨è¾¾å¼ã€‚ä½œè€…æŒ‡å‡ºï¼Œè™½ç„¶è¯¥æŠ€æœ¯å¯ä»¥å¸®åŠ©æ¨¡å‹è§£å†³è¿™ç±»é—®é¢˜ï¼Œä½†æç¤ºéœ€è¦éå¸¸å…·ä½“ï¼Œç±»ä¼¼äºç¼–ç¨‹ç®—æ³•ã€‚ç¬¬äºŒä¸ªä»»åŠ¡æ˜¯åˆ›æ„å†™ä½œï¼Œè¦æ±‚æ¨¡å‹ç”Ÿæˆä¸€ä¸ªæ•…äº‹ã€‚ç»“æœæ˜¾ç¤ºï¼Œæ€ç»´ä¹‹æ ‘æŠ€æœ¯åœ¨è¿™ä¸ªä»»åŠ¡ä¸­çš„è¡¨ç°ç•¥ä¼˜äºå…¶ä»–æç¤ºæ–¹æ³•ï¼Œä½†å·®å¼‚ä¸æ˜¾è‘—ã€‚ç¬¬ä¸‰ä¸ªä»»åŠ¡æ˜¯è¿·ä½ å¡«å­—æ¸¸æˆï¼Œæ¨¡å‹è¢«ç»™å®šä¸€ä¸ªå­—æ¯ç½‘æ ¼å’Œå¡«å…¥å•è¯çš„çº¿ç´¢ã€‚æ€ç»´ä¹‹æ ‘æŠ€æœ¯åœ¨è¿™ä¸ªä»»åŠ¡ä¸­è¡¨ç°å‡ºæé«˜çš„æ•ˆæœï¼Œå› ä¸ºå®ƒå…è®¸å›æº¯å’Œæ¢ç´¢ä¸åŒçš„è·¯å¾„ã€‚

#### å‰ªæå’Œå›æº¯çš„å½±å“
- ğŸŒ± è®ºæ–‡æå‡ºäº†å‡ ç§æ¶ˆèå®éªŒï¼Œè¯„ä¼°äº†å‰ªæå’Œå›æº¯å¯¹æ€ç»´ä¹‹æ ‘æŠ€æœ¯æ€§èƒ½çš„å½±å“ã€‚ç»“æœæ˜¾ç¤ºï¼Œå‰ªæå’Œå›æº¯æ˜¾è‘—æé«˜äº†æ¨¡å‹è§£å†³å¡«å­—æ¸¸æˆçš„èƒ½åŠ›ã€‚æœ‰è¶£çš„æ˜¯ï¼Œåœ¨è¯„ä¼°è¿‡ç¨‹ä¸­æ·»åŠ ä¸€ä¸ªå§‹ç»ˆé€‰æ‹©æœ€ä½³æ€ç»´çš„ç¥è°•ï¼Œè¿›ä¸€æ­¥æé«˜äº†æ€§èƒ½ã€‚ç„¶è€Œï¼Œå¯¹å•è¯å’Œå­—æ¯æˆåŠŸç‡çš„å½±å“å¹¶ä¸æ˜¾è‘—ã€‚ä½œè€…è¿˜æŒ‡å‡ºï¼Œå»é™¤å‰ªææˆ–å›æº¯ä¼šå¯¼è‡´è§£å†³çš„æ¸¸æˆæ€»æ•°å‡å°‘ï¼Œè¡¨æ˜è¿™äº›æŠ€æœ¯åœ¨è§„åˆ’å’Œæ¢ç´¢ä¸­çš„é‡è¦æ€§ã€‚

#### æ€»ç»“
- ğŸ“š æ€»ä½“è€Œè¨€ï¼Œè¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œé—®é¢˜è§£å†³ä»»åŠ¡çš„æœ‰è¶£æ–¹æ³•ã€‚æ€ç»´ä¹‹æ ‘æŠ€æœ¯ä»¥å…¶æ ‘æœç´¢å’Œå›æº¯èƒ½åŠ›ï¼Œåœ¨éœ€è¦æ¢ç´¢å’Œæ¨ç†çš„ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºæ½œåŠ›ã€‚ç„¶è€Œï¼Œå½“å‰å®ç°ä¸¥é‡ä¾èµ–äºæ˜ç¡®çš„æç¤ºå’Œçº¦æŸï¼Œé™åˆ¶äº†å…¶é€šç”¨æ€§ã€‚æœªæ¥çš„ç ”ç©¶åº”è¯¥ä¸“æ³¨äºå¼€å‘ä¸€ç§æ›´çµæ´»å’Œé€šç”¨çš„é—®é¢˜è§£å†³æ–¹æ³•ï¼Œåˆ©ç”¨è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ï¼Œè€Œæ— éœ€ç‰¹å®šçš„æç¤ºã€‚

### Detailed Summary
The paper titled "Tree of Thoughts: Deliberate Problem Solving with Large Language Models" proposes a decoding technique that utilizes large language models in a unique way. Instead of simply asking the model for its output once, the authors suggest using an explicit tree search over the outputs of the language model. This approach allows for branching off and backtracking, which can be particularly useful for tasks that require a pattern of investigation. The paper introduces the decoding technique and presents new tasks where it is expected to perform well.

The main proposition of the paper is to prompt a language model using input-output prompting. This means specifying the task and optionally providing an output format. For example, one could ask the model to write an email to their boss in a specific format. This approach is similar to the Chain of Thought prompting technique, where the model is instructed to explicitly provide intermediate steps in problem-solving. However, the authors argue that the Tree of Thoughts technique, which involves a tree search over the model's outputs, can yield better results. The model itself evaluates the states in the tree, allowing for branching and backtracking.

The paper explores three main tasks to evaluate the effectiveness of the Tree of Thoughts technique. The first task is the game of 24, where the model is given four numbers and asked to come up with a mathematical expression that results in 24. The authors note that while the technique can help the model solve these types of problems, the prompts need to be very specific, resembling programming algorithms. The second task is creative writing, where the model is asked to generate a story. The results show that the Tree of Thoughts technique performs slightly better than other prompting methods, but the difference is not significant. The third task is mini crosswords, where the model is given a grid of letters and clues to fill in the words. The Tree of Thoughts technique proves to be highly effective in this task, as it allows for backtracking and exploration of different paths.

The authors acknowledge that the prompts used in the evaluation tasks are quite specific and resemble programming algorithms. They argue that the goal of their research is not to solve specific tasks, as those can be readily solved with specialized NLP pipelines. Instead, they aim to develop a general problem-solving approach that leverages language models. However, the current implementation of the Tree of Thoughts technique relies heavily on explicit prompts and constraints, which limits its generality.

The paper presents several ablations to evaluate the impact of pruning and backtracking on the performance of the Tree of Thoughts technique. The results show that pruning and backtracking significantly improve the model's ability to solve crossword puzzles. Interestingly, the addition of an oracle, which always selects the best thought during evaluation, further improves performance. However, the impact on word and letter success rates is not substantial. The authors also note that removing pruning or backtracking leads to a decrease in the total number of solved games, indicating the importance of these techniques in planning and exploration.

Overall, the paper presents an interesting approach to utilizing large language models in problem-solving tasks. The Tree of Thoughts technique, with its tree search and backtracking capabilities, shows promise in tasks that require exploration and reasoning. However, the current implementation heavily relies on explicit prompts and constraints, limiting its generality. Future research should focus on developing a more flexible and general problem-solving approach that leverages the capabilities of language models without the need for specific prompts.