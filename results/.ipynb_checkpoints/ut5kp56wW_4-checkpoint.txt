ç®€è¦æ¦‚è¿°ï¼š
è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„è§£ç æŠ€æœ¯ï¼Œç§°ä¸ºâ€œTree of Thoughtsâ€ï¼Œå¯ä»¥ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹çš„é—®é¢˜è§£å†³ã€‚è¯¥æŠ€æœ¯æ¶‰åŠå¯¹è¯­è¨€æ¨¡å‹è¾“å‡ºçš„æ˜¾å¼æ ‘æœç´¢ï¼Œæ¨¡å‹æœ¬èº«å¯¹ä¸åŒçŠ¶æ€è¿›è¡Œè¯„ä¼°ã€‚è¿™å…è®¸åˆ†æ”¯å’Œå›æº¯ï¼Œå¯¹äºéœ€è¦è°ƒæŸ¥é—®é¢˜çš„ä»»åŠ¡éå¸¸æœ‰å¸®åŠ©ã€‚è®ºæ–‡æå‡ºäº†ä¸€äº›æ–°ä»»åŠ¡ï¼Œé¢„è®¡è§£ç æŠ€æœ¯å°†åœ¨è¿™äº›ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼Œå¹¶ä¸”ç»“æœè¡¨æ˜å®ƒç¡®å®åœ¨è¿™äº›ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ã€‚

äº®ç‚¹ï¼š
- ğŸŒ³ â€œTree of Thoughtsâ€æ˜¯ä¸€ç§æ–°çš„è§£ç æŠ€æœ¯ï¼Œå¯ä»¥ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹çš„é—®é¢˜è§£å†³ã€‚
- ğŸ¤– è¯¥æŠ€æœ¯æ¶‰åŠå¯¹è¯­è¨€æ¨¡å‹è¾“å‡ºçš„æ˜¾å¼æ ‘æœç´¢ï¼Œæ¨¡å‹æœ¬èº«å¯¹ä¸åŒçŠ¶æ€è¿›è¡Œè¯„ä¼°ã€‚
- ğŸ“ è®ºæ–‡æå‡ºäº†ä¸€äº›æ–°ä»»åŠ¡ï¼Œé¢„è®¡è§£ç æŠ€æœ¯å°†åœ¨è¿™äº›ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ã€‚
- ğŸ† ç»“æœè¡¨æ˜ï¼Œè¯¥æŠ€æœ¯åœ¨å¤šä¸ªä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼ŒåŒ…æ‹¬ç®—æœ¯å•è¯é—®é¢˜ã€é€»è¾‘æ¨ç†å’Œæ–‡æœ¬å®Œæˆã€‚
- ğŸ“ â€œTree of Thoughtsâ€æŠ€æœ¯æ˜¯ä¸€ç§æ€ç»´é“¾æç¤ºçš„ç±»å‹ï¼Œå…¶ä¸­è¦æ±‚æ¨¡å‹å¤šæ¬¡è¾“å‡ºå…¶æ€ç»´ï¼Œå¹¶ä¸”è¿™äº›æ€ç»´æ˜¯æ ¹æ®è¾“å…¥æç¤ºè¿›è¡Œè¯„ä¼°çš„ã€‚
- ğŸ’» è¯¥æŠ€æœ¯çš„å®ç°æ˜¯ä¸€ç§æ ‘æœç´¢ï¼Œå¯ä»¥æ˜¯å¹¿åº¦ä¼˜å…ˆæˆ–æ·±åº¦ä¼˜å…ˆï¼Œå…·æœ‰åŸºäºæ¨¡å‹å¯¹æ€ç»´çš„è¯„ä¼°çš„ä¿®å‰ªã€‚
- ğŸŒŸ â€œTree of Thoughtsâ€æŠ€æœ¯æ˜¯ä¸€ç§æœ‰å‰é€”çš„æ–°æ–¹æ³•ï¼Œç»“åˆäº†è¯­è¨€æ¨¡å‹çš„å¼ºå¤§å’Œç®—æ³•çš„ç»“æ„ï¼Œå…è®¸æ›´åŠ æ·±æ€ç†Ÿè™‘å’Œæœ‰æ•ˆçš„é—®é¢˜è§£å†³ã€‚

### Detailed Summary
The article discusses a new approach to natural language processing (NLP) called "tree of thoughts" that aims to create a more interactive and iterative process for generating and evaluating language models. The approach involves breaking down a problem into smaller steps and evaluating each step before moving on to the next one. This leads to more evaluations of the language model, but also allows for more precise and accurate results.

The paper "Tree of Thoughts: Deliberate Problem Solving with Large Language Models" proposes a new decoding technique for using large language models. The technique involves an explicit tree search over the outputs of the language model, with the model itself valuing the different states. This allows for branching off and backtracking, which can be helpful for tasks where investigating the problem is important. The paper proposes some new tasks where the decoding technique is expected to work well, and the results show that it does indeed work well on those tasks.

The paper distinguishes between two types of prompting techniques: input-output prompting and Chain of Thought prompting. Input-output prompting involves specifying the task and optionally the output format, while Chain of Thought prompting instructs the model to explicitly make intermediate steps. The Tree of Thoughts technique is a type of Chain of Thought prompting, where the model is asked to output its thoughts multiple times, and then those thoughts are evaluated by the model with respect to the input prompt.

The Tree of Thoughts technique involves generating thoughts by sampling from the language model, and then evaluating those thoughts with respect to the input prompt. The thoughts are generated one at a time, and the model is asked to make one intermediate step at a time, rather than solving the problem completely. The thoughts can be sampled multiple times, or the model can be asked to output a list of thoughts. The thoughts are then evaluated by the model, either by assigning a value or by voting for the best thought.

The Tree of Thoughts technique is implemented as a tree search, either breadth-first or depth-first, with pruning based on the model's evaluation of the thoughts. The nodes of the tree represent the different thoughts generated by the model, and the edges represent the evaluation of those thoughts. The algorithm expands the node with the highest current value assigned to it, and backtracks when no nodes are above the value threshold.

The paper evaluates the Tree of Thoughts technique on several tasks, including arithmetic word problems, logical reasoning, and text completion. The results show that the technique outperforms other decoding techniques, including input-output prompting and Chain of Thought prompting. The authors suggest that the technique could be used for other tasks where deliberate problem-solving is important, such as creative writing or scientific discovery.

The article discusses a recent paper on the use of language models for problem-solving. The paper proposes a new approach to problem-solving that involves a language model exploring its own thoughts and backtracking when necessary. The authors of the paper demonstrate the effectiveness of their approach by applying it to crossword puzzles and other word games.

The paper's approach involves using a language model to generate possible solutions to a problem. The model then evaluates each solution and selects the best one. If the selected solution is not satisfactory, the model backtracks and tries again. The authors of the paper argue that this approach is more flexible and powerful than traditional problem-solving methods.

The paper's results show that the language model approach is effective at solving crossword puzzles and other word games. The authors of the paper also perform ablations to test the impact of different factors on the model's performance. They find that pruning and backtracking are important for the model's success, but that the model's performance is not greatly affected by the use of heuristics or oracles.

The article notes that the paper's approach is still limited in some ways. For example, the model still requires explicit prompts to guide its problem-solving process. The article suggests that future research should focus on developing a more general problem-solving model that does not require explicit prompts.

The article also discusses the potential applications of the paper's approach. The author suggests that the language model approach could be used in algorithms and other problem-solving tasks. The author notes that this could lead to a future where language models are integrated with classic algorithms to create more powerful problem-solving tools.

Overall, the tree of thoughts approach represents a promising new direction for NLP that emphasizes interactive and iterative problem-solving. However, further research is needed to determine its effectiveness in a wider range of applications and to develop more efficient implementation strategies.