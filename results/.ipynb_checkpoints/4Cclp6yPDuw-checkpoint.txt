ç®€è¦æ¦‚è¿°ï¼š
æœ¬æ–‡è®¨è®ºäº†ä¸‰ç¯‡è®ºæ–‡ï¼Œæ¢è®¨äº†ä½¿ç”¨Transformerå’Œå¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰è§£å†³è¯­è¨€ç†è§£é—®é¢˜çš„æ–¹æ³•ã€‚è¿™äº›è®ºæ–‡æå‡ºäº†ä¸åŒçš„æ–¹æ³•æ¥å¤„ç†æ›´é•¿çš„æ–‡æœ¬åºåˆ—ï¼Œå¹¶ç»“åˆè®°å¿†æœºåˆ¶æ¥æŒ‡å¯¼æ¨¡å‹å¯¹åç»­æ–‡æœ¬çš„ç†è§£ã€‚

è¦ç‚¹ï¼š
- ğŸ“ˆ ç¬¬ä¸€ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§å°†Transformeræ¨ç†æ‰©å±•åˆ°100ä¸‡ç”šè‡³200ä¸‡æ ‡è®°çš„æ–¹æ³•ï¼Œé€šè¿‡å°†æ–‡æœ¬åˆ†æˆå¤šä¸ªç‰‡æ®µå¹¶ä½¿ç”¨Transformerå¤„ç†æ¯ä¸ªç‰‡æ®µï¼Œç„¶ååœ¨æ¨ç†è¿‡ç¨‹ä¸­å°†è¿™äº›ç‰‡æ®µè¿æ¥åœ¨ä¸€èµ·ã€‚è¯¥æ¨¡å‹åœ¨å„ç§ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ï¼Œå¦‚è®°å¿†ä»»åŠ¡ã€æ£€æµ‹å’Œè®°å¿†ä»»åŠ¡ä»¥åŠæ¨ç†ä»»åŠ¡ã€‚
- ğŸ“š ç¬¬äºŒç¯‡è®ºæ–‡ä»‹ç»äº†Transformer XLæ¨¡å‹ï¼Œå®ƒæ˜¯Transformeræ¨¡å‹çš„æ‰©å±•ï¼Œç”¨äºå¤„ç†æ›´é•¿çš„æ–‡æœ¬åºåˆ—ã€‚è¯¥æ¨¡å‹å¯ä»¥è®°ä½å…ˆå‰æ–‡æœ¬æ®µçš„ä¿¡æ¯ï¼Œå¹¶å°†å…¶ç”¨äºåç»­æ–‡æœ¬æ®µçš„ç†è§£ã€‚ç„¶è€Œï¼Œè¯¥æ¨¡å‹åªèƒ½å›é¡¾åˆ°å‰ä¸€ä¸ªæ–‡æœ¬æ®µï¼Œæ— æ³•æœ‰æ•ˆåœ°å­˜å‚¨ä¿¡æ¯ä»¥ä¾›æœªæ¥ä½¿ç”¨ã€‚
- ğŸ¤– ç¬¬ä¸‰ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºTransformer XLçš„æ¨¡å‹ï¼Œå®ƒå°†Transformerå’ŒRNNç»“åˆèµ·æ¥è§£å†³è¯­è¨€ç†è§£é—®é¢˜ã€‚è¯¥æ¨¡å‹å¯ä»¥å¤„ç†æ—¶é—´åå‘ä¼ æ’­ï¼Œè¿™åœ¨ä»¥å‰è¢«è®¤ä¸ºæ˜¯ä¸ç¨³å®šçš„ã€‚è¯¥æ¨¡å‹ç‰¹åˆ«é€‚ç”¨äºéœ€è¦ä»é•¿æ–‡æœ¬ä¸­æå–å°‘é‡å•ä¸ªäº‹å®çš„ä»»åŠ¡ã€‚è¯¥æ¨¡å‹å¯ä»¥å­¦ä¹ å¿½ç•¥å™ªå£°å¹¶ä¸“æ³¨äºç›¸å…³ä¿¡æ¯ã€‚
- ğŸ§  è¿™äº›è®ºæ–‡å±•ç¤ºäº†ç»“åˆTransformerå’ŒRNNè§£å†³è¯­è¨€ç†è§£é—®é¢˜çš„æ½œåŠ›ã€‚è¿™äº›è®ºæ–‡æå‡ºäº†ä¸åŒçš„æ–¹æ³•æ¥å¤„ç†æ›´é•¿çš„æ–‡æœ¬åºåˆ—ï¼Œå¹¶ç»“åˆè®°å¿†æœºåˆ¶æ¥æŒ‡å¯¼æ¨¡å‹å¯¹åç»­æ–‡æœ¬çš„ç†è§£ã€‚è¿™äº›è®ºæ–‡ä¸ºè‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„ç ”ç©¶åšå‡ºäº†è´¡çŒ®ï¼Œå¹¶å±•ç¤ºäº†ç»“åˆä¸åŒçš„ç¥ç»ç½‘ç»œæ¶æ„æ¥è§£å†³å¤æ‚çš„è¯­è¨€ç†è§£é—®é¢˜çš„æ½œåŠ›ã€‚

### Detailed Summary
The article discusses three papers that explore the use of Transformers and Recurrent Neural Networks (RNNs) to solve language understanding problems. The first paper, "Scaling Transformer to 1 Million Tokens and Beyond with RMT," presents an approach to scaling Transformer inference to a massive 1 million or even 2 million tokens. The paper considers chunking the text into multiple segments and processing each segment with a Transformer, then connecting these segments together over inference. The paper presents various tasks, such as the memorize task, the detect and memorize task, and the reasoning task, and the model performs well across all token sizes. The paper's approach is distinct from Transformer XL, as it only adds memory tokens to carry information over from one segment to the next.

The second paper, "Transformer XL: Attentive Language Models Beyond a Fixed-Length Context," discusses the Transformer XL model, which is an extension of the Transformer model used for natural language processing tasks. The Transformer XL model is designed to handle longer sequences of text by incorporating a recurrent memory mechanism. The model can remember information from previous segments of text and use it to inform its understanding of subsequent segments. However, the article notes that there is a limit to how far back the model can remember, as backpropagation through time requires a lot of memory. The model can only attend to the previous segment, and the last segment has no chance of learning what to effectively store for the future. The article explains that memory has two components: looking back into the past and effectively storing information for the future. The Transformer XL model can only do the former, as there is no backpropagation through time to teach a previous layer what to store in its memory output.

The third paper, "Transformer-XL: Unleashing the Potential of Attention Models in Language Understanding," presents a model called Transformer XL that combines the power of Transformers with RNNs to solve language understanding problems. The model can handle backpropagation through time, which was previously known to be unstable. The authors have trained the model on seven segments, which is the maximum that can fit into their GPU memory for training. Once the model is trained, it can be used to infer over thousands of segments without any problem. The model is particularly useful for tasks that require the extraction of a few single facts dispersed in a long text. The authors have investigated how the model uses memory, and they have found that if there is no fact in the input, the model uses attention from the tokens to themselves. However, if there is a fact in the memory, the model uses attention to those memory tokens. The model can be used to detect complex interdependencies and interrelations and consider many things dispersed throughout the text. The authors have found that the model can learn to ignore the noise and focus on the relevant information. The model can be used to solve distinctive tasks that require the extraction of a few single facts from a long text. The authors have compared their model with other models that scale quadratically, and they have found that their model scales linearly.

Overall, these papers demonstrate the potential of combining Transformers and RNNs to solve language understanding problems. The papers present different approaches to handling longer sequences of text and incorporating memory mechanisms to inform the model's understanding of subsequent segments. While each approach has its limitations, they all show promise in solving tasks that require the extraction of a few single facts dispersed in a long text. The authors of these papers have investigated how their models use memory and attention to effectively store and retrieve information, and they have found that their models can detect complex interdependencies and interrelations and consider many things dispersed throughout the text. These papers contribute to the ongoing research in natural language processing and demonstrate the potential of combining different neural network architectures to solve complex language understanding problems.