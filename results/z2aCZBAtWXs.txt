#### ç®€è¦æ€»ç»“
æœ¬æ–‡è®¨è®ºäº†OpenAIå‘å¸ƒçš„æ–°ç‰ˆæœ¬3.5 turboæ¨¡å‹ï¼Œå…¶ä¸Šä¸‹æ–‡çª—å£ä¸º16,000ä¸ªæ ‡è®°ã€‚ä½œè€…æ—¨åœ¨é€šè¿‡æ¢ç´¢ä¸¤ä¸ªå°é¡¹ç›®ï¼ˆæ‘˜è¦å’Œæ’°å†™é•¿æ–‡ç« ï¼‰æ¥å±•ç¤º16,000ä¸ªæ ‡è®°ä¸Šä¸‹æ–‡çª—å£çš„èƒ½åŠ›å’Œé™åˆ¶ã€‚

#### äº®ç‚¹
- ğŸ“ ä½œè€…ä½¿ç”¨LLMé“¾æ¥æ€»ç»“ä¸€ç¯‡æ¥è‡ªArxivçš„é•¿ç¯‡è®ºæ–‡ã€‚ä½œè€…ä»è®ºæ–‡ä¸­æå–æ–‡æœ¬ï¼Œå¹¶ä½¿ç”¨PyPDF2 PDFé˜…è¯»å™¨è®¡ç®—æ ‡è®°æ•°ï¼Œç»“æœä¸º17,000ä¸ªæ ‡è®°ã€‚ç”±äºè¶…è¿‡äº†16,000ä¸ªæ ‡è®°çš„é™åˆ¶ï¼Œä½œè€…å†³å®šåœ¨å‚è€ƒæ–‡çŒ®éƒ¨åˆ†æ‹†åˆ†è®ºæ–‡ã€‚åˆ é™¤å‚è€ƒæ–‡çŒ®åï¼Œæ ‡è®°æ•°å‡å°‘åˆ°ä¸åˆ°15,000ä¸ªæ ‡è®°ã€‚
- ğŸ“ ä½œè€…è®¾ç½®äº†ç³»ç»Ÿå’Œäººç±»æç¤ºï¼Œä»¥ä¾¿ä¸ºèŠå¤©æ¨¡å‹è¿›è¡Œæ‘˜è¦ã€‚ç³»ç»Ÿæç¤ºä»‹ç»äº†AIç ”ç©¶äººå‘˜åœ¨åˆ†æMLã€AIå’ŒLLMè®ºæ–‡æ–¹é¢çš„ä¸“ä¸šçŸ¥è¯†ï¼Œè€Œäººç±»æç¤ºæŒ‡ç¤ºæ¨¡å‹æ€»ç»“è®ºæ–‡ï¼Œé‡ç‚¹å…³æ³¨æ¯ä¸ªéƒ¨åˆ†çš„è¦ç‚¹ï¼Œå¹¶æä¾›æ–¹æ³•çš„æ‰©å±•æ‘˜è¦ã€‚å°†è®ºæ–‡å†…å®¹ä½œä¸ºè¾“å…¥ä¼ é€’ï¼Œå¹¶ä½¿ç”¨turbo 16Kæ¨¡å‹è®¾ç½®æ‘˜è¦é“¾ã€‚
- ğŸ“ ä½œè€…è¿è¡Œæ‘˜è¦é“¾ï¼Œå¹¶ä½¿ç”¨OpenAIå›è°ƒè®¡ç®—æ ‡è®°æ•°ã€‚ç”Ÿæˆçš„æ‘˜è¦åŒ…å«438ä¸ªæ ‡è®°ï¼Œå±•ç¤ºäº†16,000ä¸ªæ ‡è®°ä¸Šä¸‹æ–‡çª—å£åœ¨å‡å°‘è¾“å‡ºé•¿åº¦çš„åŒæ—¶ä¿æŒè®ºæ–‡è¦ç‚¹çš„æœ‰æ•ˆæ€§ã€‚ç”Ÿæˆçš„æ‘˜è¦æ¦‚è¿°äº†è®ºæ–‡çš„æ‘˜è¦ã€å¼•è¨€ã€ç›¸å…³å·¥ä½œå’Œå®éªŒã€‚
- ğŸ“ ä½œè€…å°è¯•ä½¿ç”¨16,000ä¸ªæ ‡è®°ä¸Šä¸‹æ–‡çª—å£æ’°å†™ä¸€ç¯‡5,000å­—çš„æ–‡ç« ã€‚ç„¶è€Œï¼Œä»–ä»¬åœ¨å®ç°æ‰€æœŸæœ›çš„é•¿åº¦æ–¹é¢é‡åˆ°äº†é™åˆ¶ã€‚ä»…ä»…æŒ‡ç¤ºæ¨¡å‹æ’°å†™ä¸€ç¯‡é•¿æ–‡ç« å¹¶æ²¡æœ‰äº§ç”Ÿé¢„æœŸçš„ç»“æœã€‚ä½œè€…å°è¯•é¦–å…ˆç”Ÿæˆé—®é¢˜ï¼Œç„¶åå°†å…¶ä½œä¸ºæç¤ºä¼ é€’ä»¥ç”Ÿæˆæ–‡ç« ã€‚ç„¶è€Œï¼Œå³ä½¿ä½¿ç”¨æ›´é•¿çš„ä¸Šä¸‹æ–‡æ¨¡å‹ï¼Œè¾“å‡ºä»ç„¶ä¸è¶³ï¼Œåªç”Ÿæˆäº†1,248ä¸ªæ ‡è®°ã€‚
- ğŸ“ ä½œè€…å»ºè®®ï¼Œ16,000ä¸ªæ ‡è®°ä¸Šä¸‹æ–‡çª—å£æ›´é€‚åˆä»é•¿è¾“å…¥ä¸­ç”Ÿæˆè¾ƒçŸ­æˆ–ä¸­ç­‰é•¿åº¦çš„è¾“å‡ºã€‚ä½¿ç”¨è¯¥æ¨¡å‹ç”Ÿæˆå¤§é‡æ–‡æœ¬çš„å°è¯•è¯æ˜å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä½œè€…æåˆ°äº†ä½¿ç”¨åŸºäºå†…å­˜çš„æ–¹æ³•çš„å¯èƒ½æ€§ï¼Œå…¶ä¸­æ¯ä¸ªé—®é¢˜éƒ½å•ç‹¬å›ç­”ï¼Œä½†æ‰¿è®¤è¿™ç§æ–¹æ³•çš„æˆæœ¬å’Œå¤æ‚æ€§å¢åŠ ã€‚
- ğŸ“ æ€»ä¹‹ï¼Œæœ¬æ–‡å¼ºè°ƒäº†16,000ä¸ªæ ‡è®°ä¸Šä¸‹æ–‡çª—å£åœ¨æ‘˜è¦å’Œæ’°å†™é•¿æ–‡ç« æ–¹é¢çš„ä¼˜åŠ¿å’Œé™åˆ¶ã€‚è¯¥æ¨¡å‹é€šè¿‡å‡å°‘è¾“å‡ºé•¿åº¦åŒæ—¶ä¿ç•™å…³é”®ä¿¡æ¯åœ¨æ‘˜è¦æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä½†ç”Ÿæˆé•¿æ–‡ç« æ›´å…·æŒ‘æˆ˜æ€§ã€‚ä½œè€…é¼“åŠ±åœ¨æ¨¡å‹çš„é™åˆ¶å†…å°è¯•ä¸åŒçš„æç¤ºå’Œæ–¹æ³•ä»¥å®ç°æ‰€æœŸæœ›çš„ç»“æœã€‚

### Detailed Summary
The article discusses the release of OpenAI's new version of the 3.5 turbo model with a context window of 16,000 tokens. The author aims to demonstrate the capabilities and limitations of the 16,000 token context window by exploring two mini-projects: summarization and writing a long article.

In the first mini-project, the author focuses on summarization. They use an LLM chain to summarize a long paper from Arxiv. The paper chosen is the "Tree of Thoughts" paper, which is both lengthy and dense. The author extracts the text from the paper using the PyPDF2 PDF reader and counts the number of tokens, which amounts to 17,000 tokens. Since this exceeds the 16,000 token limit, the author decides to split the paper at the references section. After removing the references, the token count is reduced to just under 15,000 tokens.

To perform the summarization, the author sets up the system and human prompts for the chat model. The system prompt introduces the AI researcher's expertise in analyzing ML, AI, and LLM papers, while the human prompt instructs the model to summarize the paper, focusing on key takeaways for each section and providing an expanded summary of the methods. The paper content is passed as input, and the summary chain is set up using the turbo 16K model.

The author runs the summary chain and uses the OpenAI callback to count the tokens. The output summary contains 438 tokens, showcasing the effectiveness of the 16,000 token context window for reducing the output length while maintaining the essence of the paper. The generated summary provides an overview of the paper, including the abstract, introduction, related works, and experiments.

In the second mini-project, the author attempts to write a 5,000-word article using the 16,000 token context window. However, they encounter limitations in achieving the desired length. Simply instructing the model to write a long article does not yield the expected results. The author tries generating questions first and then passing them as prompts to generate the article. However, even with the longer context model, the output falls short, with only 1,248 tokens generated.

The author suggests that the 16,000 token context window is more suitable for generating shorter or medium-length outputs from long inputs. Attempting to generate a large amount of text proves challenging with this model. The author mentions the possibility of using a memory-based approach, where each question is answered separately, but acknowledges the increased cost and complexity of such an approach.

In conclusion, the article highlights the advantages and limitations of the 16,000 token context window in the context of summarization and long article writing. While the model excels at summarization by reducing the output length while preserving key information, generating lengthy articles proves more challenging. The author encourages experimentation with prompts and approaches to achieve desired results within the limitations of the model.